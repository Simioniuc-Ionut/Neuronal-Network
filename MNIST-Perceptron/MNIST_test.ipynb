{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform= lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    mnist_data= []\n",
    "    mnist_labels= []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "     \n",
    "    # print(procesed_mnist_labels)    \n",
    "    return mnist_data, mnist_labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_data(mnist_data,mnist_labels):\n",
    "    mnist_data = np.array(mnist_data).reshape(-1,784)\n",
    "    mnist_labels = np.array(mnist_labels).reshape(-1,1) \n",
    "    # infer all data to rows,with 1 column\n",
    "    # convert the labels to one-hot-encoding . \n",
    "    # EX: { Red,Blue,White} as : Red = [1,0,0] , Blue = [ 0, 1, 0] and White = [ 0, 0, 1]\n",
    "    \n",
    "    \n",
    "    # Here we normalize the label data\n",
    "    procesed_mnist_labels = []\n",
    "    for mnist_label in mnist_labels:\n",
    "        a = np.array([0,0,0,0,0,0,0,0,0,0]) # or we could use : np.zeros(10)\n",
    "        value =mnist_label[0]\n",
    "        a[value] = 1\n",
    "        procesed_mnist_labels.append(np.array(a))\n",
    "        \n",
    "    # Here we normalize data set ,with a representation in [0 , 1]\n",
    "    mnist_data = mnist_data / 255\n",
    "    # print(procesed_mnist_labels)\n",
    "    return mnist_data, procesed_mnist_labels\n",
    "    "
   ],
   "id": "d1b75adb73c0aa5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# split data in batches \n",
    "def split_in_batches(data, labels, batch_size=100):\n",
    "    dataset = TensorDataset(torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    batched_data = []\n",
    "    batched_labels = []\n",
    "    \n",
    "    for batch_data, batch_labels in loader:\n",
    "        batched_data.append(batch_data.numpy())\n",
    "        batched_labels.append(batch_labels.numpy())\n",
    "    return batched_data, batched_labels\n",
    "\n",
    "def initialize_parameters(input_size, num_classes):\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases for the model.\n",
    "    - input_size: the size of the input vector (784 for 28x28 images)\n",
    "    - num_classes: the number of classes (10 for MNIST)\n",
    "    \"\"\"\n",
    "    # Initialize weights with random values from a normal distribution\n",
    "    weights = np.random.randn(input_size, num_classes) * 0.01  # Size: [784, 10]\n",
    "    \n",
    "    # Initialize biases with zero values\n",
    "    biases = np.zeros((1, num_classes))  # Size: [1, 10]   \n",
    "    return weights, biases # Practically we return Wi and Bi with i in [1,10] which represent each label\n",
    "\n",
    "def forward(data, weights, biases):\n",
    "    \"\"\"\n",
    "    Compute logits for each image.\n",
    "    - data: the dataset of images (input vectors)\n",
    "    - weights: the weights (matrix [784, 10])\n",
    "    - biases: the biases (vector [1, 10])\n",
    "    \"\"\"\n",
    "   # Compute the dot product between each image in the batch and the weights\n",
    "    # and add the bias for each class.\n",
    "    logits = (data @ weights) + biases  # Output size: [num_images, 10]\n",
    "    return logits # This is pondered sum which indicate score for each class"
   ],
   "id": "737bf83ffc0844ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "     This function transform eac score in a probability by applying softmax function\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True)) # Subtract the maximum value for numerical stability\n",
    "    probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities # the predicted class coreesponds to the highest probability\n",
    "    "
   ],
   "id": "19a599700203c05e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_cross_entropy_loss(labels, probabilities):\n",
    "    # Convert labels and probabilities to PyTorch tensors if they are not already\n",
    "    torch_labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    torch_probabilities = torch.tensor(probabilities, dtype=torch.float32)\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    cross_entropy_loss = -torch.sum(torch_labels * torch.log(torch_probabilities), dim=1)  # This is the loss for a single image\n",
    "    # Compute the average loss over the batch\n",
    "    average_loss = torch.mean(cross_entropy_loss)\n",
    "    \n",
    "    return average_loss  # This is the average loss for all images"
   ],
   "id": "2f365c8a47abb890",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_epoch(process_train_X, process_train_Y, weights, biases):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    - process_train_X: the training dataset of images\n",
    "    - process_train_Y: the training dataset of labels\n",
    "    - weights: the weights of the model\n",
    "    - biases: the biases of the model\n",
    "    \"\"\"\n",
    "    # Split the training data into batches\n",
    "    batched_train_data, batched_train_labels = split_in_batches(process_train_X, process_train_Y)\n",
    "\n",
    "    # Initialize the loss\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    for batch_data, batch_labels in zip(batched_train_data, batched_train_labels):\n",
    "        # Compute the logits\n",
    "        logits = forward(batch_data, weights, biases)\n",
    "        \n",
    "        # Compute the probabilities\n",
    "        probabilities = softmax(logits)\n",
    "        \n",
    "        # Compute the cross-entropy loss\n",
    "        loss = evaluate_cross_entropy_loss(batch_labels, probabilities)\n",
    "        \n",
    "        # Update the epoch loss\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss /= len(batched_train_data)\n",
    "    \n",
    "    print(f\"Epoch loss: {epoch_loss}\")\n",
    "    \n",
    "    return epoch_loss\n",
    "    "
   ],
   "id": "d69d881849f9bac4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_weights(process_train_X, process_train_Y, weights, biases, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Update the weights and biases for the model using gradient descent.\n",
    "    - process_train_X: the training dataset of images\n",
    "    - process_train_Y: the training dataset of labels\n",
    "    - weights: the weights of the model\n",
    "    - biases: the biases of the model\n",
    "    - learning_rate: the learning rate for the update\n",
    "    \"\"\"\n",
    "    batched_train_data, batched_train_labels = split_in_batches(process_train_X, process_train_Y)\n",
    "    \n",
    "    for batch_data, batch_labels in zip(batched_train_data, batched_train_labels):\n",
    "        # Compute the logits\n",
    "        logits = forward(batch_data, weights, biases)\n",
    "        \n",
    "        # Compute the probabilities\n",
    "        probabilities = softmax(logits)\n",
    "        # Compute the error (Target - y)\n",
    "        error = batch_labels - probabilities\n",
    "\n",
    "        # Compute the gradients for weights and biases\n",
    "        weights_gradient = np.dot(batch_data.T, error) / len(batch_data)\n",
    "        biases_gradient = np.sum(error,axis=0) / len(batch_data)\n",
    "    \n",
    "        # Update the weights and biases\n",
    "        weights -= learning_rate * weights_gradient\n",
    "        biases -= learning_rate * biases_gradient\n",
    "\n",
    "    return weights, biases"
   ],
   "id": "a7454ccc0c489dbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_learning_rate(initial_lr, epoch, decay):\n",
    "    return initial_lr / (1 + decay * epoch)\n"
   ],
   "id": "989289c92761c2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def calculate_accuracy(data, labels, weights, biases):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model.\n",
    "    - data: the dataset (either train or test)\n",
    "    - labels: the true labels\n",
    "    - weights: the weights of the model\n",
    "    - biases: the biases of the model\n",
    "    \"\"\"\n",
    "    # Compute logits (raw scores for each class)\n",
    "    logits = forward(data, weights, biases)\n",
    "    \n",
    "    # Compute probabilities (softmax output)\n",
    "    probabilities = softmax(logits)\n",
    "    \n",
    "    # Get the predicted class for each data point\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    # Get the true class from the labels (assuming labels are one-hot encoded)\n",
    "    true_classes = np.argmax(labels, axis=1)\n",
    "    \n",
    "    # Calculate how many predictions are correct\n",
    "    correct_predictions = np.sum(predicted_classes == true_classes)\n",
    "    \n",
    "    # Calculate accuracy as a percentage\n",
    "    accuracy = correct_predictions / len(labels) * 100\n",
    "    \n",
    "    return accuracy\n"
   ],
   "id": "1b9eef589272c642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# obtain train and test raw data\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_x, test_y = download_mnist(False)\n",
    "\n",
    "# process data ,normalizing \n",
    "process_train_X , process_train_Y = process_data(train_X,train_Y)\n",
    "process_test_x,process_test_y=process_data(test_x,test_y)\n",
    "# first time we initialize random weights for training\n",
    "weights,biases = initialize_parameters(784,10) # 784 is the number of pixels in the image and 10 is the number of classes\n",
    "\n"
   ],
   "id": "fa0e8d8733fadc9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "biases"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-20T15:14:42.407199Z",
     "start_time": "2024-10-20T15:14:28.518650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model for one epoch\n",
    "# Încărcare weights și biases din fișier\n",
    "weights = np.load(\"weights.npy\")\n",
    "biases = np.load(\"biases.npy\")\n",
    "\n",
    "num_epochs= 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    # Actualizează learning rate-ul\n",
    "    # learning_rate = update_learning_rate(initial_lr, epoch, decay)\n",
    "\n",
    "    # Antrenează modelul și actualizează weights și biases\n",
    "    epoch_loss = train_epoch(process_train_X, process_train_Y, weights, biases)\n",
    "    weights, biases = update_weights(process_train_X, process_train_Y, weights, biases, 0.001)\n",
    "    # print(\"epoch \",epoch,\"weights\",weights)\n",
    "    # print(\"epoch\",epoch,\"biases\",biases)\n",
    "    \n",
    "    print(f\"Loss after epoch {epoch + 1}: {epoch_loss}\")\n",
    "   \n",
    "\n",
    "# Calculează acuratețea pe setul de antrenament\n",
    "train_accuracy = calculate_accuracy(process_train_X, process_train_Y, weights, biases)   \n",
    "print(f\"Train Accuracy: {train_accuracy}%\")\n",
    "# print(\"learning rate\",learning_rate)\n",
    "# Salvare weights și biases într-un fișier\n",
    "\n"
   ],
   "id": "b8f0d629c9b8dac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch loss: nan\n",
      "Loss after epoch 1: nan\n",
      "Epoch 2/100\n",
      "Epoch loss: nan\n",
      "Loss after epoch 2: nan\n",
      "Epoch 3/100\n",
      "Epoch loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[99], line 14\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Actualizează learning rate-ul\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# learning_rate = update_learning_rate(initial_lr, epoch, decay)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Antrenează modelul și actualizează weights și biases\u001B[39;00m\n\u001B[0;32m     13\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m train_epoch(process_train_X, process_train_Y, weights, biases)\n\u001B[1;32m---> 14\u001B[0m weights, biases \u001B[38;5;241m=\u001B[39m \u001B[43mupdate_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_train_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocess_train_Y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbiases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# print(\"epoch \",epoch,\"weights\",weights)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# print(\"epoch\",epoch,\"biases\",biases)\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoss after epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[80], line 10\u001B[0m, in \u001B[0;36mupdate_weights\u001B[1;34m(process_train_X, process_train_Y, weights, biases, learning_rate)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_weights\u001B[39m(process_train_X, process_train_Y, weights, biases, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m):\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m    Update the weights and biases for the model using gradient descent.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m    - process_train_X: the training dataset of images\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m    - learning_rate: the learning rate for the update\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m     batched_train_data, batched_train_labels \u001B[38;5;241m=\u001B[39m \u001B[43msplit_in_batches\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_train_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocess_train_Y\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_data, batch_labels \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(batched_train_data, batched_train_labels):\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;66;03m# Compute the logits\u001B[39;00m\n\u001B[0;32m     14\u001B[0m         logits \u001B[38;5;241m=\u001B[39m forward(batch_data, weights, biases)\n",
      "Cell \u001B[1;32mIn[76], line 12\u001B[0m, in \u001B[0;36msplit_in_batches\u001B[1;34m(data, labels, batch_size)\u001B[0m\n\u001B[0;32m      9\u001B[0m batched_data \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     10\u001B[0m batched_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 12\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_labels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatched_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatched_labels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_labels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 212\u001B[0m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T14:45:39.440137Z",
     "start_time": "2024-10-20T14:45:39.435435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Salvare weights și biases într-un fișier\n",
    "# np.save(\"weights.npy\", weights)\n",
    "# np.save(\"biases.npy\", biases)\n"
   ],
   "id": "7bb2e0ea08e96241",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Epoch loss: 2.3064732551574707 \n",
    "\n",
    "Epoch loss: 2.306473731994629\n",
    "\n",
    "Epoch loss: 2.29582142829895\n",
    "\n",
    "Epoch loss: 2.2958221435546875\n"
   ],
   "id": "f8cbee6af2062cd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# %pip install --upgrade certifi",
   "id": "7b8407a858f8db5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# # Example batch of one-hot encoded labels (batch_size=3, num_classes=4)\n",
    "# labels = np.array([\n",
    "#     [0, 0, 1, 0],\n",
    "#     [0, 1, 0, 0],\n",
    "#     [1, 0, 0, 0]\n",
    "# ])\n",
    "# \n",
    "# # Example batch of predicted probabilities (batch_size=3, num_classes=4)\n",
    "# probabilities = np.array([\n",
    "#     [0.1, 0.2, 0.6, 0.1],\n",
    "#     [0.3, 0.4, 0.2, 0.1],\n",
    "#     [0.7, 0.1, 0.1, 0.1]\n",
    "# ])\n",
    "# \n",
    "# # Compute the cross-entropy loss for each image\n",
    "# cross_entropy_loss = -np.sum(labels * np.log(probabilities), axis=1)\n",
    "# \n",
    "# # Compute the average loss over the batch\n",
    "# average_loss = np.mean(cross_entropy_loss)\n",
    "# \n",
    "# print(\"Cross-Entropy Loss for each image:\", cross_entropy_loss)\n",
    "# print(\"Average Cross-Entropy Loss for the batch:\", average_loss)"
   ],
   "id": "882276fd9624043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sample input data\n",
    "# process_train_X = np.array([[0.5, 0.2, 0.1], [0.9, 0.7, 0.3]])\n",
    "# process_train_Y = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "# \n",
    "# # Initial weights and biases\n",
    "# weights = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "# biases = np.array([[0.1, 0.2, 0.3]])\n",
    "# \n",
    "# # Learning rate\n",
    "# learning_rate = 0.01\n",
    "# \n",
    "# # Define the required functions\n",
    "# # def split_in_batches(data, labels, batch_size=2):\n",
    "# #     return [data], [labels]\n",
    "# \n",
    "# # def compute_logits(data, weights, biases):\n",
    "# #     return np.dot(data, weights) + biases\n",
    "# # \n",
    "# # def softmax(logits):\n",
    "# #     exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "# #     return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "# # \n",
    "# # # Function to test\n",
    "# # def update_weights(process_train_X, process_train_Y, weights, biases, learning_rate=0.01):\n",
    "# #     batched_train_data, batched_train_labels = split_in_batches(process_train_X, process_train_Y)\n",
    "# # \n",
    "# #     for batch_data, batch_labels in zip(batched_train_data, batched_train_labels):\n",
    "# #         logits = compute_logits(batch_data, weights, biases)\n",
    "# #         probabilities = softmax(logits)\n",
    "# #         error = batch_labels - probabilities\n",
    "# #         weights_gradient = np.dot(batch_data.T, error) / len(batch_data)\n",
    "# #         biases_gradient = np.sum(error, axis=0) / len(batch_data)\n",
    "# #         weights -= learning_rate * weights_gradient\n",
    "# #         biases -= learning_rate * biases_gradient\n",
    "# #     return weights, biases\n",
    "# \n",
    "# # Run the test\n",
    "# updated_weights, updated_biases = update_weights(process_train_X, process_train_Y, weights, biases, learning_rate)\n",
    "# \n",
    "# # Print the results\n",
    "# print(\"Updated weights:\")\n",
    "# print(updated_weights)\n",
    "# print(\"Updated biases:\")\n",
    "# print(updated_biases)"
   ],
   "id": "c7c38bf073c9376",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
