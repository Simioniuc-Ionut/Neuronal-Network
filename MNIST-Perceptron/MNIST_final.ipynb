{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:17:06.155716Z",
     "start_time": "2024-10-28T13:16:53.917036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform= lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    mnist_data= []\n",
    "    mnist_labels= []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "def process_data(mnist_data, mnist_labels):\n",
    "    mnist_data = np.array(mnist_data).reshape(-1, 784)\n",
    "    mnist_labels = np.array(mnist_labels).reshape(-1, 1)\n",
    "\n",
    "    procesed_mnist_labels = []\n",
    "    for mnist_label in mnist_labels:\n",
    "        a = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        value = mnist_label[0]\n",
    "        a[value] = 1\n",
    "        procesed_mnist_labels.append(np.array(a))\n",
    "\n",
    "    mnist_data = mnist_data / 255\n",
    "    return mnist_data, procesed_mnist_labels"
   ],
   "id": "8e8d8268e0d3e58c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:17:06.198860Z",
     "start_time": "2024-10-28T13:17:06.190806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Split data into batches\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def split_in_batches(data, labels, batch_size=100):\n",
    "    dataset = TensorDataset(torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    batched_data = []\n",
    "    batched_labels = []\n",
    "\n",
    "    for batch_data, batch_labels in loader:\n",
    "        batched_data.append(batch_data.numpy())\n",
    "        batched_labels.append(batch_labels.numpy())\n",
    "    return batched_data, batched_labels "
   ],
   "id": "95542551f2625ee7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:17:06.622231Z",
     "start_time": "2024-10-28T13:17:06.607525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.weights = np.random.randn(input_size, num_classes) * 0.01\n",
    "        self.bias = np.zeros((1, num_classes))\n",
    "    \n",
    "    # Here we compute the softmax function \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True) # normalization \n",
    "    \n",
    "    # Here we compute the forward pass \n",
    "    def forward(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.softmax(z) # we return the probabilities of each class\n",
    "    \n",
    "    # Here we compute the loss\n",
    "    def compute_loss(self, Y_pred, Y_true):\n",
    "        m = Y_true.shape[0] \n",
    "        loss = -np.sum(Y_true * np.log(Y_pred + 1e-9)) / m # cross-entropy loss ,we compute mean loss\n",
    "        return loss\n",
    "\n",
    "    # Here we actually compute the gradients and update the weights and biases\n",
    "    def backward(self, X, Y_pred, Y_true, learning_rate):\n",
    "        m = X.shape[0] # number of examples\n",
    "        dz = Y_pred - Y_true  # error term\n",
    "        dw = np.dot(X.T, dz) / m  # gradient for weights\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m  # gradient for biases\n",
    "        # aici e acelasi lucru cu weights += learning_rate * dw daca dz = Y_true - Y_pred.\n",
    "        # practic aplicam gradient descent : x = x - df/dx ,(derivata lui f in functie de x)\n",
    "        self.weights -= learning_rate * dw\n",
    "        self.bias -= learning_rate * db"
   ],
   "id": "61fd26140b3f6b75",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T17:10:12.968803Z",
     "start_time": "2024-10-28T17:10:12.964564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Forward pass (logits computation)\n",
    "def train_epoch(process_train_X, process_train_Y, perceptron, learning_rate):\n",
    "    batched_train_data, batched_train_labels = split_in_batches(process_train_X, process_train_Y)\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_data, batch_labels in zip(batched_train_data, batched_train_labels):\n",
    "        probabilities = perceptron.forward(batch_data)\n",
    "        loss = perceptron.compute_loss(probabilities, batch_labels)\n",
    "        perceptron.backward(batch_data, probabilities, batch_labels, learning_rate)\n",
    "        epoch_loss += loss\n",
    "\n",
    "    epoch_loss /= len(batched_train_data)\n",
    "    print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "    return epoch_loss\n"
   ],
   "id": "e1b373eaad763eb8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T17:10:10.681433Z",
     "start_time": "2024-10-28T17:10:10.676234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(data, labels, perceptron):\n",
    "    probabilities = perceptron.forward(data)\n",
    "    predicted_classes = np.argmax(probabilities, axis=1) # get the index of the highest probability\n",
    "    true_classes = np.argmax(labels, axis=1)   # get the index of the true class\n",
    "    correct_predictions = np.sum(predicted_classes == true_classes) # count how many predictions were correct\n",
    "    accuracy = correct_predictions / len(labels) * 100 \n",
    "    return accuracy"
   ],
   "id": "ca8ef1b22f920623",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T17:11:37.986387Z",
     "start_time": "2024-10-28T17:10:15.883649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X, train_Y = download_mnist(True)\n",
    "test_x, test_y = download_mnist(False)\n",
    "\n",
    "process_train_X, process_train_Y = process_data(train_X, train_Y)\n",
    "process_test_x, process_test_y = process_data(test_x, test_y)\n",
    "\n",
    "# just first time\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "learning_rate = 0.1\n",
    "perceptron = Perceptron(input_size, num_classes)\n",
    "\n",
    "## here we train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_loss = train_epoch(process_train_X, process_train_Y, perceptron, learning_rate)\n",
    "    print(f\"Loss after epoch {epoch + 1}: {epoch_loss}\")\n",
    "\n",
    "# Calculate the accuracy on the training data\n",
    "train_accuracy = calculate_accuracy(process_train_X, process_train_Y, perceptron)\n",
    "print(f\"Train Accuracy: {train_accuracy}%\")"
   ],
   "id": "c39e6c11e3b6b7ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_32112\\908102043.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  dataset = TensorDataset(torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.5339384244571144\n",
      "Loss after epoch 1: 0.5339384244571144\n",
      "Epoch 2/100\n",
      "Epoch loss: 0.3588183438962715\n",
      "Loss after epoch 2: 0.3588183438962715\n",
      "Epoch 3/100\n",
      "Epoch loss: 0.3307755577402603\n",
      "Loss after epoch 3: 0.3307755577402603\n",
      "Epoch 4/100\n",
      "Epoch loss: 0.3163661694580946\n",
      "Loss after epoch 4: 0.3163661694580946\n",
      "Epoch 5/100\n",
      "Epoch loss: 0.30688279593574724\n",
      "Loss after epoch 5: 0.30688279593574724\n",
      "Epoch 6/100\n",
      "Epoch loss: 0.3001344055427969\n",
      "Loss after epoch 6: 0.3001344055427969\n",
      "Epoch 7/100\n",
      "Epoch loss: 0.29483189321120346\n",
      "Loss after epoch 7: 0.29483189321120346\n",
      "Epoch 8/100\n",
      "Epoch loss: 0.2906950970926531\n",
      "Loss after epoch 8: 0.2906950970926531\n",
      "Epoch 9/100\n",
      "Epoch loss: 0.28709878620431556\n",
      "Loss after epoch 9: 0.28709878620431556\n",
      "Epoch 10/100\n",
      "Epoch loss: 0.2842889208935372\n",
      "Loss after epoch 10: 0.2842889208935372\n",
      "Epoch 11/100\n",
      "Epoch loss: 0.2817325364140093\n",
      "Loss after epoch 11: 0.2817325364140093\n",
      "Epoch 12/100\n",
      "Epoch loss: 0.27937401500148923\n",
      "Loss after epoch 12: 0.27937401500148923\n",
      "Epoch 13/100\n",
      "Epoch loss: 0.277732402384127\n",
      "Loss after epoch 13: 0.277732402384127\n",
      "Epoch 14/100\n",
      "Epoch loss: 0.2759127263660432\n",
      "Loss after epoch 14: 0.2759127263660432\n",
      "Epoch 15/100\n",
      "Epoch loss: 0.274183640436392\n",
      "Loss after epoch 15: 0.274183640436392\n",
      "Epoch 16/100\n",
      "Epoch loss: 0.2731274080415455\n",
      "Loss after epoch 16: 0.2731274080415455\n",
      "Epoch 17/100\n",
      "Epoch loss: 0.271497225924846\n",
      "Loss after epoch 17: 0.271497225924846\n",
      "Epoch 18/100\n",
      "Epoch loss: 0.27039324467673026\n",
      "Loss after epoch 18: 0.27039324467673026\n",
      "Epoch 19/100\n",
      "Epoch loss: 0.26936775083157677\n",
      "Loss after epoch 19: 0.26936775083157677\n",
      "Epoch 20/100\n",
      "Epoch loss: 0.2683520958496626\n",
      "Loss after epoch 20: 0.2683520958496626\n",
      "Epoch 21/100\n",
      "Epoch loss: 0.267273489346571\n",
      "Loss after epoch 21: 0.267273489346571\n",
      "Epoch 22/100\n",
      "Epoch loss: 0.26647284281119554\n",
      "Loss after epoch 22: 0.26647284281119554\n",
      "Epoch 23/100\n",
      "Epoch loss: 0.265346467188841\n",
      "Loss after epoch 23: 0.265346467188841\n",
      "Epoch 24/100\n",
      "Epoch loss: 0.26475640428675545\n",
      "Loss after epoch 24: 0.26475640428675545\n",
      "Epoch 25/100\n",
      "Epoch loss: 0.26396897878784437\n",
      "Loss after epoch 25: 0.26396897878784437\n",
      "Epoch 26/100\n",
      "Epoch loss: 0.26319120776876426\n",
      "Loss after epoch 26: 0.26319120776876426\n",
      "Epoch 27/100\n",
      "Epoch loss: 0.2623360619608219\n",
      "Loss after epoch 27: 0.2623360619608219\n",
      "Epoch 28/100\n",
      "Epoch loss: 0.2616544309078235\n",
      "Loss after epoch 28: 0.2616544309078235\n",
      "Epoch 29/100\n",
      "Epoch loss: 0.2613809904672035\n",
      "Loss after epoch 29: 0.2613809904672035\n",
      "Epoch 30/100\n",
      "Epoch loss: 0.2605399320395526\n",
      "Loss after epoch 30: 0.2605399320395526\n",
      "Epoch 31/100\n",
      "Epoch loss: 0.26014668615804654\n",
      "Loss after epoch 31: 0.26014668615804654\n",
      "Epoch 32/100\n",
      "Epoch loss: 0.2593613156541646\n",
      "Loss after epoch 32: 0.2593613156541646\n",
      "Epoch 33/100\n",
      "Epoch loss: 0.2589520429179316\n",
      "Loss after epoch 33: 0.2589520429179316\n",
      "Epoch 34/100\n",
      "Epoch loss: 0.25841029933492865\n",
      "Loss after epoch 34: 0.25841029933492865\n",
      "Epoch 35/100\n",
      "Epoch loss: 0.25794481841199296\n",
      "Loss after epoch 35: 0.25794481841199296\n",
      "Epoch 36/100\n",
      "Epoch loss: 0.2576615080826544\n",
      "Loss after epoch 36: 0.2576615080826544\n",
      "Epoch 37/100\n",
      "Epoch loss: 0.25701797028403617\n",
      "Loss after epoch 37: 0.25701797028403617\n",
      "Epoch 38/100\n",
      "Epoch loss: 0.2565721753764011\n",
      "Loss after epoch 38: 0.2565721753764011\n",
      "Epoch 39/100\n",
      "Epoch loss: 0.256095962568741\n",
      "Loss after epoch 39: 0.256095962568741\n",
      "Epoch 40/100\n",
      "Epoch loss: 0.25598181214997506\n",
      "Loss after epoch 40: 0.25598181214997506\n",
      "Epoch 41/100\n",
      "Epoch loss: 0.2554903288087206\n",
      "Loss after epoch 41: 0.2554903288087206\n",
      "Epoch 42/100\n",
      "Epoch loss: 0.255115301089029\n",
      "Loss after epoch 42: 0.255115301089029\n",
      "Epoch 43/100\n",
      "Epoch loss: 0.25469250530373516\n",
      "Loss after epoch 43: 0.25469250530373516\n",
      "Epoch 44/100\n",
      "Epoch loss: 0.2542103810155293\n",
      "Loss after epoch 44: 0.2542103810155293\n",
      "Epoch 45/100\n",
      "Epoch loss: 0.25399023068097565\n",
      "Loss after epoch 45: 0.25399023068097565\n",
      "Epoch 46/100\n",
      "Epoch loss: 0.25373338733309136\n",
      "Loss after epoch 46: 0.25373338733309136\n",
      "Epoch 47/100\n",
      "Epoch loss: 0.2532928390181615\n",
      "Loss after epoch 47: 0.2532928390181615\n",
      "Epoch 48/100\n",
      "Epoch loss: 0.2529967693334978\n",
      "Loss after epoch 48: 0.2529967693334978\n",
      "Epoch 49/100\n",
      "Epoch loss: 0.2524702906214889\n",
      "Loss after epoch 49: 0.2524702906214889\n",
      "Epoch 50/100\n",
      "Epoch loss: 0.2522422270314429\n",
      "Loss after epoch 50: 0.2522422270314429\n",
      "Epoch 51/100\n",
      "Epoch loss: 0.2520108788156925\n",
      "Loss after epoch 51: 0.2520108788156925\n",
      "Epoch 52/100\n",
      "Epoch loss: 0.25141758253553204\n",
      "Loss after epoch 52: 0.25141758253553204\n",
      "Epoch 53/100\n",
      "Epoch loss: 0.2514095090346898\n",
      "Loss after epoch 53: 0.2514095090346898\n",
      "Epoch 54/100\n",
      "Epoch loss: 0.2512378846101223\n",
      "Loss after epoch 54: 0.2512378846101223\n",
      "Epoch 55/100\n",
      "Epoch loss: 0.2508928403765998\n",
      "Loss after epoch 55: 0.2508928403765998\n",
      "Epoch 56/100\n",
      "Epoch loss: 0.25053860758710034\n",
      "Loss after epoch 56: 0.25053860758710034\n",
      "Epoch 57/100\n",
      "Epoch loss: 0.25033335953724883\n",
      "Loss after epoch 57: 0.25033335953724883\n",
      "Epoch 58/100\n",
      "Epoch loss: 0.25031353933881784\n",
      "Loss after epoch 58: 0.25031353933881784\n",
      "Epoch 59/100\n",
      "Epoch loss: 0.249972236839394\n",
      "Loss after epoch 59: 0.249972236839394\n",
      "Epoch 60/100\n",
      "Epoch loss: 0.2497188883359311\n",
      "Loss after epoch 60: 0.2497188883359311\n",
      "Epoch 61/100\n",
      "Epoch loss: 0.24929496173796223\n",
      "Loss after epoch 61: 0.24929496173796223\n",
      "Epoch 62/100\n",
      "Epoch loss: 0.24932328139673784\n",
      "Loss after epoch 62: 0.24932328139673784\n",
      "Epoch 63/100\n",
      "Epoch loss: 0.24877702622783548\n",
      "Loss after epoch 63: 0.24877702622783548\n",
      "Epoch 64/100\n",
      "Epoch loss: 0.24883032825084028\n",
      "Loss after epoch 64: 0.24883032825084028\n",
      "Epoch 65/100\n",
      "Epoch loss: 0.24845594960718367\n",
      "Loss after epoch 65: 0.24845594960718367\n",
      "Epoch 66/100\n",
      "Epoch loss: 0.24800038724869608\n",
      "Loss after epoch 66: 0.24800038724869608\n",
      "Epoch 67/100\n",
      "Epoch loss: 0.24807454800915546\n",
      "Loss after epoch 67: 0.24807454800915546\n",
      "Epoch 68/100\n",
      "Epoch loss: 0.24762439316257884\n",
      "Loss after epoch 68: 0.24762439316257884\n",
      "Epoch 69/100\n",
      "Epoch loss: 0.24774798121240735\n",
      "Loss after epoch 69: 0.24774798121240735\n",
      "Epoch 70/100\n",
      "Epoch loss: 0.2474954805966771\n",
      "Loss after epoch 70: 0.2474954805966771\n",
      "Epoch 71/100\n",
      "Epoch loss: 0.24723077291390202\n",
      "Loss after epoch 71: 0.24723077291390202\n",
      "Epoch 72/100\n",
      "Epoch loss: 0.24706976125792451\n",
      "Loss after epoch 72: 0.24706976125792451\n",
      "Epoch 73/100\n",
      "Epoch loss: 0.24695005108405235\n",
      "Loss after epoch 73: 0.24695005108405235\n",
      "Epoch 74/100\n",
      "Epoch loss: 0.24656989888851186\n",
      "Loss after epoch 74: 0.24656989888851186\n",
      "Epoch 75/100\n",
      "Epoch loss: 0.24635654570252205\n",
      "Loss after epoch 75: 0.24635654570252205\n",
      "Epoch 76/100\n",
      "Epoch loss: 0.24624246624460516\n",
      "Loss after epoch 76: 0.24624246624460516\n",
      "Epoch 77/100\n",
      "Epoch loss: 0.24611518536068214\n",
      "Loss after epoch 77: 0.24611518536068214\n",
      "Epoch 78/100\n",
      "Epoch loss: 0.24584117482938136\n",
      "Loss after epoch 78: 0.24584117482938136\n",
      "Epoch 79/100\n",
      "Epoch loss: 0.24584420435658638\n",
      "Loss after epoch 79: 0.24584420435658638\n",
      "Epoch 80/100\n",
      "Epoch loss: 0.245431808179789\n",
      "Loss after epoch 80: 0.245431808179789\n",
      "Epoch 81/100\n",
      "Epoch loss: 0.2453997894843062\n",
      "Loss after epoch 81: 0.2453997894843062\n",
      "Epoch 82/100\n",
      "Epoch loss: 0.24524286359061592\n",
      "Loss after epoch 82: 0.24524286359061592\n",
      "Epoch 83/100\n",
      "Epoch loss: 0.2451553028478296\n",
      "Loss after epoch 83: 0.2451553028478296\n",
      "Epoch 84/100\n",
      "Epoch loss: 0.24500984636382464\n",
      "Loss after epoch 84: 0.24500984636382464\n",
      "Epoch 85/100\n",
      "Epoch loss: 0.24481802371530012\n",
      "Loss after epoch 85: 0.24481802371530012\n",
      "Epoch 86/100\n",
      "Epoch loss: 0.24457925889852755\n",
      "Loss after epoch 86: 0.24457925889852755\n",
      "Epoch 87/100\n",
      "Epoch loss: 0.2445519725875372\n",
      "Loss after epoch 87: 0.2445519725875372\n",
      "Epoch 88/100\n",
      "Epoch loss: 0.24431668040046323\n",
      "Loss after epoch 88: 0.24431668040046323\n",
      "Epoch 89/100\n",
      "Epoch loss: 0.24424972176557427\n",
      "Loss after epoch 89: 0.24424972176557427\n",
      "Epoch 90/100\n",
      "Epoch loss: 0.24382815376594574\n",
      "Loss after epoch 90: 0.24382815376594574\n",
      "Epoch 91/100\n",
      "Epoch loss: 0.24404150820024734\n",
      "Loss after epoch 91: 0.24404150820024734\n",
      "Epoch 92/100\n",
      "Epoch loss: 0.24371281898805008\n",
      "Loss after epoch 92: 0.24371281898805008\n",
      "Epoch 93/100\n",
      "Epoch loss: 0.24373284750420973\n",
      "Loss after epoch 93: 0.24373284750420973\n",
      "Epoch 94/100\n",
      "Epoch loss: 0.24331754524954483\n",
      "Loss after epoch 94: 0.24331754524954483\n",
      "Epoch 95/100\n",
      "Epoch loss: 0.2432713312681353\n",
      "Loss after epoch 95: 0.2432713312681353\n",
      "Epoch 96/100\n",
      "Epoch loss: 0.24307462113215525\n",
      "Loss after epoch 96: 0.24307462113215525\n",
      "Epoch 97/100\n",
      "Epoch loss: 0.2428865863376392\n",
      "Loss after epoch 97: 0.2428865863376392\n",
      "Epoch 98/100\n",
      "Epoch loss: 0.242875748597687\n",
      "Loss after epoch 98: 0.242875748597687\n",
      "Epoch 99/100\n",
      "Epoch loss: 0.24283016277382904\n",
      "Loss after epoch 99: 0.24283016277382904\n",
      "Epoch 100/100\n",
      "Epoch loss: 0.2425551656112188\n",
      "Loss after epoch 100: 0.2425551656112188\n",
      "Train Accuracy: 93.355%\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:06:23.219642Z",
     "start_time": "2024-10-20T16:06:23.211429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model to a file\n",
    "import pickle\n",
    "def save_model(perceptron, filename='perceptron_model.pkl'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        # we serialize the model data (weights and bias) into a file\n",
    "        pickle.dump({'weights': perceptron.weights, 'bias': perceptron.bias}, f)\n",
    "\n",
    "# Save the model after training\n",
    "save_model(perceptron)"
   ],
   "id": "db38eb6a9530a0a",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:17:19.249430Z",
     "start_time": "2024-10-28T13:17:19.242247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model from a file\n",
    "def load_model(filename='perceptron_model.pkl'):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    perceptron = Perceptron(input_size, num_classes)\n",
    "    perceptron.weights = model_data['weights']\n",
    "    perceptron.bias = model_data['bias']\n",
    "    return perceptron"
   ],
   "id": "6650dbd0e84947f5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:17:21.426719Z",
     "start_time": "2024-10-28T13:17:21.390569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "learning_rate = 0.1\n",
    "# perceptron = Perceptron(input_size, num_classes)\n",
    "\n",
    "perceptron=load_model()"
   ],
   "id": "c306d4b510292a4a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:32:25.093489Z",
     "start_time": "2024-10-28T13:32:25.054876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test accuracy on validation data\n",
    "validation_accuracy = calculate_accuracy(process_test_x, process_test_y, perceptron)\n",
    "print(f\"Validation Accuracy: {validation_accuracy}%\")"
   ],
   "id": "b26cbb8409bfe79e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.52%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install tensorboard",
   "id": "461b201578b12695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:30:39.448120Z",
     "start_time": "2024-10-20T16:30:39.407763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Select one image for each label\n",
    "selected_images = []\n",
    "selected_labels = []\n",
    "for i in range(10):\n",
    "    for img, lbl in zip(process_train_X, process_train_Y):\n",
    "        if np.argmax(lbl) == i:\n",
    "            selected_images.append(img)\n",
    "            selected_labels.append(lbl)\n",
    "            break\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(selected_images, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(selected_labels, dtype=torch.float32)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/perceptron_experiment_labels')\n",
    "\n",
    "# Add data to TensorBoard\n",
    "writer.add_embedding(data_tensor, metadata=labels_tensor.argmax(dim=1), label_img=data_tensor.view(-1, 1, 28, 28))\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ],
   "id": "652ad655d52813f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Documents\\Facultate\\Anul3\\Sem1\\RN\\MNIST-Perceptron\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:30:03.062373Z",
     "start_time": "2024-10-20T16:30:02.008907Z"
    }
   },
   "cell_type": "code",
   "source": "%pip tensorboard --logdir=runs",
   "id": "29ac7034fcb2acd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"tensorboard\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T22:18:35.782412Z",
     "start_time": "2024-10-20T22:18:33.975124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Ensure the data is in the correct format\n",
    "process_train_X = np.array(process_train_X).reshape(-1, 784)  # Reshape to (num_samples, 784)\n",
    "process_train_Y = np.array(process_train_Y)\n",
    "\n",
    "# Visualizing the weights and cost for all iterations\n",
    "figsize(8, 6)\n",
    "plt.figure()\n",
    "\n",
    "# Plot the data points\n",
    "for i in range(10):\n",
    "    indices = np.argmax(process_train_Y, axis=1) == i\n",
    "    plt.scatter(process_train_X[indices, 0], process_train_X[indices, 1], label=f'Label {i}', marker='o')\n",
    "\n",
    "# Plot the decision boundary\n",
    "x_min, x_max = process_train_X[:, 0].min() - 1, process_train_X[:, 0].max() + 1\n",
    "y_min, y_max = process_train_X[:, 1].min() - 1, process_train_X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = perceptron.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.argmax(Z, axis=1).reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "e50ee5641f22a6ac",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (400,2) and (784,10) not aligned: 2 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m y_min, y_max \u001B[38;5;241m=\u001B[39m process_train_X[:, \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, process_train_X[:, \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     21\u001B[0m xx, yy \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmeshgrid(np\u001B[38;5;241m.\u001B[39marange(x_min, x_max, \u001B[38;5;241m0.1\u001B[39m), np\u001B[38;5;241m.\u001B[39marange(y_min, y_max, \u001B[38;5;241m0.1\u001B[39m))\n\u001B[1;32m---> 22\u001B[0m Z \u001B[38;5;241m=\u001B[39m \u001B[43mperceptron\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_\u001B[49m\u001B[43m[\u001B[49m\u001B[43mxx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m Z \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(Z, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(xx\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     25\u001B[0m plt\u001B[38;5;241m.\u001B[39mcontourf(xx, yy, Z, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n",
      "Cell \u001B[1;32mIn[5], line 11\u001B[0m, in \u001B[0;36mPerceptron.forward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m---> 11\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(z)\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (400,2) and (784,10) not aligned: 2 (dim 1) != 784 (dim 0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAH5CAYAAACSzLaMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjRklEQVR4nO3dfZBV9X348c8uC4tRd7c87XV1kdjagEqlhbCu0xnbsJM1saNUHMmOilJGauJTA7WCEmjSZmhirA/xgUmnDrVKpVhrE0vJELCJDSvKYgzPYztGUXIXkeyuosDKnt8f/rhmdUEwe2G/8HrNnGE493vu/X73ZPU9J+ceS7IsywIAABJUerQnAAAAn5SYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBklR3tCRwNXV1dsW3btjj55JOjpKTkaE8HAIAPybIs3nrrraipqYnS0gNffz0uY3bbtm1RW1t7tKcBAMDH2Lp1a5x22mkHfP24jNmTTz45It7/4VRUVBzl2QAA8GEdHR1RW1tb6LYDOS5jdv+tBRUVFWIWAKAP+7hbQn0BDACAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZAACSJWYBAEiWmAUAIFlHJGbvv//+GDFiRAwcODDq6uriueeeO+j4JUuWxMiRI2PgwIExevToWLp06QHHXnfddVFSUhJ33313L88aAIC+rugxu3jx4pgxY0bMmzcv1q5dG+eee240NjbG9u3bexy/atWqaGpqimnTpsULL7wQEydOjIkTJ8b69es/Mvbf//3f49lnn42amppiLwMAgD6o6DH793//93HttdfG1KlT46yzzooFCxbEpz71qXjooYd6HH/PPffEhRdeGLfcckuMGjUq/uZv/ib+4A/+IO67775u415//fW48cYb49FHH43+/fsXexkAAPRBRY3ZvXv3RktLSzQ0NHzwgaWl0dDQEM3NzT0e09zc3G18RERjY2O38V1dXXHVVVfFLbfcEmefffbHzmPPnj3R0dHRbQMAIH1FjdkdO3bEvn37orq6utv+6urqyOfzPR6Tz+c/dvy3vvWtKCsri5tuuumQ5jF//vyorKwsbLW1tYe5EgAA+qLknmbQ0tIS99xzTyxcuDBKSkoO6ZjZs2dHe3t7Ydu6dWuRZwkAwJFQ1JgdMmRI9OvXL1pbW7vtb21tjVwu1+MxuVzuoOOfeeaZ2L59ewwfPjzKysqirKwsXnnllZg5c2aMGDGix/csLy+PioqKbhsAAOkraswOGDAgxo4dGytWrCjs6+rqihUrVkR9fX2Px9TX13cbHxGxfPnywvirrroqfv7zn8fPfvazwlZTUxO33HJL/PCHPyzeYgAA6HPKiv0BM2bMiKuvvjrGjRsX48ePj7vvvjt27doVU6dOjYiIKVOmxKmnnhrz58+PiIibb745LrjggrjzzjvjoosuisceeyzWrFkT3/ve9yIiYvDgwTF48OBun9G/f//I5XLxmc98ptjLAQCgDyl6zE6ePDneeOONmDt3buTz+RgzZkwsW7as8CWvV199NUpLP7hAfP7558eiRYtizpw5cdttt8WZZ54ZTz75ZJxzzjnFnioAAIkpybIsO9qTONI6OjqisrIy2tvb3T8LANAHHWqvJfc0AwAA2E/MAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJEvMAgCQLDELAECyxCwAAMkSswAAJOuIxOz9998fI0aMiIEDB0ZdXV0899xzBx2/ZMmSGDlyZAwcODBGjx4dS5cuLbzW2dkZt956a4wePTpOPPHEqKmpiSlTpsS2bduKvQwAAPqYosfs4sWLY8aMGTFv3rxYu3ZtnHvuudHY2Bjbt2/vcfyqVauiqakppk2bFi+88EJMnDgxJk6cGOvXr4+IiHfeeSfWrl0bX/va12Lt2rXxxBNPxJYtW+Liiy8u9lIAAOhjSrIsy4r5AXV1dfHZz3427rvvvoiI6Orqitra2rjxxhtj1qxZHxk/efLk2LVrVzz11FOFfeedd16MGTMmFixY0ONnPP/88zF+/Ph45ZVXYvjw4R87p46OjqisrIz29vaoqKj4hCsDAKBYDrXXinpldu/evdHS0hINDQ0ffGBpaTQ0NERzc3OPxzQ3N3cbHxHR2Nh4wPEREe3t7VFSUhJVVVU9vr5nz57o6OjotgEAkL6ixuyOHTti3759UV1d3W1/dXV15PP5Ho/J5/OHNX737t1x6623RlNT0wGrff78+VFZWVnYamtrP8FqAADoa5J+mkFnZ2dcfvnlkWVZPPjggwccN3v27Ghvby9sW7duPYKzBACgWMqK+eZDhgyJfv36RWtra7f9ra2tkcvlejwml8sd0vj9IfvKK6/EypUrD3ovRXl5eZSXl3/CVQAA0FcV9crsgAEDYuzYsbFixYrCvq6urlixYkXU19f3eEx9fX238RERy5cv7zZ+f8i+9NJL8aMf/SgGDx5cnAUAANCnFfXKbETEjBkz4uqrr45x48bF+PHj4+67745du3bF1KlTIyJiypQpceqpp8b8+fMjIuLmm2+OCy64IO6888646KKL4rHHHos1a9bE9773vYh4P2Qvu+yyWLt2bTz11FOxb9++wv20gwYNigEDBhR7SQAA9BFFj9nJkyfHG2+8EXPnzo18Ph9jxoyJZcuWFb7k9eqrr0Zp6QcXiM8///xYtGhRzJkzJ2677bY488wz48knn4xzzjknIiJef/31+P73vx8REWPGjOn2WU8//XT80R/9UbGXBABAH1H058z2RZ4zCwDQt/WJ58wCAEAxiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWVHewIA/GZyX7k94tJLI0pKIrIs4oknIv/AN4/2tACOiCNyZfb++++PESNGxMCBA6Ouri6ee+65g45fsmRJjBw5MgYOHBijR4+OpUuXdns9y7KYO3dunHLKKXHCCSdEQ0NDvPTSS8VcAkCflPtRS8Rll0X06xdRWvr+n5dd9v5+gONA0WN28eLFMWPGjJg3b16sXbs2zj333GhsbIzt27f3OH7VqlXR1NQU06ZNixdeeCEmTpwYEydOjPXr1xfGfPvb34577703FixYEKtXr44TTzwxGhsbY/fu3cVeDkCfkftRy/sB25PSUkELHBdKsizLivkBdXV18dnPfjbuu+++iIjo6uqK2trauPHGG2PWrFkfGT958uTYtWtXPPXUU4V95513XowZMyYWLFgQWZZFTU1NzJw5M/7yL/8yIiLa29ujuro6Fi5cGF/60pc+dk4dHR1RWVkZ7e3tUVFR0UsrBThycl+5/f0rshHv317wYfv/0f744245AJJ0qL1W1Cuze/fujZaWlmhoaPjgA0tLo6GhIZqbm3s8prm5udv4iIjGxsbC+Jdffjny+Xy3MZWVlVFXV3fA99yzZ090dHR02wCStv8e2Z5CNuKD1y699MjOC+AIK2rM7tixI/bt2xfV1dXd9ldXV0c+n+/xmHw+f9Dx+/88nPecP39+VFZWFrba2tpPtB6APuNAEftJxwEk6rh4NNfs2bOjvb29sG3duvVoTwngN3Ood4gV904ygKOuqDE7ZMiQ6NevX7S2tnbb39raGrlcrsdjcrncQcfv//Nw3rO8vDwqKiq6bQBJe+KJ90P1QLG6/7Unnjiy8wI4wooaswMGDIixY8fGihUrCvu6urpixYoVUV9f3+Mx9fX13cZHRCxfvrww/tOf/nTkcrluYzo6OmL16tUHfE+AY03+gW9GdHW9/5cPB+3+v3d1+fIXcMwr+m0GM2bMiH/4h3+If/qnf4pNmzbFl7/85di1a1dMnTo1IiKmTJkSs2fPLoy/+eabY9myZXHnnXfG5s2b46//+q9jzZo1ccMNN0RERElJSfzFX/xF/O3f/m18//vfj3Xr1sWUKVOipqYmJk6cWOzlAPQZ+YaxHwTth3V1vf86wDGu6P8FsMmTJ8cbb7wRc+fOjXw+H2PGjIlly5YVvsD16quvRumvPSfx/PPPj0WLFsWcOXPitttuizPPPDOefPLJOOeccwpj/uqv/ip27doV06dPj7a2tvjDP/zDWLZsWQwcOLDYywHoU/INY/0XwIDjWtGfM9sXec4sAEDf1ieeMwsAAMUkZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJJVtJjduXNnXHHFFVFRURFVVVUxbdq0ePvttw96zO7du+P666+PwYMHx0knnRSTJk2K1tbWwusvvvhiNDU1RW1tbZxwwgkxatSouOeee4q1BAAA+riixewVV1wRGzZsiOXLl8dTTz0VP/nJT2L69OkHPearX/1q/OAHP4glS5bEj3/849i2bVtceumlhddbWlpi2LBh8cgjj8SGDRvi9ttvj9mzZ8d9991XrGUAANCHlWRZlvX2m27atCnOOuuseP7552PcuHEREbFs2bL44he/GK+99lrU1NR85Jj29vYYOnRoLFq0KC677LKIiNi8eXOMGjUqmpub47zzzuvxs66//vrYtGlTrFy58pDn19HREZWVldHe3h4VFRWfYIUAABTTofZaUa7MNjc3R1VVVSFkIyIaGhqitLQ0Vq9e3eMxLS0t0dnZGQ0NDYV9I0eOjOHDh0dzc/MBP6u9vT0GDRp00Pns2bMnOjo6um0AAKSvKDGbz+dj2LBh3faVlZXFoEGDIp/PH/CYAQMGRFVVVbf91dXVBzxm1apVsXjx4o+9fWH+/PlRWVlZ2Gpraw99MQAA9FmHFbOzZs2KkpKSg26bN28u1ly7Wb9+fVxyySUxb968+PznP3/QsbNnz4729vbCtnXr1iMyRwAAiqvscAbPnDkzrrnmmoOOOeOMMyKXy8X27du77X/vvfdi586dkcvlejwul8vF3r17o62trdvV2dbW1o8cs3HjxpgwYUJMnz495syZ87HzLi8vj/Ly8o8dBwBAWg4rZocOHRpDhw792HH19fXR1tYWLS0tMXbs2IiIWLlyZXR1dUVdXV2Px4wdOzb69+8fK1asiEmTJkVExJYtW+LVV1+N+vr6wrgNGzbE5z73ubj66qvjm9/85uFMHwCAY0xRnmYQEfGFL3whWltbY8GCBdHZ2RlTp06NcePGxaJFiyIi4vXXX48JEybEww8/HOPHj4+IiC9/+cuxdOnSWLhwYVRUVMSNN94YEe/fGxvx/q0Fn/vc56KxsTHuuOOOwmf169fvkCJ7P08zAADo2w611w7ryuzhePTRR+OGG26ICRMmRGlpaUyaNCnuvffewuudnZ2xZcuWeOeddwr77rrrrsLYPXv2RGNjYzzwwAOF1x9//PF444034pFHHolHHnmksP/000+PX/ziF8VaCgAAfVTRrsz2Za7MAgD0bUf1ObMAAHAkiFkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkiVkAAJIlZgEASJaYBQAgWWIWAIBkFS1md+7cGVdccUVUVFREVVVVTJs2Ld5+++2DHrN79+64/vrrY/DgwXHSSSfFpEmTorW1tcexb775Zpx22mlRUlISbW1tRVgBAAB9XdFi9oorrogNGzbE8uXL46mnnoqf/OQnMX369IMe89WvfjV+8IMfxJIlS+LHP/5xbNu2LS699NIex06bNi1+7/d+rxhTBwAgESVZlmW9/aabNm2Ks846K55//vkYN25cREQsW7YsvvjFL8Zrr70WNTU1Hzmmvb09hg4dGosWLYrLLrssIiI2b94co0aNiubm5jjvvPMKYx988MFYvHhxzJ07NyZMmBC/+tWvoqqq6pDn19HREZWVldHe3h4VFRW/2WIBAOh1h9prRbky29zcHFVVVYWQjYhoaGiI0tLSWL16dY/HtLS0RGdnZzQ0NBT2jRw5MoYPHx7Nzc2FfRs3boxvfOMb8fDDD0dp6aFNf8+ePdHR0dFtAwAgfUWJ2Xw+H8OGDeu2r6ysLAYNGhT5fP6AxwwYMOAjV1irq6sLx+zZsyeamprijjvuiOHDhx/yfObPnx+VlZWFrba29vAWBABAn3RYMTtr1qwoKSk56LZ58+ZizTVmz54do0aNiiuvvPKwj2tvby9sW7duLdIMAQA4ksoOZ/DMmTPjmmuuOeiYM844I3K5XGzfvr3b/vfeey927twZuVyux+NyuVzs3bs32traul2dbW1tLRyzcuXKWLduXTz++OMREbH/dt8hQ4bE7bffHl//+td7fO/y8vIoLy8/lCUCAJCQw4rZoUOHxtChQz92XH19fbS1tUVLS0uMHTs2It4P0a6urqirq+vxmLFjx0b//v1jxYoVMWnSpIiI2LJlS7z66qtRX18fERH/9m//Fu+++27hmOeffz7+7M/+LJ555pn47d/+7cNZCgAAx4DDitlDNWrUqLjwwgvj2muvjQULFkRnZ2fccMMN8aUvfanwJIPXX389JkyYEA8//HCMHz8+KisrY9q0aTFjxowYNGhQVFRUxI033hj19fWFJxl8OFh37NhR+LzDeZoBAADHhqLEbETEo48+GjfccENMmDAhSktLY9KkSXHvvfcWXu/s7IwtW7bEO++8U9h31113Fcbu2bMnGhsb44EHHijWFAEASFxRnjPb13nOLABA33ZUnzMLAABHgpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSJWQAAkiVmAQBIlpgFACBZYhYAgGSVHe0JHA1ZlkVEREdHx1GeCQAAPdnfafu77UCOy5h96623IiKitrb2KM8EAICDeeutt6KysvKAr5dkH5e7x6Curq7Ytm1bnHzyyVFSUnK0p3NM6OjoiNra2ti6dWtUVFQc7elwmJy/9DmH6XMO0+b89b4sy+Ktt96KmpqaKC098J2xx+WV2dLS0jjttNOO9jSOSRUVFX6JE+b8pc85TJ9zmDbnr3cd7Irsfr4ABgBAssQsAADJErP0ivLy8pg3b16Ul5cf7anwCTh/6XMO0+ccps35O3qOyy+AAQBwbHBlFgCAZIlZAACSJWYBAEiWmAUAIFliFgCAZIlZDsnOnTvjiiuuiIqKiqiqqopp06bF22+/fdBjdu/eHddff30MHjw4TjrppJg0aVK0trb2OPbNN9+M0047LUpKSqKtra0IK6AY5/DFF1+MpqamqK2tjRNOOCFGjRoV99xzT7GXcty4//77Y8SIETFw4MCoq6uL55577qDjlyxZEiNHjoyBAwfG6NGjY+nSpd1ez7Is5s6dG6ecckqccMIJ0dDQEC+99FIxl3Bc683z19nZGbfeemuMHj06TjzxxKipqYkpU6bEtm3bir2M41pv/w7+uuuuuy5KSkri7rvv7uVZH4cyOAQXXnhhdu6552bPPvts9swzz2S/8zu/kzU1NR30mOuuuy6rra3NVqxYka1ZsyY777zzsvPPP7/HsZdcckn2hS98IYuI7Fe/+lURVkAxzuE//uM/ZjfddFP23//939n//d//Zf/8z/+cnXDCCdl3v/vdYi/nmPfYY49lAwYMyB566KFsw4YN2bXXXptVVVVlra2tPY7/6U9/mvXr1y/79re/nW3cuDGbM2dO1r9//2zdunWFMX/3d3+XVVZWZk8++WT24osvZhdffHH26U9/Onv33XeP1LKOG719/tra2rKGhoZs8eLF2ebNm7Pm5uZs/Pjx2dixY4/kso4rxfgd3O+JJ57Izj333Kympia76667irySY5+Y5WNt3Lgxi4js+eefL+z7r//6r6ykpCR7/fXXezymra0t69+/f7ZkyZLCvk2bNmURkTU3N3cb+8ADD2QXXHBBtmLFCjFbJMU+h7/uK1/5SvbHf/zHvTf549T48eOz66+/vvD3ffv2ZTU1Ndn8+fN7HH/55ZdnF110Ubd9dXV12Z//+Z9nWZZlXV1dWS6Xy+64447C621tbVl5eXn2L//yL0VYwfGtt89fT5577rksIrJXXnmldyZNN8U6h6+99lp26qmnZuvXr89OP/10MdsL3GbAx2pubo6qqqoYN25cYV9DQ0OUlpbG6tWrezympaUlOjs7o6GhobBv5MiRMXz48Ghubi7s27hxY3zjG9+Ihx9+OEpL/c+xWIp5Dj+svb09Bg0a1HuTPw7t3bs3Wlpauv3sS0tLo6Gh4YA/++bm5m7jIyIaGxsL419++eXI5/PdxlRWVkZdXd1BzyeHrxjnryft7e1RUlISVVVVvTJvPlCsc9jV1RVXXXVV3HLLLXH22WcXZ/LHIfXAx8rn8zFs2LBu+8rKymLQoEGRz+cPeMyAAQM+8g/Z6urqwjF79uyJpqamuOOOO2L48OFFmTvvK9Y5/LBVq1bF4sWLY/r06b0y7+PVjh07Yt++fVFdXd1t/8F+9vl8/qDj9/95OO/JJ1OM8/dhu3fvjltvvTWampqioqKidyZOQbHO4be+9a0oKyuLm266qfcnfRwTs8exWbNmRUlJyUG3zZs3F+3zZ8+eHaNGjYorr7yyaJ9xrDva5/DXrV+/Pi655JKYN29efP7znz8inwnHo87Ozrj88ssjy7J48MEHj/Z0OEQtLS1xzz33xMKFC6OkpORoT+eYUna0J8DRM3PmzLjmmmsOOuaMM86IXC4X27dv77b/vffei507d0Yul+vxuFwuF3v37o22trZuV/ZaW1sLx6xcuTLWrVsXjz/+eES8/03riIghQ4bE7bffHl//+tc/4cqOH0f7HO63cePGmDBhQkyfPj3mzJnzidbCB4YMGRL9+vX7yNM/evrZ75fL5Q46fv+fra2tccopp3QbM2bMmF6cPcU4f/vtD9lXXnklVq5c6apskRTjHD7zzDOxffv2bv9P5L59+2LmzJlx9913xy9+8YveXcTx5GjftEvft//LQ2vWrCns++EPf3hIXx56/PHHC/s2b97c7ctD//u//5utW7eusD300ENZRGSrVq064LdF+WSKdQ6zLMvWr1+fDRs2LLvllluKt4Dj0Pjx47Mbbrih8Pd9+/Zlp5566kG/fPInf/In3fbV19d/5Atg3/nOdwqvt7e3+wJYkfT2+cuyLNu7d282ceLE7Oyzz862b99enIlT0NvncMeOHd3+nbdu3bqspqYmu/XWW7PNmzcXbyHHATHLIbnwwguz3//9389Wr16d/c///E925plndnus02uvvZZ95jOfyVavXl3Yd91112XDhw/PVq5cma1Zsyarr6/P6uvrD/gZTz/9tKcZFFExzuG6deuyoUOHZldeeWX2y1/+srD5F+1v7rHHHsvKy8uzhQsXZhs3bsymT5+eVVVVZfl8PsuyLLvqqquyWbNmFcb/9Kc/zcrKyrLvfOc72aZNm7J58+b1+Giuqqqq7D/+4z+yn//859kll1zi0VxF0tvnb+/evdnFF1+cnXbaadnPfvazbr9ve/bsOSprPNYV43fwwzzNoHeIWQ7Jm2++mTU1NWUnnXRSVlFRkU2dOjV76623Cq+//PLLWURkTz/9dGHfu+++m33lK1/Jfuu3fiv71Kc+lf3pn/5p9stf/vKAnyFmi6sY53DevHlZRHxkO/3004/gyo5d3/3ud7Phw4dnAwYMyMaPH589++yzhdcuuOCC7Oqrr+42/l//9V+z3/3d380GDBiQnX322dl//ud/dnu9q6sr+9rXvpZVV1dn5eXl2YQJE7ItW7YciaUcl3rz/O3//exp+/XfWXpXb/8OfpiY7R0lWfb/b1QEAIDEeJoBAADJErMAACRLzAIAkCwxCwBAssQsAADJErMAACRLzAIAkCwxCwBAssQsAADJErMAACRLzAIAkKz/B1CfXWzxvCNaAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T16:28:07.303904Z",
     "start_time": "2024-10-28T16:28:07.280422Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0]\n",
      " [1 0 0 1 1 1]]\n",
      "[[1 2 3 4 5 6]\n",
      " [6 5 4 3 2 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,6) and (2,6) not aligned: 6 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m b_transposed \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# print(b_transposed)\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m c \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(c)\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (2,6) and (2,6) not aligned: 6 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "execution_count": 22,
   "source": [
    "a = np.array([[0,0,1,1,1,0],[1,0,0,1,1,1]])\n",
    "print(a)\n",
    "b = np.array([[1,2,3,4,5,6],[6,5,4,3,2,1]])\n",
    "print(b)\n",
    "# Transpose b to match the shape for dot product\n",
    "b_transposed = b.T\n",
    "# print(b_transposed)\n",
    "c = np.dot(a,b)\n",
    "print(c)"
   ],
   "id": "8761fad8192e6f63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
