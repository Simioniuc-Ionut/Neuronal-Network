{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:04:00.308508Z",
     "start_time": "2024-12-30T20:03:59.713151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from random import random\n",
    "\n",
    "import flappy_bird_gymnasium\n",
    "from gymnasium import envs\n",
    "\n",
    "print(envs.registry.keys())\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "# Configurare\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=True,)"
   ],
   "id": "f47fd294ca39805a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0', 'FlappyBird-v0'])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:07:46.548577Z",
     "start_time": "2024-12-30T20:07:46.542873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Q-Learning\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 100\n"
   ],
   "id": "dfb3bd62d7b9c339",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:04:10.589735Z",
     "start_time": "2024-12-30T20:04:01.447906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),              # Conversie la imagine PIL\n",
    "    transforms.Grayscale(num_output_channels=1),               # Convertire în tonuri de gri\n",
    "    transforms.Resize((84,84)),         # Redimensionare la 84x84 pixeli\n",
    "    transforms.ToTensor() # Conversie la tensor PyTorch\n",
    "])"
   ],
   "id": "fda7602667fa248a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:04:10.605529Z",
     "start_time": "2024-12-30T20:04:10.597750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)  # Buffer cu lungime maximă\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)  # Adaugă experiența (state, action, reward, next_state, done)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)  # Selectează un eșantion aleatoriu\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ],
   "id": "ccd923fe7cd071c9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9ae205d95d914feb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:04:21.849523Z",
     "start_time": "2024-12-30T20:04:21.844998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "replay_buffer = deque(maxlen=20000)  # Buffer pentru stocarea tranzițiilor\n"
   ],
   "id": "b733ea4dc8bae42d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:04:22.389264Z",
     "start_time": "2024-12-30T20:04:22.383168Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.action_space)",
   "id": "edb778d9a4f7ec47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:53:06.788644Z",
     "start_time": "2024-12-30T20:52:45.416182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import keyboard  # Pentru a detecta apăsările de taste\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Configurare environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # Joc manual\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count_frames=0\n",
    "\n",
    "    # Inițializează primul frame procesat\n",
    "    frame = env.render()\n",
    "    processed_current_frame = preprocess(frame)     \n",
    "    while not done:\n",
    "        \n",
    "        # Joacă manual (space = jump, altfel = stay)\n",
    "        # if keyboard.is_pressed(\"space\"):\n",
    "        #     action = 1  # Jump\n",
    "        # else:\n",
    "        #     action = 0 \n",
    "        # Stay\n",
    "        if np.random.rand() < 0.1:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0 \n",
    "    \n",
    "        new_state, reward, done,_, info = env.step(action)\n",
    "        \n",
    "        next_frame = env.render()\n",
    "        \n",
    "        processed_next_frame = preprocess(next_frame)\n",
    "        # Extrage array-ul de 12 valori din new_state\n",
    "        \"\"\"\n",
    "        new_state contains :\n",
    "        'last_pipe_horizontal_position',\n",
    "        'last_top_pipe_vertical_position',\n",
    "        'last_bottom_pipe_vertical_position'\n",
    "        'next_pipe_horizontal_position'\n",
    "        'next_top_pipe_vertical_position'\n",
    "        'next_bottom_pipe_vertical_position',\n",
    "        'next_next_pipe_horizontal_position'\n",
    "        'next_next_top_pipe_vertical_position',\n",
    "        'next_next_bottom_pipe_vertical_position',\n",
    "        'player_vertical_position',\n",
    "        'player_vertical_velocity',\n",
    "        'player_rotation'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        if count_frames % 4 == 0 or done:\n",
    "            replay_buffer.append((processed_current_frame, action, reward,processed_next_frame, new_state, done))\n",
    "        \n",
    "        # Actualize for next iteration\n",
    "        processed_current_frame = processed_next_frame\n",
    "        count_frames += 1\n",
    "        if done:\n",
    "            break\n",
    "    "
   ],
   "id": "a13aed199d28d01f",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[64], line 38\u001B[0m\n\u001B[0;32m     34\u001B[0m new_state, reward, done,_, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     36\u001B[0m next_frame \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[1;32m---> 38\u001B[0m processed_next_frame \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_frame\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Extrage array-ul de 12 valori din new_state\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03mnew_state contains :\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m'last_pipe_horizontal_position',\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m'player_rotation'\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    475\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    476\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[1;32m--> 477\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpil_interpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot inappropriate size arg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2365\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2353\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2354\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce(factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2355\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce)\n\u001B[0;32m   2356\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2357\u001B[0m         )\n\u001B[0;32m   2358\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2359\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2360\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2361\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2362\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2363\u001B[0m         )\n\u001B[1;32m-> 2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:53:08.361223Z",
     "start_time": "2024-12-30T20:53:08.354793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "# print(f\"buffer one example\",replay_buffer[0],len(replay_buffer[0]))\n"
   ],
   "id": "97ff3369e2affe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 4855\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Configurare environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\" , use_lidar=False)\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Joc manual\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count_frames=0\n",
    "    \n",
    "    frame =env.render() \n",
    "    print(\" frame \" ,frame)\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Joacă manual (space = jump, altfel = stay)\n",
    "        if keyboard.is_pressed(\"space\"):\n",
    "            action = 1  # Jump\n",
    "        else:\n",
    "            action = 0 \n",
    "\n",
    "        new_state, reward, done, _ , info = env.step(action)  \n",
    "        print(\"shape\",new_state.shape)\n",
    "        print(\"new state \",new_state )\n",
    "        \n",
    "        # Randează jocul\n",
    "        frame =env.render() \n",
    "        print(\" frame \" ,frame)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "                "
   ],
   "id": "82d1095cce7db3cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Assuming `processed_frame` is the grayscale image tensor\n",
    "\n",
    "if len(replay_buffer) > 0:\n",
    "    first_img = replay_buffer[0][0].squeeze(0)\n",
    "else:\n",
    "    first_img = None\n",
    "    \n",
    "def show_grayscale_image(image_tensor):\n",
    "    plt.imshow(image_tensor.numpy(), cmap='gray')\n",
    "    plt.title('Grayscale Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.pause(0.1)  # Pause to allow the image to be displayed"
   ],
   "id": "6a47b4215b8319e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "for experience in replay_buffer:\n",
    "    img = experience[0].squeeze(0)  # Extract the grayscale image tensor\n",
    "    show_grayscale_image(img)\n",
    "    img = experience[3].squeeze(0)  # Extract the grayscale image tensor\n",
    "    show_grayscale_image(img)"
   ],
   "id": "7a731fe33fc046d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:06:10.441558Z",
     "start_time": "2024-12-30T20:06:10.432243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CNN\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Define the layers of the convolutional neural network.\n",
    "\n",
    "    Parameters:\n",
    "    in_channels: int\n",
    "        The number of channels in the input image. For MNIST, this is 1 (grayscale images).\n",
    "    num_classes: int\n",
    "        The number of classes we want to predict, in our case 12 .\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_classes=12):\n",
    "        super().__init__()\n",
    "        # First convolutional layer: 1 input channel, 8 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
    "        # Max pooling layer: 2x2 window, stride 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # Second convolutional layerl: 8 input channels, 16 output channels, 3x3 kernel, stride 1 , padding1\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16, kernel_size=5, stride=1,padding=0)\n",
    "        # Second convolutional layerl: 8 input channels, 16 output channels, 3x3 kernel, stride 1 , padding1\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=32, kernel_size=5, stride=1,padding=0)\n",
    "        # Fully connected layer: 16 * 7 * 7  input features (after two 2x2 poolings), state output features (num_classes) \n",
    "        self.fc1 = nn.Linear(32*7*7,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            x: torch.Tensor\n",
    "                The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "                The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
    "        x = self.pool(x)               # Apply max pooling\n",
    "        x = torch.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
    "        x = self.pool(x)               # Apply max pooling\n",
    "        x = torch.relu(self.conv3(x))  # Apply third convolution and ReLU activation\n",
    "        x = self.pool(x)               # Apply max pooling\n",
    "        x = x.view(x.size(0), -1)      # Flatten the tensor\n",
    "        x = self.fc1(x)                # Apply fully connected layer\n",
    "        return x\n",
    "    "
   ],
   "id": "8cbe98e1b476f871",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:06:10.883965Z",
     "start_time": "2024-12-30T20:06:10.877655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = 7056  # 84x84 pixels (not directly used in CNN)\n",
    "num_classes = 12 # 12 characteristics from use_lidar = False\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10  "
   ],
   "id": "3faa9ded6d9cae09",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:06:20.827630Z",
     "start_time": "2024-12-30T20:06:20.818661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "def train_feature_extractor(model, replay_buffer, batch_size, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Antrenează rețeaua CNN pentru extragerea caracteristicilor.\n",
    "\n",
    "    Parameters:\n",
    "        model: nn.Module\n",
    "            Modelul CNN pentru extragerea caracteristicilor.\n",
    "        replay_buffer: deque\n",
    "            Lista cu experiențe (frame-uri, acțiuni, recompense, next_frame-uri, state-uri și done flag).\n",
    "        batch_size: int\n",
    "            Dimensiunea batch-ului.\n",
    "        num_epochs: int\n",
    "            Numărul de epoci.\n",
    "        learning_rate: float\n",
    "            Rata de învățare.\n",
    "    \"\"\"\n",
    "    # Pregătește datele pentru antrenare\n",
    "    frames = [experience[3] for experience in replay_buffer]  # Frame-urile curente\n",
    "    targets = [experience[4] for experience in replay_buffer]  # Target-ul (new_state)\n",
    "\n",
    "    # Convert frames și targets în tensors\n",
    "    frames = torch.tensor(np.array(frames), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
    "\n",
    "    # Creează un dataset și un dataloader cu shuffle pe episoade\n",
    "    dataset = TensorDataset(frames, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inițializează pierderea și optimizer-ul\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Activează modul de antrenare\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)  # Pierdere față de ținte\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ],
   "id": "debf6b96aa7cd41f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:06:19.330788Z",
     "start_time": "2024-12-30T20:06:19.298030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# [bird_y, bird_velocity, pipe_x1, pipe_x2]\n",
    "feature_dim = 12  # Dimensiunea vectorului de caracteristici\n",
    "model_actiune_curenta = CNN(in_channels=1, num_classes=feature_dim)  # Creare model CNN\n"
   ],
   "id": "2e3e7ed7416cdeb8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:27.530703Z",
     "start_time": "2024-12-30T20:09:02.445711Z"
    }
   },
   "cell_type": "code",
   "source": "train_feature_extractor(model_actiune_curenta, replay_buffer, batch_size, 50, learning_rate)  # Antrenare model\n",
   "id": "186a5cde039669b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.2567\n",
      "Epoch [2/50], Loss: 1.1026\n",
      "Epoch [3/50], Loss: 1.0455\n",
      "Epoch [4/50], Loss: 0.9804\n",
      "Epoch [5/50], Loss: 0.9094\n",
      "Epoch [6/50], Loss: 0.8698\n",
      "Epoch [7/50], Loss: 0.8004\n",
      "Epoch [8/50], Loss: 0.7272\n",
      "Epoch [9/50], Loss: 0.6817\n",
      "Epoch [10/50], Loss: 0.6478\n",
      "Epoch [11/50], Loss: 0.6069\n",
      "Epoch [12/50], Loss: 0.5804\n",
      "Epoch [13/50], Loss: 0.5425\n",
      "Epoch [14/50], Loss: 0.5126\n",
      "Epoch [15/50], Loss: 0.4963\n",
      "Epoch [16/50], Loss: 0.4890\n",
      "Epoch [17/50], Loss: 0.4557\n",
      "Epoch [18/50], Loss: 0.4288\n",
      "Epoch [19/50], Loss: 0.4259\n",
      "Epoch [20/50], Loss: 0.4077\n",
      "Epoch [21/50], Loss: 0.3981\n",
      "Epoch [22/50], Loss: 0.3763\n",
      "Epoch [23/50], Loss: 0.3694\n",
      "Epoch [24/50], Loss: 0.3631\n",
      "Epoch [25/50], Loss: 0.3529\n",
      "Epoch [26/50], Loss: 0.3520\n",
      "Epoch [27/50], Loss: 0.3370\n",
      "Epoch [28/50], Loss: 0.3324\n",
      "Epoch [29/50], Loss: 0.3355\n",
      "Epoch [30/50], Loss: 0.3243\n",
      "Epoch [31/50], Loss: 0.3161\n",
      "Epoch [32/50], Loss: 0.3094\n",
      "Epoch [33/50], Loss: 0.3073\n",
      "Epoch [34/50], Loss: 0.3068\n",
      "Epoch [35/50], Loss: 0.2959\n",
      "Epoch [36/50], Loss: 0.2985\n",
      "Epoch [37/50], Loss: 0.2849\n",
      "Epoch [38/50], Loss: 0.2878\n",
      "Epoch [39/50], Loss: 0.2895\n",
      "Epoch [40/50], Loss: 0.2809\n",
      "Epoch [41/50], Loss: 0.2813\n",
      "Epoch [42/50], Loss: 0.2834\n",
      "Epoch [43/50], Loss: 0.2792\n",
      "Epoch [44/50], Loss: 0.2691\n",
      "Epoch [45/50], Loss: 0.2603\n",
      "Epoch [46/50], Loss: 0.2634\n",
      "Epoch [47/50], Loss: 0.2608\n",
      "Epoch [48/50], Loss: 0.2568\n",
      "Epoch [49/50], Loss: 0.2541\n",
      "Epoch [50/50], Loss: 0.2504\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:29.425870Z",
     "start_time": "2024-12-30T20:11:29.415519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_feature_extractor(model, validation_data, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Evaluează modelul pe un set de validare și calculează MSE și acuratețea.\n",
    "\n",
    "    Parameters:\n",
    "        model: nn.Module\n",
    "            Modelul CNN antrenat.\n",
    "        validation_data: list of tuples\n",
    "            Lista de (frame, target) pentru validare.\n",
    "        tolerance: float\n",
    "            Intervalul de toleranță pentru a considera o predicție corectă.\n",
    "\n",
    "    Returns:\n",
    "        mse: float\n",
    "            Mean Squared Error pe setul de validare.\n",
    "        accuracy: float\n",
    "            Acuratețea în procente.\n",
    "    \"\"\"\n",
    "    model.eval()  # Mod de evaluare (fără actualizare a ponderilor)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Nu calculăm gradientele\n",
    "        for frame, target in validation_data:\n",
    "            frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0)  # Adaugă dimensiunea batch\n",
    "            target = torch.tensor(target, dtype=torch.float32).unsqueeze(0)\n",
    "            prediction = model(frame)\n",
    "\n",
    "            loss = mse_loss(prediction, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Verificăm dacă predicția este în toleranță\n",
    "            if torch.all(torch.abs(prediction - target) < tolerance):\n",
    "                correct_predictions += 1\n",
    "\n",
    "    mse = total_loss / len(validation_data)\n",
    "    accuracy = (correct_predictions / len(validation_data)) * 100\n",
    "    return mse, accuracy\n"
   ],
   "id": "f7047f217906a4",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:30.112110Z",
     "start_time": "2024-12-30T20:11:30.107433Z"
    }
   },
   "cell_type": "code",
   "source": "validation_data = []",
   "id": "7f2947c34afb5ce6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:47.995105Z",
     "start_time": "2024-12-30T20:11:41.398907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import keyboard  # Pentru a detecta apăsările de taste\n",
    "\n",
    "# Extract validation data\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Configurare environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Joc manual\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count_frames=0\n",
    "\n",
    "    # Inițializează primul frame procesat\n",
    "    frame = env.render()\n",
    "        \n",
    "    while not done:\n",
    "        \n",
    "        # Joacă manual (space = jump, altfel = stay)\n",
    "        # if keyboard.is_pressed(\"space\"):\n",
    "        #     action = 1  # Jump\n",
    "        # else:\n",
    "        #     action = 0 \n",
    "        # Stay\n",
    "        if np.random.rand() < 0.1:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0 \n",
    "    \n",
    "        new_state, reward, done,_, info = env.step(action)\n",
    "        \n",
    "        next_frame = env.render()\n",
    "        \n",
    "        processed_next_frame = preprocess(next_frame)\n",
    "        # Extrage array-ul de 12 valori din new_state\n",
    "        \"\"\"\n",
    "        new_state contains :\n",
    "        'last_pipe_horizontal_position',\n",
    "        'last_top_pipe_vertical_position',\n",
    "        'last_bottom_pipe_vertical_position'\n",
    "        'next_pipe_horizontal_position'\n",
    "        'next_top_pipe_vertical_position'\n",
    "        'next_bottom_pipe_vertical_position',\n",
    "        'next_next_pipe_horizontal_position'\n",
    "        'next_next_top_pipe_vertical_position',\n",
    "        'next_next_bottom_pipe_vertical_position',\n",
    "        'player_vertical_position',\n",
    "        'player_vertical_velocity',\n",
    "        'player_rotation'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        if count_frames % 4 == 0 or done:\n",
    "            validation_data.append((processed_next_frame,new_state))\n",
    "        \n",
    "        count_frames += 1\n",
    "        if done:\n",
    "            break"
   ],
   "id": "f92fb66b068b6db8",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:48.404332Z",
     "start_time": "2024-12-30T20:11:48.392481Z"
    }
   },
   "cell_type": "code",
   "source": "len(validation_data)",
   "id": "cda7e7828e68b991",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:54.537831Z",
     "start_time": "2024-12-30T20:11:53.958594Z"
    }
   },
   "cell_type": "code",
   "source": "validate_feature_extractor(model_actiune_curenta, validation_data, tolerance=0.3)  # Validare model",
   "id": "8613670373ac9c90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_21800\\1639196621.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0)  # Adaugă dimensiunea batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.006073929454982539, 79.04761904761905)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Actor-Critic , nefunctional",
   "id": "4e640b1f357f864d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 1: Definirea Rețelelor Actor și Critic",
   "id": "3dfbd11b929bc47e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:11:57.949596Z",
     "start_time": "2024-12-30T20:11:57.940831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Probabilitățile pentru acțiuni\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Valoarea scalară a stării\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n"
   ],
   "id": "c657cff3a3f2d8ac",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 2: Inițializarea Mediului și Rețelelor",
   "id": "64beaaab9626be1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:12:39.170177Z",
     "start_time": "2024-12-30T20:12:39.159489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dimensiunea state-ului (12 caracteristici)\n",
    "state_dim = 12\n",
    "# Dimensiunea spațiului de acțiuni (2 acțiuni: flap sau no flap)\n",
    "action_dim = 2\n",
    "\n",
    "# Inițializăm Actorul și Criticul\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "# Optimizatori\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "# Funcția de pierdere pentru Critic\n",
    "critic_loss_fn = nn.MSELoss()\n"
   ],
   "id": "db7c2670355eb47d",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 3: Funcții de Ajutor",
   "id": "5263776b4388c18e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:43:38.182270Z",
     "start_time": "2024-12-30T20:43:38.175240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Alegerea unei acțiuni pe baza politicii:\n",
    "def select_action(state, actor):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convertim în tensor\n",
    "    action_probs = actor(state_tensor)\n",
    "    action = torch.multinomial(action_probs, 1).item()  # Alegem o acțiune\n",
    "    \n",
    "    return action, action_probs\n",
    "\n",
    "def select_action_without_explore(state, actor):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convertim în tensor\n",
    "    action_probs = actor(state_tensor)\n",
    "    action = torch.argmax(action_probs).item()  # Alegem acțiunea cu probabilitatea cea mai mare\n",
    "\n",
    "    return action, action_probs\n"
   ],
   "id": "d3edc8ec5c2d58e0",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:05:53.940749Z",
     "start_time": "2024-12-30T21:05:53.934284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Actualizarea Actorului:\n",
    "def update_actor(actor, actor_optimizer, log_prob, advantage):\n",
    "    advantage = torch.tensor(advantage, dtype=torch.float32)\n",
    "    actor_loss = -(log_prob * advantage.detach()).mean()  # Gradientul politicii\n",
    "    \n",
    "    # Adăugăm regularizarea entropiei pentru explorare \n",
    "    # entropy_loss = -torch.sum(action_probs * torch.log(action_probs))\n",
    "    # actor_loss += 0.01 * entropy_loss  # Adaugă regularizarea entropiei\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "    actor_optimizer.step()\n"
   ],
   "id": "f1b567ed56e055a0",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:15:51.009252Z",
     "start_time": "2024-12-30T20:15:51.003170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Actualizarea Criticului:\n",
    "def update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    td_target_tensor = torch.FloatTensor([td_target])\n",
    "    value = critic(state_tensor).squeeze(0)\n",
    "    loss = critic_loss_fn(value, td_target_tensor)\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    critic_optimizer.step()\n"
   ],
   "id": "5ac33fbd16c7f553",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 4: Ciclu Principal de Învățare",
   "id": "1f8323e7fd068250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# vers 1\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparametrii\n",
    "gamma = 0.99  # Discount factor\n",
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # Resetăm mediul (simulat sau real)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # 1. Alegem o acțiune\n",
    "        action, action_probs = select_action(state, actor)\n",
    "        log_prob = torch.log(action_probs[0, action])\n",
    "\n",
    "        # 2. Executăm acțiunea și primim noua stare și recompensa\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward \n",
    "        \n",
    "        # 3. Calculăm valoarea TD Target și Advantage\n",
    "        value = critic(torch.FloatTensor(state).unsqueeze(0)).item()\n",
    "        next_value = critic(torch.FloatTensor(next_state).unsqueeze(0)).item() if not done else 0\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # 4. Actualizăm Actorul și Criticul\n",
    "        update_actor(actor, actor_optimizer, log_prob, advantage)\n",
    "        update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn)\n",
    "\n",
    "        # 5. Trecem la următoarea stare\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n"
   ],
   "id": "87638a032e565082",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:48:21.791057Z",
     "start_time": "2024-12-30T20:45:03.416153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vers 2\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparametrii\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.3 # Valoarea inițială pentru explorare\n",
    "epsilon_min = 0.1  # Valoarea minimă pentru explorare\n",
    "epsilon_decay = 0.995  # Rata de scădere a epsilonului\n",
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # Resetăm mediul\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Explorare epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Alegem o acțiune aleatorie pentru explorare\n",
    "            action = np.random.choice([0, 1])\n",
    "            log_prob = None  # Nu avem log_prob pentru acțiuni exploratorii\n",
    "        else:\n",
    "            # Alegem acțiunea pe baza actorului\n",
    "            action, action_probs = select_action(state, actor)\n",
    "            log_prob = torch.log(action_probs[0, action])\n",
    "\n",
    "        # Executăm acțiunea în mediul de simulare\n",
    "        next_state, reward, done, _ , _ = env.step(action)\n",
    "        \n",
    "        # Calculul centrului pipe-ului următor\n",
    "        next_pipe_center = (state[4] + state[5]) / 2\n",
    "        distance_from_center = abs(state[9] - next_pipe_center)\n",
    "\n",
    "        # Penalizare pentru distanță față de centru\n",
    "        reward -= distance_from_center * 0.01\n",
    "        \n",
    "        # Calculăm TD Target și Advantage\n",
    "        value = critic(torch.FloatTensor(state).unsqueeze(0)).item()\n",
    "        next_value = critic(torch.FloatTensor(next_state).unsqueeze(0)).item() if not done else 0\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # Actualizăm Actorul doar dacă acțiunea este determinată de model\n",
    "        if log_prob is not None:\n",
    "            update_actor(actor, actor_optimizer, log_prob, advantage)\n",
    "\n",
    "        # Actualizăm Criticul\n",
    "        update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn)\n",
    "\n",
    "        # Actualizăm starea curentă\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # Decay pentru epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n"
   ],
   "id": "faacefb3a967cf20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Reward: 6.618964843750001\n",
      "Episode 2/100, Reward: 3.7777148437500006\n",
      "Episode 3/100, Reward: 0.7657226562500001\n",
      "Episode 4/100, Reward: -4.052578125\n",
      "Episode 5/100, Reward: -0.4299023437500006\n",
      "Episode 6/100, Reward: 3.8028124999999986\n",
      "Episode 7/100, Reward: 3.81048828125\n",
      "Episode 8/100, Reward: -1.65126953125\n",
      "Episode 9/100, Reward: -3.4380859374999986\n",
      "Episode 10/100, Reward: 1.3847265625\n",
      "Episode 11/100, Reward: 3.84712890625\n",
      "Episode 12/100, Reward: -0.43164062499999867\n",
      "Episode 13/100, Reward: 3.8341015625\n",
      "Episode 14/100, Reward: -1.0382031250000003\n",
      "Episode 15/100, Reward: 3.7736132812500003\n",
      "Episode 16/100, Reward: 3.8657031249999996\n",
      "Episode 17/100, Reward: 3.7964453125\n",
      "Episode 18/100, Reward: -3.4517968749999994\n",
      "Episode 19/100, Reward: 3.8273437500000007\n",
      "Episode 20/100, Reward: 3.8325390625000004\n",
      "Episode 21/100, Reward: 0.7866406250000006\n",
      "Episode 22/100, Reward: 3.8584570312500004\n",
      "Episode 23/100, Reward: 3.7819531249999994\n",
      "Episode 24/100, Reward: 3.805839843750001\n",
      "Episode 25/100, Reward: 3.8079687500000015\n",
      "Episode 26/100, Reward: 4.76677734375\n",
      "Episode 27/100, Reward: 3.8042578124999995\n",
      "Episode 28/100, Reward: -3.4407421875000006\n",
      "Episode 29/100, Reward: 0.7725585937499997\n",
      "Episode 30/100, Reward: 3.8373437500000005\n",
      "Episode 31/100, Reward: 1.9732226562500006\n",
      "Episode 32/100, Reward: 3.8018359375\n",
      "Episode 33/100, Reward: 6.0449414062499995\n",
      "Episode 34/100, Reward: 3.8925585937500014\n",
      "Episode 35/100, Reward: 3.8213476562500004\n",
      "Episode 36/100, Reward: 3.801269531250001\n",
      "Episode 37/100, Reward: 3.803125\n",
      "Episode 38/100, Reward: 3.870175781250001\n",
      "Episode 39/100, Reward: 3.8130078125000004\n",
      "Episode 40/100, Reward: 3.8201171874999993\n",
      "Episode 41/100, Reward: 1.98734375\n",
      "Episode 42/100, Reward: 3.8730664062500004\n",
      "Episode 43/100, Reward: 3.8497460937500003\n",
      "Episode 44/100, Reward: 3.81525390625\n",
      "Episode 45/100, Reward: 3.8541601562500007\n",
      "Episode 46/100, Reward: 3.8344140624999996\n",
      "Episode 47/100, Reward: 3.808730468750001\n",
      "Episode 48/100, Reward: 3.8583593750000027\n",
      "Episode 49/100, Reward: 3.84603515625\n",
      "Episode 50/100, Reward: 3.831210937500001\n",
      "Episode 51/100, Reward: 1.3803320312499998\n",
      "Episode 52/100, Reward: 4.332734375\n",
      "Episode 53/100, Reward: -2.849980468749999\n",
      "Episode 54/100, Reward: 4.559687500000002\n",
      "Episode 55/100, Reward: 4.702675781250001\n",
      "Episode 56/100, Reward: 4.2268750000000015\n",
      "Episode 57/100, Reward: 3.8149414062500004\n",
      "Episode 58/100, Reward: -4.6470312499999995\n",
      "Episode 59/100, Reward: 3.8289062500000006\n",
      "Episode 60/100, Reward: 1.9732226562500006\n",
      "Episode 61/100, Reward: 3.8235546875000006\n",
      "Episode 62/100, Reward: 3.8444921874999993\n",
      "Episode 63/100, Reward: 3.7799414062500016\n",
      "Episode 64/100, Reward: 3.82513671875\n",
      "Episode 65/100, Reward: 3.7894335937500014\n",
      "Episode 66/100, Reward: 3.8122265624999994\n",
      "Episode 67/100, Reward: 3.79142578125\n",
      "Episode 68/100, Reward: 3.8253125\n",
      "Episode 69/100, Reward: 3.82583984375\n",
      "Episode 70/100, Reward: 3.7679687500000005\n",
      "Episode 71/100, Reward: 3.773281250000001\n",
      "Episode 72/100, Reward: -0.42550781249999825\n",
      "Episode 73/100, Reward: 3.8456640625\n",
      "Episode 74/100, Reward: 1.9732226562500006\n",
      "Episode 75/100, Reward: 4.000546875\n",
      "Episode 76/100, Reward: 3.816972656250001\n",
      "Episode 77/100, Reward: 3.827148437499999\n",
      "Episode 78/100, Reward: 3.8432031250000005\n",
      "Episode 79/100, Reward: 3.7934765624999986\n",
      "Episode 80/100, Reward: 3.8423046874999995\n",
      "Episode 81/100, Reward: 3.8629687500000007\n",
      "Episode 82/100, Reward: 4.068359375000001\n",
      "Episode 83/100, Reward: 4.321054687499998\n",
      "Episode 84/100, Reward: 3.7770703124999976\n",
      "Episode 85/100, Reward: 3.803886718750002\n",
      "Episode 86/100, Reward: 3.8354296875000014\n",
      "Episode 87/100, Reward: -0.435273437499999\n",
      "Episode 88/100, Reward: 3.8316601562500017\n",
      "Episode 89/100, Reward: 3.78328125\n",
      "Episode 90/100, Reward: 3.8578515624999996\n",
      "Episode 91/100, Reward: -3.4469335937500003\n",
      "Episode 92/100, Reward: 3.834082031250002\n",
      "Episode 93/100, Reward: 3.8656250000000005\n",
      "Episode 94/100, Reward: 3.80171875\n",
      "Episode 95/100, Reward: 8.297304687499999\n",
      "Episode 96/100, Reward: 3.841445312500002\n",
      "Episode 97/100, Reward: 4.269902343750001\n",
      "Episode 98/100, Reward: 3.3262890625000003\n",
      "Episode 99/100, Reward: 0.18097656249999927\n",
      "Episode 100/100, Reward: 4.455097656250002\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelul actor-critic cu CNN\n",
   "id": "dd0fa42d27d95940"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:12:44.818058Z",
     "start_time": "2024-12-30T20:12:44.809444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inițializăm Actorul și Criticul\n",
    "actor2 = Actor(state_dim, action_dim)\n",
    "critic2 = Critic(state_dim)\n",
    "\n",
    "# Optimizatori\n",
    "actor_optimizer2 = optim.Adam(actor2.parameters(), lr=0.001)\n",
    "critic_optimizer2 = optim.Adam(critic2.parameters(), lr=0.001)\n",
    "\n",
    "# Funcția de pierdere pentru Critic\n",
    "critic_loss_fn2 = nn.MSELoss()"
   ],
   "id": "39dbcd3c5b9ef652",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T21:07:55.283791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparametrii\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Valoarea inițială pentru explorare\n",
    "epsilon_min = 0.1  # Valoarea minimă pentru explorare\n",
    "epsilon_decay = 0.995  # Rata de scădere a epsilonului\n",
    "num_episodes = 1000\n",
    "model_actiune_curenta.eval()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # Resetăm mediul\n",
    "    state_pixels = env.render()\n",
    "    # print(\"state_pixels\",state_pixels.shape)\n",
    "    processed_frame = preprocess(state_pixels).unsqueeze(0)\n",
    "    # print(\"processed_frame\",processed_frame.shape)\n",
    "    state = model_actiune_curenta(processed_frame)[0]\n",
    "    state = state.detach().numpy()\n",
    "    # print(\"state\",state)\n",
    "    # print(\"state\",state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Explorare epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Alegem o acțiune aleatorie pentru explorare\n",
    "            action = np.random.choice([0, 1])\n",
    "            log_prob = None  # Nu avem log_prob pentru acțiuni exploratorii\n",
    "        else:\n",
    "            # Alegem acțiunea pe baza actorului\n",
    "            action, action_probs = select_action(state, actor2)\n",
    "            log_prob = torch.log(action_probs[0, action])\n",
    "\n",
    "        # Executăm acțiunea în mediul de simulare\n",
    "        next_state_target , reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # Compute next state with our model CNN\n",
    "        new_state_pixels = env.render()\n",
    "        procesed_frame = preprocess(new_state_pixels).unsqueeze(0)\n",
    "        next_state = model_actiune_curenta(procesed_frame)[0]\n",
    "        next_state = next_state.detach().numpy()\n",
    "        \n",
    "        \n",
    "        # print(\"next_state\",next_state)\n",
    "        # print(\"next_state_target\",next_state_target)\n",
    "        # Calculul centrului pipe-ului următor\n",
    "        # next_pipe_center = (state[4] + state[5]) / 2\n",
    "        # distance_from_center = abs(state[9] - next_pipe_center)\n",
    "        # \n",
    "        # # Penalizare pentru distanță față de centru\n",
    "        # reward -= distance_from_center * 0.01\n",
    "\n",
    "        # Calculăm TD Target și Advantage\n",
    "        value = critic2(torch.FloatTensor(state).unsqueeze(0)).item()\n",
    "        next_value = critic2(torch.FloatTensor(next_state).unsqueeze(0)).item() if not done else 0\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # Actualizăm Actorul doar dacă acțiunea este determinată de model\n",
    "        if log_prob is not None:\n",
    "            update_actor(actor2, actor_optimizer2, log_prob, advantage)\n",
    "\n",
    "        # Actualizăm Criticul\n",
    "        update_critic(critic2, critic_optimizer2, state, td_target,critic_loss_fn2)\n",
    "\n",
    "        # Actualizăm starea curentă\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        # Add regularization noise for more robust exploration\n",
    "        state = state + np.random.normal(0, 0.05, size=state.shape)  # Adaugă zgomot\n",
    "\n",
    "\n",
    "    # Decay pentru epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n"
   ],
   "id": "27c22baf9a938840",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Reward: -6.299999999999999\n",
      "Episode 2/1000, Reward: -8.099999999999998\n",
      "Episode 3/1000, Reward: -8.7\n",
      "Episode 4/1000, Reward: -8.7\n",
      "Episode 5/1000, Reward: -7.499999999999998\n",
      "Episode 6/1000, Reward: -8.099999999999998\n",
      "Episode 7/1000, Reward: -4.499999999999998\n",
      "Episode 8/1000, Reward: -8.099999999999998\n",
      "Episode 9/1000, Reward: -8.099999999999998\n",
      "Episode 10/1000, Reward: -8.099999999999998\n",
      "Episode 11/1000, Reward: -7.499999999999998\n",
      "Episode 12/1000, Reward: -8.099999999999998\n",
      "Episode 13/1000, Reward: -6.299999999999999\n",
      "Episode 14/1000, Reward: -8.099999999999998\n",
      "Episode 15/1000, Reward: -5.699999999999998\n",
      "Episode 16/1000, Reward: -8.099999999999998\n",
      "Episode 17/1000, Reward: -7.499999999999998\n",
      "Episode 18/1000, Reward: -7.499999999999998\n",
      "Episode 19/1000, Reward: -6.899999999999999\n",
      "Episode 20/1000, Reward: -7.499999999999998\n",
      "Episode 21/1000, Reward: -8.099999999999998\n",
      "Episode 22/1000, Reward: -8.099999999999998\n",
      "Episode 23/1000, Reward: -6.899999999999999\n",
      "Episode 24/1000, Reward: -7.499999999999998\n",
      "Episode 25/1000, Reward: -7.499999999999998\n",
      "Episode 26/1000, Reward: -6.899999999999999\n",
      "Episode 27/1000, Reward: -6.899999999999999\n",
      "Episode 28/1000, Reward: -5.699999999999998\n",
      "Episode 29/1000, Reward: -6.899999999999999\n",
      "Episode 30/1000, Reward: -3.299999999999998\n",
      "Episode 31/1000, Reward: -8.099999999999998\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 5: Testare",
   "id": "1aa1187cf759aba1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:06:23.216206Z",
     "start_time": "2024-12-30T21:06:01.146568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\" , use_lidar=False)\n",
    "# print(\"Modelul Actor-Critic cu CNN\",actor2)\n",
    "# print(\"Modelul Actor-Critic fara CNN\",actor)\n",
    "for i in range(10):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = select_action_without_explore(state, actor2)\n",
    "        # print(\"action\",action)\n",
    "        state, reward, done, _,_= env.step(action)\n",
    "        env.render()  # Vizualizăm episodul\n"
   ],
   "id": "1bc16d7064331276",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[71], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m action, _ \u001B[38;5;241m=\u001B[39m select_action_without_explore(state, actor2)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# print(\"action\",action)\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m state, reward, done, _,_\u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m env\u001B[38;5;241m.\u001B[39mrender()  \u001B[38;5;66;03m# Vizualizăm episodul\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:322\u001B[0m, in \u001B[0;36mWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[0;32m    320\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    321\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flappy_bird_gymnasium\\envs\\flappy_bird_env.py:260\u001B[0m, in \u001B[0;36mFlappyBirdEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    257\u001B[0m         low_pipe[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m new_low_pipe[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 260\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m obs, reward_private_zone \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_observation()\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flappy_bird_gymnasium\\envs\\flappy_bird_env.py:407\u001B[0m, in \u001B[0;36mFlappyBirdEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_display()\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_display()\n\u001B[1;32m--> 407\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fps_clock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# evaluare",
   "id": "b0d8e1a219ce1a2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(env, actor, num_eval_episodes=100):\n",
    "    scores = []\n",
    "    survival_times = []\n",
    "    pipes_passed = []\n",
    "\n",
    "    for _ in range(num_eval_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        frames = 0\n",
    "        pipes = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = select_action_without_explore(state, actor)  # Politica deterministă (fără explorare)\n",
    "            next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "            # Adunăm scorurile și statistici\n",
    "            episode_reward += reward\n",
    "            frames += 1\n",
    "            if reward > 0.5:  # Ex: Recompensă mare pentru trecerea printr-un pipe\n",
    "                pipes += 1\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "        survival_times.append(frames)\n",
    "        pipes_passed.append(pipes)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_survival = np.mean(survival_times)\n",
    "    avg_pipes = np.mean(pipes_passed)\n",
    "\n",
    "    print(f\"Evaluare pe {num_eval_episodes} episoade:\")\n",
    "    print(f\"Scor mediu: {avg_score}\")\n",
    "    print(f\"Timp mediu de supraviețuire: {avg_survival} cadre\")\n",
    "    print(f\"Pipe-uri trecute în medie: {avg_pipes}\")\n",
    "\n",
    "    return scores, survival_times, pipes_passed\n",
    "\n",
    "# După antrenare:\n",
    "scores, survival_times, pipes_passed = evaluate_model(env, actor)\n",
    "\n",
    "# Vizualizare\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Recompensa medie\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(scores)\n",
    "plt.title(\"Scor pe Episod\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Scor\")\n",
    "\n",
    "# Supraviețuirea medie\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(survival_times)\n",
    "plt.title(\"Timp de Supraviețuire pe Episod\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Cadre\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "6e2876e9748c3cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7ef173c51f73a1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "934fae4e5978434d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
