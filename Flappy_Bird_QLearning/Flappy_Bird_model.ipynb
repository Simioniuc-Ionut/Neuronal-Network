{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:26.678849Z",
     "start_time": "2025-01-11T12:53:23.446021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from random import random\n",
    "\n",
    "import flappy_bird_gymnasium\n",
    "from gymnasium import envs\n",
    "\n",
    "print(envs.registry.keys())\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "# Configurare\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=True,)"
   ],
   "id": "f47fd294ca39805a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0', 'FlappyBird-v0'])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:27.426818Z",
     "start_time": "2025-01-11T12:53:27.419277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Q-Learning\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 100\n"
   ],
   "id": "dfb3bd62d7b9c339",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:39.581202Z",
     "start_time": "2025-01-11T12:53:27.446816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "'''preprocessing img'''\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),              # Conversie la imagine PIL\n",
    "    transforms.Grayscale(num_output_channels=1),               # Convertire în tonuri de gri\n",
    "    transforms.Resize((84,84)),         # Redimensionare la 84x84 pixeli\n",
    "    transforms.ToTensor() # Conversie la tensor PyTorch\n",
    "])"
   ],
   "id": "fda7602667fa248a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:39.657281Z",
     "start_time": "2025-01-11T12:53:39.648857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "'''replay buffer class'''\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)  # Buffer cu lungime maximă\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)  # Adaugă experiența (state, action, reward, next_state, done)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)  # Selectează un eșantion aleatoriu\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ],
   "id": "ccd923fe7cd071c9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:39.710875Z",
     "start_time": "2025-01-11T12:53:39.704373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "'''initialize replay_buffer'''\n",
    "replay_buffer = deque(maxlen=20000)  # Buffer pentru stocarea tranzițiilor\n"
   ],
   "id": "b733ea4dc8bae42d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:39.754320Z",
     "start_time": "2025-01-11T12:53:39.747867Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.action_space)",
   "id": "edb778d9a4f7ec47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.397010Z",
     "start_time": "2025-01-11T12:53:39.848478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import keyboard  \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "'''add to buffer frames form game'''\n",
    "\n",
    "# Configurare environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # Joc manual\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count_frames=0\n",
    "\n",
    "    # Inițializează primul frame procesat\n",
    "    frame = env.render()\n",
    "    \n",
    "    processed_current_frame = preprocess(frame)     \n",
    "    while not done:\n",
    "        \n",
    "        # play manual (space = jump, altfel = stay)\n",
    "        # if keyboard.is_pressed(\"space\"):\n",
    "        #     action = 1  # Jump\n",
    "        # else:\n",
    "        #     action = 0 \n",
    "        # Stay\n",
    "        \n",
    "        if np.random.rand() < 0.1:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0 \n",
    "    \n",
    "        new_state, reward, done,_, info = env.step(action)\n",
    "        \n",
    "        next_frame = env.render()\n",
    "        \n",
    "        processed_next_frame = preprocess(next_frame)\n",
    "        # Extrage array-ul de 12 valori din new_state\n",
    "        \"\"\"\n",
    "        new_state contains :\n",
    "        'last_pipe_horizontal_position',\n",
    "        'last_top_pipe_vertical_position',\n",
    "        'last_bottom_pipe_vertical_position'\n",
    "        'next_pipe_horizontal_position'\n",
    "        'next_top_pipe_vertical_position'\n",
    "        'next_bottom_pipe_vertical_position',\n",
    "        'next_next_pipe_horizontal_position'\n",
    "        'next_next_top_pipe_vertical_position',\n",
    "        'next_next_bottom_pipe_vertical_position',\n",
    "        'player_vertical_position',\n",
    "        'player_vertical_velocity',\n",
    "        'player_rotation'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        if count_frames % 4 == 0 or done:\n",
    "            replay_buffer.append((processed_current_frame, action, reward,processed_next_frame, new_state, done))\n",
    "        \n",
    "        # Actualize for next iteration\n",
    "        processed_current_frame = processed_next_frame\n",
    "        count_frames += 1\n",
    "        if done:\n",
    "            break\n",
    "    "
   ],
   "id": "a13aed199d28d01f",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 42\u001B[0m\n\u001B[0;32m     38\u001B[0m new_state, reward, done,_, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     40\u001B[0m next_frame \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[1;32m---> 42\u001B[0m processed_next_frame \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_frame\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Extrage array-ul de 12 valori din new_state\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03mnew_state contains :\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m'last_pipe_horizontal_position',\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;124;03m'player_rotation'\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001B[0m, in \u001B[0;36mToPILImage.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    226\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    232\u001B[0m \n\u001B[0;32m    233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_pil_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:324\u001B[0m, in \u001B[0;36mto_pil_image\u001B[1;34m(pic, mode)\u001B[0m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    322\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnpimg\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnpimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3342\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3339\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrides\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m requires either tobytes() or tostring()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3340\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m-> 3342\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrombuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mraw\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrawmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3185\u001B[0m, in \u001B[0;36mfrombuffer\u001B[1;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[0;32m   3181\u001B[0m         im\u001B[38;5;241m.\u001B[39mfrombytes(data, decoder_name, decoder_args)\n\u001B[0;32m   3182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m im\n\u001B[1;32m-> 3185\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrombuffer\u001B[39m(\n\u001B[0;32m   3186\u001B[0m     mode: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   3187\u001B[0m     size: \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m   3188\u001B[0m     data: \u001B[38;5;28mbytes\u001B[39m \u001B[38;5;241m|\u001B[39m SupportsArrayInterface,\n\u001B[0;32m   3189\u001B[0m     decoder_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3190\u001B[0m     \u001B[38;5;241m*\u001B[39margs: Any,\n\u001B[0;32m   3191\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image:\n\u001B[0;32m   3192\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3193\u001B[0m \u001B[38;5;124;03m    Creates an image memory referencing pixel data in a byte buffer.\u001B[39;00m\n\u001B[0;32m   3194\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3222\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.1.4\u001B[39;00m\n\u001B[0;32m   3223\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   3225\u001B[0m     _check_size(size)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.426279900Z",
     "start_time": "2024-12-31T07:57:26.753941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "# print(f\"buffer one example\",replay_buffer[0],len(replay_buffer[0]))\n"
   ],
   "id": "97ff3369e2affe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 3244\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.493684600Z",
     "start_time": "2024-12-30T22:32:15.799452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "'''view and look at states forms'''\n",
    "\n",
    "# Configurare environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\" , use_lidar=False)\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Joc manual\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count_frames=0\n",
    "    \n",
    "    frame =env.render() \n",
    "    print(\" frame \" ,frame)\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Joacă manual (space = jump, altfel = stay)\n",
    "        if keyboard.is_pressed(\"space\"):\n",
    "            action = 1  # Jump\n",
    "        else:\n",
    "            action = 0 \n",
    "\n",
    "        new_state, reward, done, _ , info = env.step(action)  \n",
    "        print(\"shape\",new_state.shape)\n",
    "        print(\"new state \",new_state )\n",
    "        \n",
    "        # Randează jocul\n",
    "        frame =env.render() \n",
    "        print(\" frame \" ,frame)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "                "
   ],
   "id": "82d1095cce7db3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.98611111  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.4609375  -0.8         0.46666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.97222222  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.44726562 -0.7         0.43333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.95833333  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.43554688 -0.6         0.4       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.94444444  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.42578125 -0.5         0.36666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.93055556  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.41796875 -0.4         0.33333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.91666667  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.41210938 -0.3         0.3       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.90277778  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.40820312 -0.2         0.26666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.88888889  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.40625    -0.1         0.23333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.875     0.3125    0.5078125 1.        0.        1.        1.\n",
      " 0.        1.        0.40625   0.        0.2      ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.86111111 0.3125     0.5078125  1.         0.         1.\n",
      " 1.         0.         1.         0.40820312 0.1        0.16666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.84722222 0.3125     0.5078125  1.         0.         1.\n",
      " 1.         0.         1.         0.41210938 0.2        0.13333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.83333333 0.3125     0.5078125  1.         0.         1.\n",
      " 1.         0.         1.         0.41796875 0.3        0.1       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.81944444 0.3125     0.5078125  1.         0.         1.\n",
      " 1.         0.         1.         0.42578125 0.4        0.06666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.80555556 0.3125     0.5078125  1.         0.         1.\n",
      " 1.         0.         1.         0.43554688 0.5        0.03333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [0.79166667 0.3125     0.5078125  1.         0.         1.\n",
      " 1.         0.         1.         0.44726562 0.6        0.        ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.77777778  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.4609375   0.7        -0.03333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.76388889  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.4765625   0.8        -0.06666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.75        0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.49414062  0.9        -0.1       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.73611111  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.51367188  1.         -0.13333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.72222222  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.53320312  1.         -0.16666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.70833333  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.55273438  1.         -0.2       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.69444444  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.57226562  1.         -0.23333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.68055556  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.59179688  1.         -0.26666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.66666667  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.61132812  1.         -0.3       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.65277778  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.63085938  1.         -0.33333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.63888889  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.65039062  1.         -0.36666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.625       0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.66992188  1.         -0.4       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.61111111  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.68945312  1.         -0.43333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.59722222  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.70898438  1.         -0.46666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.58333333  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.72851562  1.         -0.5       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.56944444  0.3125      0.5078125   1.          0.          1.\n",
      "  1.          0.          1.          0.743125    1.         -0.53333333]\n",
      " frame  None\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.98611111  0.25390625  0.44921875  1.          0.          1.\n",
      "  1.          0.          1.          0.4609375  -0.8         0.46666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.97222222  0.25390625  0.44921875  1.          0.          1.\n",
      "  1.          0.          1.          0.44726562 -0.7         0.43333333]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.95833333  0.25390625  0.44921875  1.          0.          1.\n",
      "  1.          0.          1.          0.43554688 -0.6         0.4       ]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.94444444  0.25390625  0.44921875  1.          0.          1.\n",
      "  1.          0.          1.          0.42578125 -0.5         0.36666667]\n",
      " frame  None\n",
      "shape (12,)\n",
      "new state  [ 0.93055556  0.25390625  0.44921875  1.          0.          1.\n",
      "  1.          0.          1.          0.41796875 -0.4         0.33333333]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 29\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew state \u001B[39m\u001B[38;5;124m\"\u001B[39m,new_state )\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Randează jocul\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m frame \u001B[38;5;241m=\u001B[39m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m frame \u001B[39m\u001B[38;5;124m\"\u001B[39m ,frame)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m     )\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:332\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RenderFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[RenderFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:303\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_render_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv)\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flappy_bird_gymnasium\\envs\\flappy_bird_env.py:407\u001B[0m, in \u001B[0;36mFlappyBirdEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_display()\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_display()\n\u001B[1;32m--> 407\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fps_clock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Assuming `processed_frame` is the grayscale image tensor\n",
    "\n",
    "'''Visualize images from buffer'''\n",
    "\n",
    "if len(replay_buffer) > 0:\n",
    "    first_img = replay_buffer[0][0].squeeze(0)\n",
    "else:\n",
    "    first_img = None\n",
    "    \n",
    "def show_grayscale_image(image_tensor):\n",
    "    plt.imshow(image_tensor.numpy(), cmap='gray')\n",
    "    plt.title('Grayscale Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.pause(0.1)  # Pause to allow the image to be displayed"
   ],
   "id": "6a47b4215b8319e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for experience in replay_buffer:\n",
    "    img = experience[0].squeeze(0)  # Extract the grayscale image tensor\n",
    "    show_grayscale_image(img)\n",
    "    img = experience[3].squeeze(0)  # Extract the grayscale image tensor\n",
    "    show_grayscale_image(img)"
   ],
   "id": "7a731fe33fc046d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.494684500Z",
     "start_time": "2024-12-31T07:54:51.470584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CNN\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "'''CNN class definition'''\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Define the layers of the convolutional neural network.\n",
    "\n",
    "    Parameters:\n",
    "    in_channels: int\n",
    "        The number of channels in the input image. For MNIST, this is 1 (grayscale images).\n",
    "    num_classes: int\n",
    "        The number of classes we want to predict, in our case 12 .\n",
    "    \"\"\"\"\"\n",
    "    def __init__(self, in_channels, num_classes=12):\n",
    "        super().__init__()\n",
    "        # First convolutional layer: 1 input channel, 8 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
    "        # Max pooling layer: 2x2 window, stride 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # Second convolutional layerl: 8 input channels, 16 output channels, 3x3 kernel, stride 1 , padding1\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16, kernel_size=5, stride=1,padding=0)\n",
    "        # Second convolutional layerl: 8 input channels, 16 output channels, 3x3 kernel, stride 1 , padding1\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=32, kernel_size=5, stride=1,padding=0)\n",
    "        # Fully connected layer: 16 * 7 * 7  input features (after two 2x2 poolings), state output features (num_classes) \n",
    "        self.fc1 = nn.Linear(32*7*7,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            x: torch.Tensor\n",
    "                The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "                The output tensor after passing through the network.\n",
    "        \"\"\"\"\"\n",
    "        x = torch.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
    "        x = self.pool(x)               # Apply max pooling\n",
    "        x = torch.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
    "        x = self.pool(x)               # Apply max pooling\n",
    "        x = torch.relu(self.conv3(x))  # Apply third convolution and ReLU activation\n",
    "        x = self.pool(x)               # Apply max pooling\n",
    "        x = x.view(x.size(0), -1)      # Flatten the tensor\n",
    "        x = self.fc1(x)                # Apply fully connected layer\n",
    "        return x\n",
    "    "
   ],
   "id": "8cbe98e1b476f871",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.494684500Z",
     "start_time": "2024-12-31T07:54:51.614196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = 7056  # 84x84 pixels (not directly used in CNN)\n",
    "num_classes = 12 # 12 characteristics from use_lidar = False\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10  "
   ],
   "id": "3faa9ded6d9cae09",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.494684500Z",
     "start_time": "2024-12-31T07:54:51.713430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "'''train CNN to extract relevant features'''\n",
    "\n",
    "def train_feature_extractor(model, replay_buffer, batch_size, num_epochs, learning_rate):\n",
    "    \"\"\" \n",
    "    Antrenează rețeaua CNN pentru extragerea caracteristicilor.\n",
    "\n",
    "    Parameters:\n",
    "        model: nn.Module\n",
    "            Modelul CNN pentru extragerea caracteristicilor.\n",
    "        replay_buffer: deque\n",
    "            Lista cu experiențe (frame-uri, acțiuni, recompense, next_frame-uri, state-uri și done flag).\n",
    "        batch_size: int\n",
    "            Dimensiunea batch-ului.\n",
    "        num_epochs: int\n",
    "            Numărul de epoci.\n",
    "        learning_rate: float\n",
    "            Rata de învățare.\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    # Preparing data for training\n",
    "    frames = [experience[3] for experience in replay_buffer]  # Frame-urile curente\n",
    "    targets = [experience[4] for experience in replay_buffer]  # Target-ul (new_state)\n",
    "\n",
    "    # Convert frames and targets to tensors\n",
    "    frames = torch.tensor(np.array(frames), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
    "\n",
    "    # Create a dataset and a dataloader with shuffle to episodes\n",
    "    dataset = TensorDataset(frames, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inițializează pierderea și optimizer-ul\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Activează modul de antrenare\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)  # Pierdere față de ținte\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ],
   "id": "debf6b96aa7cd41f",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.494684500Z",
     "start_time": "2024-12-31T07:54:51.764778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# [bird_y, bird_velocity, pipe_x1, pipe_x2]\n",
    "feature_dim = 12  # Dimensiunea vectorului de caracteristici\n",
    "model_actiune_curenta = CNN(in_channels=1, num_classes=feature_dim)  # Creare model CNN\n"
   ],
   "id": "2e3e7ed7416cdeb8",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.496126Z",
     "start_time": "2024-12-31T07:58:59.134115Z"
    }
   },
   "cell_type": "code",
   "source": "train_feature_extractor(model_actiune_curenta, replay_buffer, batch_size, 50, learning_rate)  # Antrenare model\n",
   "id": "186a5cde039669b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.4337\n",
      "Epoch [2/50], Loss: 1.2674\n",
      "Epoch [3/50], Loss: 1.2361\n",
      "Epoch [4/50], Loss: 1.2083\n",
      "Epoch [5/50], Loss: 1.1802\n",
      "Epoch [6/50], Loss: 1.1015\n",
      "Epoch [7/50], Loss: 1.0373\n",
      "Epoch [8/50], Loss: 0.9581\n",
      "Epoch [9/50], Loss: 0.8802\n",
      "Epoch [10/50], Loss: 0.8238\n",
      "Epoch [11/50], Loss: 0.7391\n",
      "Epoch [12/50], Loss: 0.6983\n",
      "Epoch [13/50], Loss: 0.6739\n",
      "Epoch [14/50], Loss: 0.6120\n",
      "Epoch [15/50], Loss: 0.5692\n",
      "Epoch [16/50], Loss: 0.5537\n",
      "Epoch [17/50], Loss: 0.5274\n",
      "Epoch [18/50], Loss: 0.5003\n",
      "Epoch [19/50], Loss: 0.4879\n",
      "Epoch [20/50], Loss: 0.4708\n",
      "Epoch [21/50], Loss: 0.4530\n",
      "Epoch [22/50], Loss: 0.4594\n",
      "Epoch [23/50], Loss: 0.4279\n",
      "Epoch [24/50], Loss: 0.4272\n",
      "Epoch [25/50], Loss: 0.4083\n",
      "Epoch [26/50], Loss: 0.3891\n",
      "Epoch [27/50], Loss: 0.3755\n",
      "Epoch [28/50], Loss: 0.3676\n",
      "Epoch [29/50], Loss: 0.3648\n",
      "Epoch [30/50], Loss: 0.3688\n",
      "Epoch [31/50], Loss: 0.3471\n",
      "Epoch [32/50], Loss: 0.3451\n",
      "Epoch [33/50], Loss: 0.3398\n",
      "Epoch [34/50], Loss: 0.3371\n",
      "Epoch [35/50], Loss: 0.3349\n",
      "Epoch [36/50], Loss: 0.3307\n",
      "Epoch [37/50], Loss: 0.3254\n",
      "Epoch [38/50], Loss: 0.3228\n",
      "Epoch [39/50], Loss: 0.3224\n",
      "Epoch [40/50], Loss: 0.2993\n",
      "Epoch [41/50], Loss: 0.3015\n",
      "Epoch [42/50], Loss: 0.2979\n",
      "Epoch [43/50], Loss: 0.2988\n",
      "Epoch [44/50], Loss: 0.2925\n",
      "Epoch [45/50], Loss: 0.2952\n",
      "Epoch [46/50], Loss: 0.2853\n",
      "Epoch [47/50], Loss: 0.2961\n",
      "Epoch [48/50], Loss: 0.2774\n",
      "Epoch [49/50], Loss: 0.2755\n",
      "Epoch [50/50], Loss: 0.2715\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.496126Z",
     "start_time": "2024-12-31T08:01:52.253145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_feature_extractor(model, validation_data, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Evaluează modelul pe un set de validare și calculează MSE și acuratețea.\n",
    "\n",
    "    Parameters:\n",
    "        model: nn.Module\n",
    "            Modelul CNN antrenat.\n",
    "        validation_data: list of tuples\n",
    "            Lista de (frame, target) pentru validare.\n",
    "        tolerance: float\n",
    "            Intervalul de toleranță pentru a considera o predicție corectă.\n",
    "\n",
    "    Returns:\n",
    "        mse: float\n",
    "            Mean Squared Error pe setul de validare.\n",
    "        accuracy: float\n",
    "            Acuratețea în procente.\n",
    "    \"\"\"\"\"\n",
    "    model.eval()  # Mod de evaluare (fără actualizare a ponderilor)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Nu calculăm gradientele\n",
    "        for frame, target in validation_data:\n",
    "            frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0)  # Adaugă dimensiunea batch\n",
    "            target = torch.tensor(target, dtype=torch.float32).unsqueeze(0)\n",
    "            prediction = model(frame)\n",
    "\n",
    "            loss = mse_loss(prediction, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Verificăm dacă predicția este în toleranță\n",
    "            if torch.all(torch.abs(prediction - target) < tolerance):\n",
    "                correct_predictions += 1\n",
    "\n",
    "    mse = total_loss / len(validation_data)\n",
    "    accuracy = (correct_predictions / len(validation_data)) * 100\n",
    "    return mse, accuracy\n"
   ],
   "id": "f7047f217906a4",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.496916500Z",
     "start_time": "2024-12-31T08:01:36.579306Z"
    }
   },
   "cell_type": "code",
   "source": "validation_data = []",
   "id": "7f2947c34afb5ce6",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.496916500Z",
     "start_time": "2024-12-31T08:01:36.992063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import keyboard  # Pentru a detecta apăsările de taste\n",
    "\n",
    "# Extract validation data\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Configurare environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Joc manual\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count_frames=0\n",
    "\n",
    "    # Inițializează primul frame procesat\n",
    "    frame = env.render()\n",
    "        \n",
    "    while not done:\n",
    "        \n",
    "        # Play manual (space = jump, altfel = stay)\n",
    "        # if keyboard.is_pressed(\"space\"):\n",
    "        #     action = 1  # Jump\n",
    "        # else:\n",
    "        #     action = 0 \n",
    "        # Stay\n",
    "        if np.random.rand() < 0.1:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0 \n",
    "    \n",
    "        new_state, reward, done,_, info = env.step(action)\n",
    "        \n",
    "        next_frame = env.render()\n",
    "        \n",
    "        processed_next_frame = preprocess(next_frame)\n",
    "        # Extrage array-ul de 12 valori din new_state\n",
    "        \"\"\"\n",
    "        new_state contains :\n",
    "        'last_pipe_horizontal_position',\n",
    "        'last_top_pipe_vertical_position',\n",
    "        'last_bottom_pipe_vertical_position'\n",
    "        'next_pipe_horizontal_position'\n",
    "        'next_top_pipe_vertical_position'\n",
    "        'next_bottom_pipe_vertical_position',\n",
    "        'next_next_pipe_horizontal_position'\n",
    "        'next_next_top_pipe_vertical_position',\n",
    "        'next_next_bottom_pipe_vertical_position',\n",
    "        'player_vertical_position',\n",
    "        'player_vertical_velocity',\n",
    "        'player_rotation'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        if count_frames % 4 == 0 or done:\n",
    "            validation_data.append((processed_next_frame,new_state))\n",
    "        \n",
    "        count_frames += 1\n",
    "        if done:\n",
    "            break"
   ],
   "id": "f92fb66b068b6db8",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.497426300Z",
     "start_time": "2024-12-31T08:01:56.262812Z"
    }
   },
   "cell_type": "code",
   "source": "len(validation_data)",
   "id": "cda7e7828e68b991",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.509868400Z",
     "start_time": "2024-12-31T08:01:57.582764Z"
    }
   },
   "cell_type": "code",
   "source": "validate_feature_extractor(model_actiune_curenta, validation_data, tolerance=0.3)  # Validare model",
   "id": "8613670373ac9c90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_26280\\1639196621.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  frame = torch.tensor(frame, dtype=torch.float32).unsqueeze(0)  # Adaugă dimensiunea batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0069626689814699435, 80.0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Actor-Critic , nefunctional",
   "id": "4e640b1f357f864d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 1: Definirea Retelelor Actor și Critic",
   "id": "3dfbd11b929bc47e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.510877500Z",
     "start_time": "2024-12-31T08:02:16.261791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Probabilitățile pentru acțiuni\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Valoarea scalară a stării\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n"
   ],
   "id": "c657cff3a3f2d8ac",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 2: Inițializarea Mediului și Rețelelor",
   "id": "64beaaab9626be1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.511878600Z",
     "start_time": "2024-12-31T07:41:49.334764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dimensiunea state-ului (12 caracteristici)\n",
    "state_dim = 12\n",
    "# Dimensiunea spațiului de acțiuni (2 acțiuni: flap sau no flap)\n",
    "action_dim = 2\n",
    "\n",
    "# Inițializăm Actorul și Criticul\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "# Optimizatori\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# Funcția de pierdere pentru Critic\n",
    "critic_loss_fn = nn.MSELoss()\n"
   ],
   "id": "db7c2670355eb47d",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 3: Funcții de Ajutor",
   "id": "5263776b4388c18e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.512876600Z",
     "start_time": "2024-12-31T07:42:18.246141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Alegerea unei acțiuni pe baza politicii:\n",
    "def select_action(state, actor):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convertim în tensor\n",
    "    action_probs = actor(state_tensor)\n",
    "    action = torch.multinomial(action_probs, 1).item()  # Chose an action\n",
    "    return action, action_probs\n",
    "\n",
    "def select_action_without_explore(state, actor):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convertim în tensor\n",
    "    action_probs = actor(state_tensor)\n",
    "    action = torch.argmax(action_probs).item()  # Alegem acțiunea cu probabilitatea cea mai mare\n",
    "    return action, action_probs\n"
   ],
   "id": "d3edc8ec5c2d58e0",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.512876600Z",
     "start_time": "2024-12-31T07:26:14.049753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Actualizarea Actorului:\n",
    "def update_actor(actor, actor_optimizer, log_prob, advantage):\n",
    "    \n",
    "    advantage = torch.tensor(advantage, dtype=torch.float32)\n",
    "    actor_loss = -(log_prob * advantage.detach()).mean()  # Gradientul politicii\n",
    "    \n",
    "    # Adăugăm regularizarea entropiei pentru explorare \n",
    "    # entropy_loss = -torch.sum(action_probs * torch.log(action_probs))\n",
    "    # actor_loss += 0.01 * entropy_loss  # Adaugă regularizarea entropiei\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "    actor_optimizer.step()\n"
   ],
   "id": "f1b567ed56e055a0",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.513868900Z",
     "start_time": "2024-12-31T07:26:17.736545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Actualizarea Criticului:\n",
    "def update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    td_target_tensor = torch.FloatTensor([td_target])\n",
    "    value = critic(state_tensor)\n",
    "    loss = critic_loss_fn(value, td_target_tensor)\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    critic_optimizer.step()\n"
   ],
   "id": "5ac33fbd16c7f553",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 4: Ciclu Principal de Învățare",
   "id": "1f8323e7fd068250"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.513868900Z",
     "start_time": "2024-12-31T07:26:22.945964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vers 1\n",
    "'''Of policy without exploration'''\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparametrii\n",
    "gamma = 0.99  # Discount factor\n",
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # Resetăm mediul (simulat sau real)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # 1. Chose an action\n",
    "        action, action_probs = select_action(state, actor)\n",
    "        log_prob = torch.log(action_probs[0, action])\n",
    "\n",
    "        # 2. Executăm acțiunea și primim noua stare și recompensa\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward \n",
    "        \n",
    "        # 3. Compute TD Target and Advantage values\n",
    "        value = critic(torch.FloatTensor(state).unsqueeze(0)).item()\n",
    "        next_value = critic(torch.FloatTensor(next_state).unsqueeze(0)).item() if not done else 0\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # 4. Actualize Actorul and Criticul\n",
    "        update_actor(actor, actor_optimizer, log_prob, advantage)\n",
    "        update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn)\n",
    "\n",
    "        # 5. Next state\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n"
   ],
   "id": "87638a032e565082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Reward: -8.7\n",
      "Episode 2/1000, Reward: -9.299999999999999\n",
      "Episode 3/1000, Reward: -9.299999999999999\n",
      "Episode 4/1000, Reward: -9.299999999999999\n",
      "Episode 5/1000, Reward: -7.499999999999998\n",
      "Episode 6/1000, Reward: -2.699999999999998\n",
      "Episode 7/1000, Reward: -8.7\n",
      "Episode 8/1000, Reward: -9.299999999999999\n",
      "Episode 9/1000, Reward: -9.299999999999999\n",
      "Episode 10/1000, Reward: -9.299999999999999\n",
      "Episode 11/1000, Reward: -9.299999999999999\n",
      "Episode 12/1000, Reward: -9.299999999999999\n",
      "Episode 13/1000, Reward: -9.299999999999999\n",
      "Episode 14/1000, Reward: -9.299999999999999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[64], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m log_prob \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog(action_probs[\u001B[38;5;241m0\u001B[39m, action])\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# 2. Executăm acțiunea și primim noua stare și recompensa\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m next_state, reward, done, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward \n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# 3. Calculăm valoarea TD Target și Advantage\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:322\u001B[0m, in \u001B[0;36mWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[0;32m    320\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    321\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flappy_bird_gymnasium\\envs\\flappy_bird_env.py:260\u001B[0m, in \u001B[0;36mFlappyBirdEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    257\u001B[0m         low_pipe[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m new_low_pipe[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 260\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m obs, reward_private_zone \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_observation()\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flappy_bird_gymnasium\\envs\\flappy_bird_env.py:407\u001B[0m, in \u001B[0;36mFlappyBirdEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_display()\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_display()\n\u001B[1;32m--> 407\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fps_clock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train Actor-Critic without CNN",
   "id": "f9f74e9b96305cdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.513868900Z",
     "start_time": "2024-12-31T07:45:08.182977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vers 2 \n",
    "'''On policy ,with exploration'''\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparametrii\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Valoarea inițială pentru explorare\n",
    "epsilon_min = 0.1  # Valoarea minimă pentru explorare\n",
    "epsilon_decay = 0.995  # Rata de scădere a epsilonului\n",
    "num_episodes = 1000\n",
    "actor.train()\n",
    "critic.train()\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # Resetăm mediul\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Explorare epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Alegem o acțiune aleatorie pentru explorare\n",
    "            action = np.random.choice([0, 1])\n",
    "            log_prob = None  # Nu avem log_prob pentru acțiuni exploratorii\n",
    "        else:\n",
    "            # Alegem acțiunea pe baza actorului\n",
    "            action, action_probs = select_action(state, actor)\n",
    "            log_prob = torch.log(action_probs[0, action])\n",
    "\n",
    "        # Executăm acțiunea în mediul de simulare\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Calculul centrului pipe-ului următor\n",
    "        next_pipe_center = (state[4] + state[5]) / 2\n",
    "        distance_from_center = abs(state[9] - next_pipe_center)\n",
    "\n",
    "        # Penalizare pentru distanță față de centru\n",
    "        reward -= distance_from_center * 0.01\n",
    "\n",
    "        # Calculăm TD Target și Advantage\n",
    "        value = critic(torch.FloatTensor(state).unsqueeze(0)).item()\n",
    "        next_value = critic(torch.FloatTensor(next_state).unsqueeze(0)).item() if not done else 0\n",
    "        td_target = reward + gamma * next_value\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # Actualizăm Actorul doar dacă acțiunea este determinată de model\n",
    "        if log_prob is not None:\n",
    "            update_actor(actor, actor_optimizer, log_prob, advantage)\n",
    "\n",
    "        # Actualizăm Criticul\n",
    "        update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn)\n",
    "\n",
    "        # Actualizăm starea curentă\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # Decay pentru epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "env.close()"
   ],
   "id": "faacefb3a967cf20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Reward: -7.6764453125\n",
      "Episode 2/1000, Reward: -8.890078125\n",
      "Episode 3/1000, Reward: -7.06744140625\n",
      "Episode 4/1000, Reward: -7.672167968749998\n",
      "Episode 5/1000, Reward: -8.282578125\n",
      "Episode 6/1000, Reward: -5.86423828125\n",
      "Episode 7/1000, Reward: -8.8881640625\n",
      "Episode 8/1000, Reward: -7.683671874999999\n",
      "Episode 9/1000, Reward: -8.87701171875\n",
      "Episode 10/1000, Reward: -7.6841796874999995\n",
      "Episode 11/1000, Reward: -8.274296875\n",
      "Episode 12/1000, Reward: -7.06439453125\n",
      "Episode 13/1000, Reward: -4.66697265625\n",
      "Episode 14/1000, Reward: -8.88904296875\n",
      "Episode 15/1000, Reward: -6.474921875\n",
      "Episode 16/1000, Reward: -7.0778125\n",
      "Episode 17/1000, Reward: -7.67880859375\n",
      "Episode 18/1000, Reward: -7.080859374999999\n",
      "Episode 19/1000, Reward: -7.6694140625\n",
      "Episode 20/1000, Reward: -7.680292968750001\n",
      "Episode 21/1000, Reward: -7.07185546875\n",
      "Episode 22/1000, Reward: -8.2769921875\n",
      "Episode 23/1000, Reward: -7.079589843749997\n",
      "Episode 24/1000, Reward: -7.06984375\n",
      "Episode 25/1000, Reward: -6.47646484375\n",
      "Episode 26/1000, Reward: -6.4551367187499995\n",
      "Episode 27/1000, Reward: -8.27544921875\n",
      "Episode 28/1000, Reward: -7.676347656249998\n",
      "Episode 29/1000, Reward: -7.084863281250001\n",
      "Episode 30/1000, Reward: -7.673710937500001\n",
      "Episode 31/1000, Reward: -8.8979296875\n",
      "Episode 32/1000, Reward: -6.465312500000001\n",
      "Episode 33/1000, Reward: -7.6877148437499985\n",
      "Episode 34/1000, Reward: -7.681484375\n",
      "Episode 35/1000, Reward: -8.284921875\n",
      "Episode 36/1000, Reward: -8.293808593749999\n",
      "Episode 37/1000, Reward: -8.291308593750001\n",
      "Episode 38/1000, Reward: -7.079238281249999\n",
      "Episode 39/1000, Reward: -7.678632812499999\n",
      "Episode 40/1000, Reward: -5.87046875\n",
      "Episode 41/1000, Reward: -7.67388671875\n",
      "Episode 42/1000, Reward: -7.669785156249999\n",
      "Episode 43/1000, Reward: -7.075234374999999\n",
      "Episode 44/1000, Reward: -4.660312500000001\n",
      "Episode 45/1000, Reward: -8.2737109375\n",
      "Episode 46/1000, Reward: -8.288164062499998\n",
      "Episode 47/1000, Reward: -5.857656249999999\n",
      "Episode 48/1000, Reward: -7.06615234375\n",
      "Episode 49/1000, Reward: -8.282343749999999\n",
      "Episode 50/1000, Reward: -5.865742187499999\n",
      "Episode 51/1000, Reward: -5.85248046875\n",
      "Episode 52/1000, Reward: -7.686660156249999\n",
      "Episode 53/1000, Reward: -7.075507812499999\n",
      "Episode 54/1000, Reward: -7.680507812499998\n",
      "Episode 55/1000, Reward: -6.464687499999999\n",
      "Episode 56/1000, Reward: -7.691035156250001\n",
      "Episode 57/1000, Reward: -7.680136718750001\n",
      "Episode 58/1000, Reward: -4.0590625\n",
      "Episode 59/1000, Reward: -6.462285156250001\n",
      "Episode 60/1000, Reward: -4.046093749999999\n",
      "Episode 61/1000, Reward: -5.272792968750001\n",
      "Episode 62/1000, Reward: -7.077285156249999\n",
      "Episode 63/1000, Reward: -5.857539062500001\n",
      "Episode 64/1000, Reward: -6.4590039062499995\n",
      "Episode 65/1000, Reward: -7.079726562499999\n",
      "Episode 66/1000, Reward: -6.470859375\n",
      "Episode 67/1000, Reward: -7.077597656249998\n",
      "Episode 68/1000, Reward: -7.68099609375\n",
      "Episode 69/1000, Reward: -4.061484375\n",
      "Episode 70/1000, Reward: -5.8684375\n",
      "Episode 71/1000, Reward: -5.256191406250001\n",
      "Episode 72/1000, Reward: -6.4745117187500005\n",
      "Episode 73/1000, Reward: -7.679765624999999\n",
      "Episode 74/1000, Reward: -7.0601953124999985\n",
      "Episode 75/1000, Reward: -2.243574218749999\n",
      "Episode 76/1000, Reward: -5.87056640625\n",
      "Episode 77/1000, Reward: -5.2778906249999995\n",
      "Episode 78/1000, Reward: -6.47263671875\n",
      "Episode 79/1000, Reward: -4.04935546875\n",
      "Episode 80/1000, Reward: -7.07408203125\n",
      "Episode 81/1000, Reward: -2.8640625\n",
      "Episode 82/1000, Reward: 1.9811914062500007\n",
      "Episode 83/1000, Reward: -7.673476562499999\n",
      "Episode 84/1000, Reward: -6.4668164062499995\n",
      "Episode 85/1000, Reward: 1.3981835937500002\n",
      "Episode 86/1000, Reward: -4.071171875000001\n",
      "Episode 87/1000, Reward: -3.458125\n",
      "Episode 88/1000, Reward: -5.2680078125\n",
      "Episode 89/1000, Reward: -5.8684375\n",
      "Episode 90/1000, Reward: -6.47509765625\n",
      "Episode 91/1000, Reward: 0.17779296875000017\n",
      "Episode 92/1000, Reward: -6.469453124999999\n",
      "Episode 93/1000, Reward: -6.466796875\n",
      "Episode 94/1000, Reward: -6.457226562499999\n",
      "Episode 95/1000, Reward: -2.248007812500001\n",
      "Episode 96/1000, Reward: -4.65818359375\n",
      "Episode 97/1000, Reward: -5.865644531249999\n",
      "Episode 98/1000, Reward: -4.65166015625\n",
      "Episode 99/1000, Reward: -7.662187499999998\n",
      "Episode 100/1000, Reward: -8.280859375\n",
      "Episode 101/1000, Reward: -7.0735546875\n",
      "Episode 102/1000, Reward: -3.4436132812499993\n",
      "Episode 103/1000, Reward: 1.3476953125000006\n",
      "Episode 104/1000, Reward: -5.87591796875\n",
      "Episode 105/1000, Reward: -2.2482617187500002\n",
      "Episode 106/1000, Reward: 3.77357421875\n",
      "Episode 107/1000, Reward: -7.685878906249998\n",
      "Episode 108/1000, Reward: -6.458183593749999\n",
      "Episode 109/1000, Reward: -4.0522656249999995\n",
      "Episode 110/1000, Reward: -6.476757812500001\n",
      "Episode 111/1000, Reward: -3.4387500000000006\n",
      "Episode 112/1000, Reward: -5.85873046875\n",
      "Episode 113/1000, Reward: -6.4658007812500005\n",
      "Episode 114/1000, Reward: -2.8421484374999997\n",
      "Episode 115/1000, Reward: -4.649609374999999\n",
      "Episode 116/1000, Reward: -1.6382421874999995\n",
      "Episode 117/1000, Reward: -1.6327734375000005\n",
      "Episode 118/1000, Reward: -4.659921875\n",
      "Episode 119/1000, Reward: -7.0776953124999995\n",
      "Episode 120/1000, Reward: -4.654999999999999\n",
      "Episode 121/1000, Reward: -2.2267382812499994\n",
      "Episode 122/1000, Reward: 3.7903124999999998\n",
      "Episode 123/1000, Reward: -4.6498828125\n",
      "Episode 124/1000, Reward: -6.4658789062499995\n",
      "Episode 125/1000, Reward: -7.68662109375\n",
      "Episode 126/1000, Reward: -3.4450390625000002\n",
      "Episode 127/1000, Reward: -2.2367187499999988\n",
      "Episode 128/1000, Reward: -7.66890625\n",
      "Episode 129/1000, Reward: -2.8441796874999996\n",
      "Episode 130/1000, Reward: -5.262636718749999\n",
      "Episode 131/1000, Reward: -4.0570703125\n",
      "Episode 132/1000, Reward: -5.247207031249999\n",
      "Episode 133/1000, Reward: 3.7783398437499995\n",
      "Episode 134/1000, Reward: -5.257441406250001\n",
      "Episode 135/1000, Reward: -5.264316406249999\n",
      "Episode 136/1000, Reward: -1.03943359375\n",
      "Episode 137/1000, Reward: 3.852089843749999\n",
      "Episode 138/1000, Reward: -5.87052734375\n",
      "Episode 139/1000, Reward: -1.6295898437500005\n",
      "Episode 140/1000, Reward: -3.444003906249999\n",
      "Episode 141/1000, Reward: -2.83390625\n",
      "Episode 142/1000, Reward: 1.9790234375000004\n",
      "Episode 143/1000, Reward: -4.6551757812500005\n",
      "Episode 144/1000, Reward: -7.074902343750001\n",
      "Episode 145/1000, Reward: -1.6278125000000006\n",
      "Episode 146/1000, Reward: 3.7818750000000008\n",
      "Episode 147/1000, Reward: -4.666542968750001\n",
      "Episode 148/1000, Reward: -2.8509179687500006\n",
      "Episode 149/1000, Reward: -5.263496093749999\n",
      "Episode 150/1000, Reward: 3.7927929687499993\n",
      "Episode 151/1000, Reward: -6.46919921875\n",
      "Episode 152/1000, Reward: -5.869414062500001\n",
      "Episode 153/1000, Reward: 0.17115234375\n",
      "Episode 154/1000, Reward: -4.059863281249999\n",
      "Episode 155/1000, Reward: -4.666816406249999\n",
      "Episode 156/1000, Reward: -5.26263671875\n",
      "Episode 157/1000, Reward: -1.624511718749999\n",
      "Episode 158/1000, Reward: -2.8598828124999995\n",
      "Episode 159/1000, Reward: -8.2786328125\n",
      "Episode 160/1000, Reward: 0.17187499999999978\n",
      "Episode 161/1000, Reward: -5.268222656249999\n",
      "Episode 162/1000, Reward: 3.815234375000001\n",
      "Episode 163/1000, Reward: 3.7828906249999994\n",
      "Episode 164/1000, Reward: -6.4779687500000005\n",
      "Episode 165/1000, Reward: 3.8102734375\n",
      "Episode 166/1000, Reward: 3.9002539062499997\n",
      "Episode 167/1000, Reward: -3.45099609375\n",
      "Episode 168/1000, Reward: -4.6567187500000005\n",
      "Episode 169/1000, Reward: -7.0650390624999995\n",
      "Episode 170/1000, Reward: -2.83455078125\n",
      "Episode 171/1000, Reward: -3.4647656250000005\n",
      "Episode 172/1000, Reward: 3.7651171875\n",
      "Episode 173/1000, Reward: -2.835722656249999\n",
      "Episode 174/1000, Reward: 3.792539062500001\n",
      "Episode 175/1000, Reward: 3.7999804687500003\n",
      "Episode 176/1000, Reward: -0.43197265624999925\n",
      "Episode 177/1000, Reward: -1.0347656249999995\n",
      "Episode 178/1000, Reward: -1.0349609374999993\n",
      "Episode 179/1000, Reward: -5.2690234375\n",
      "Episode 180/1000, Reward: -1.6462499999999993\n",
      "Episode 181/1000, Reward: 3.9088867187500007\n",
      "Episode 182/1000, Reward: -4.6559375\n",
      "Episode 183/1000, Reward: -2.8445507812499993\n",
      "Episode 184/1000, Reward: -5.85408203125\n",
      "Episode 185/1000, Reward: -6.469433593750001\n",
      "Episode 186/1000, Reward: -1.0444335937499991\n",
      "Episode 187/1000, Reward: 3.7778125000000014\n",
      "Episode 188/1000, Reward: 3.15587890625\n",
      "Episode 189/1000, Reward: 3.7607617187500004\n",
      "Episode 190/1000, Reward: -2.857089843749999\n",
      "Episode 191/1000, Reward: 1.9771875\n",
      "Episode 192/1000, Reward: -1.64208984375\n",
      "Episode 193/1000, Reward: 3.7769921874999985\n",
      "Episode 194/1000, Reward: -5.262499999999999\n",
      "Episode 195/1000, Reward: 3.7866210937499987\n",
      "Episode 196/1000, Reward: -3.452695312499999\n",
      "Episode 197/1000, Reward: 3.8508007812500002\n",
      "Episode 198/1000, Reward: 3.86361328125\n",
      "Episode 199/1000, Reward: 3.8316406249999995\n",
      "Episode 200/1000, Reward: 1.9798632812499986\n",
      "Episode 201/1000, Reward: 3.8191992187499997\n",
      "Episode 202/1000, Reward: -6.467011718749999\n",
      "Episode 203/1000, Reward: 3.970507812500001\n",
      "Episode 204/1000, Reward: -5.8709960937500005\n",
      "Episode 205/1000, Reward: 1.9813476562500014\n",
      "Episode 206/1000, Reward: 3.8139843750000013\n",
      "Episode 207/1000, Reward: 4.55322265625\n",
      "Episode 208/1000, Reward: -4.062226562499999\n",
      "Episode 209/1000, Reward: -1.652031249999998\n",
      "Episode 210/1000, Reward: -4.65193359375\n",
      "Episode 211/1000, Reward: -2.2593359375000004\n",
      "Episode 212/1000, Reward: 3.799374999999998\n",
      "Episode 213/1000, Reward: -5.269785156249999\n",
      "Episode 214/1000, Reward: 2.5875976562500007\n",
      "Episode 215/1000, Reward: 3.81046875\n",
      "Episode 216/1000, Reward: -2.8589843750000004\n",
      "Episode 217/1000, Reward: 3.827500000000001\n",
      "Episode 218/1000, Reward: -1.6365429687500004\n",
      "Episode 219/1000, Reward: -2.835722656249999\n",
      "Episode 220/1000, Reward: 3.7803125000000017\n",
      "Episode 221/1000, Reward: 3.7913671875000015\n",
      "Episode 222/1000, Reward: 3.1948046874999996\n",
      "Episode 223/1000, Reward: 3.7824999999999998\n",
      "Episode 224/1000, Reward: 3.835097656250001\n",
      "Episode 225/1000, Reward: 3.81595703125\n",
      "Episode 226/1000, Reward: -0.4482421875\n",
      "Episode 227/1000, Reward: 1.3811523437500006\n",
      "Episode 228/1000, Reward: -3.4393359375\n",
      "Episode 229/1000, Reward: 3.8360546875\n",
      "Episode 230/1000, Reward: 2.59333984375\n",
      "Episode 231/1000, Reward: 3.774199218749999\n",
      "Episode 232/1000, Reward: 3.799101562500001\n",
      "Episode 233/1000, Reward: 3.7638085937500003\n",
      "Episode 234/1000, Reward: 3.8340234375\n",
      "Episode 235/1000, Reward: 3.842402343750001\n",
      "Episode 236/1000, Reward: 4.2941796875\n",
      "Episode 237/1000, Reward: -4.050468749999999\n",
      "Episode 238/1000, Reward: -1.6188085937500003\n",
      "Episode 239/1000, Reward: -0.42367187500000003\n",
      "Episode 240/1000, Reward: 3.82712890625\n",
      "Episode 241/1000, Reward: -2.2391210937499997\n",
      "Episode 242/1000, Reward: -4.057402343749999\n",
      "Episode 243/1000, Reward: -1.6351562499999996\n",
      "Episode 244/1000, Reward: 3.8482421875000004\n",
      "Episode 245/1000, Reward: 3.7838476562500003\n",
      "Episode 246/1000, Reward: 3.802324218750001\n",
      "Episode 247/1000, Reward: -4.65861328125\n",
      "Episode 248/1000, Reward: 4.7275390625\n",
      "Episode 249/1000, Reward: 3.8190234375\n",
      "Episode 250/1000, Reward: 3.8450781250000006\n",
      "Episode 251/1000, Reward: 4.962265625000003\n",
      "Episode 252/1000, Reward: 3.8029882812499998\n",
      "Episode 253/1000, Reward: 3.6204687500000006\n",
      "Episode 254/1000, Reward: 5.916777343750001\n",
      "Episode 255/1000, Reward: 3.8689843750000015\n",
      "Episode 256/1000, Reward: 1.9912109374999996\n",
      "Episode 257/1000, Reward: 3.7789062500000017\n",
      "Episode 258/1000, Reward: -2.23140625\n",
      "Episode 259/1000, Reward: -0.43468749999999823\n",
      "Episode 260/1000, Reward: 1.9929296875\n",
      "Episode 261/1000, Reward: -2.25091796875\n",
      "Episode 262/1000, Reward: 0.1595703125000001\n",
      "Episode 263/1000, Reward: -1.6249414062499994\n",
      "Episode 264/1000, Reward: 3.955781250000001\n",
      "Episode 265/1000, Reward: 1.3644921875000011\n",
      "Episode 266/1000, Reward: 0.7671679687500004\n",
      "Episode 267/1000, Reward: 8.291542968749996\n",
      "Episode 268/1000, Reward: 3.7648046875000016\n",
      "Episode 269/1000, Reward: 0.16179687499999984\n",
      "Episode 270/1000, Reward: 3.81748046875\n",
      "Episode 271/1000, Reward: 3.7926171875000003\n",
      "Episode 272/1000, Reward: 3.78515625\n",
      "Episode 273/1000, Reward: 3.83009765625\n",
      "Episode 274/1000, Reward: 4.826699218750001\n",
      "Episode 275/1000, Reward: 4.921171875\n",
      "Episode 276/1000, Reward: 3.818496093750001\n",
      "Episode 277/1000, Reward: 3.8541601562500007\n",
      "Episode 278/1000, Reward: 8.34943359375\n",
      "Episode 279/1000, Reward: 3.8096875000000017\n",
      "Episode 280/1000, Reward: -1.0436132812500003\n",
      "Episode 281/1000, Reward: 3.83482421875\n",
      "Episode 282/1000, Reward: 4.234257812500001\n",
      "Episode 283/1000, Reward: 3.8674023437499994\n",
      "Episode 284/1000, Reward: 3.8445117187499993\n",
      "Episode 285/1000, Reward: 3.7897460937500003\n",
      "Episode 286/1000, Reward: 4.150195312499999\n",
      "Episode 287/1000, Reward: 1.9732226562500006\n",
      "Episode 288/1000, Reward: 3.7727343749999998\n",
      "Episode 289/1000, Reward: 3.8645117187499998\n",
      "Episode 290/1000, Reward: 3.8563476562500005\n",
      "Episode 291/1000, Reward: 3.8293749999999998\n",
      "Episode 292/1000, Reward: 3.1810351562500006\n",
      "Episode 293/1000, Reward: -2.2571484375000006\n",
      "Episode 294/1000, Reward: 3.850371093749998\n",
      "Episode 295/1000, Reward: 3.8083203125\n",
      "Episode 296/1000, Reward: 6.25287109375\n",
      "Episode 297/1000, Reward: 3.7959960937500004\n",
      "Episode 298/1000, Reward: 2.0025585937500003\n",
      "Episode 299/1000, Reward: 3.8316796875\n",
      "Episode 300/1000, Reward: 3.805625\n",
      "Episode 301/1000, Reward: 3.8200585937500002\n",
      "Episode 302/1000, Reward: 3.81419921875\n",
      "Episode 303/1000, Reward: 1.9732226562500006\n",
      "Episode 304/1000, Reward: -4.0569921875\n",
      "Episode 305/1000, Reward: 3.851250000000001\n",
      "Episode 306/1000, Reward: 3.788613281250001\n",
      "Episode 307/1000, Reward: 3.838398437500002\n",
      "Episode 308/1000, Reward: 3.8454296875000002\n",
      "Episode 309/1000, Reward: 3.787226562500001\n",
      "Episode 310/1000, Reward: 4.5528515625\n",
      "Episode 311/1000, Reward: 3.8041015624999996\n",
      "Episode 312/1000, Reward: 3.8062499999999995\n",
      "Episode 313/1000, Reward: 8.324765625000003\n",
      "Episode 314/1000, Reward: 3.7917773437500015\n",
      "Episode 315/1000, Reward: 3.802656250000001\n",
      "Episode 316/1000, Reward: 3.787988281250001\n",
      "Episode 317/1000, Reward: 3.7866015625000005\n",
      "Episode 318/1000, Reward: 2.6553906250000003\n",
      "Episode 319/1000, Reward: 4.00111328125\n",
      "Episode 320/1000, Reward: 3.849492187500001\n",
      "Episode 321/1000, Reward: 3.9351171875000004\n",
      "Episode 322/1000, Reward: 3.80134765625\n",
      "Episode 323/1000, Reward: 0.1779882812499991\n",
      "Episode 324/1000, Reward: 3.8472656250000004\n",
      "Episode 325/1000, Reward: 6.1336914062500005\n",
      "Episode 326/1000, Reward: 3.8468554687500003\n",
      "Episode 327/1000, Reward: 3.8018359375\n",
      "Episode 328/1000, Reward: 6.446445312500001\n",
      "Episode 329/1000, Reward: 3.1823437500000002\n",
      "Episode 330/1000, Reward: 4.849003906250001\n",
      "Episode 331/1000, Reward: 3.806874999999998\n",
      "Episode 332/1000, Reward: 4.61330078125\n",
      "Episode 333/1000, Reward: 3.8187500000000014\n",
      "Episode 334/1000, Reward: 3.80296875\n",
      "Episode 335/1000, Reward: 3.1736718749999984\n",
      "Episode 336/1000, Reward: 3.818613281250001\n",
      "Episode 337/1000, Reward: 3.8023828125000003\n",
      "Episode 338/1000, Reward: 1.9673632812499993\n",
      "Episode 339/1000, Reward: 8.269433593750001\n",
      "Episode 340/1000, Reward: 3.8294140625000006\n",
      "Episode 341/1000, Reward: 1.9732226562500006\n",
      "Episode 342/1000, Reward: 3.7801171874999984\n",
      "Episode 343/1000, Reward: 1.9732226562500006\n",
      "Episode 344/1000, Reward: 3.845332031250001\n",
      "Episode 345/1000, Reward: 2.592578125\n",
      "Episode 346/1000, Reward: 4.1430078125000005\n",
      "Episode 347/1000, Reward: 6.1472265625\n",
      "Episode 348/1000, Reward: 2.582402343750001\n",
      "Episode 349/1000, Reward: 3.843476562500002\n",
      "Episode 350/1000, Reward: 3.793671875000002\n",
      "Episode 351/1000, Reward: 3.812617187500001\n",
      "Episode 352/1000, Reward: 3.813164062499998\n",
      "Episode 353/1000, Reward: -1.0336523437499996\n",
      "Episode 354/1000, Reward: 3.8434960937500007\n",
      "Episode 355/1000, Reward: 3.803828124999998\n",
      "Episode 356/1000, Reward: 3.8296484374999986\n",
      "Episode 357/1000, Reward: 4.142929687499999\n",
      "Episode 358/1000, Reward: 3.869179687499999\n",
      "Episode 359/1000, Reward: 3.7808593749999995\n",
      "Episode 360/1000, Reward: 3.8630078125000002\n",
      "Episode 361/1000, Reward: 3.808144531249998\n",
      "Episode 362/1000, Reward: 3.7825781250000006\n",
      "Episode 363/1000, Reward: 3.8205273437499994\n",
      "Episode 364/1000, Reward: 2.8471484375\n",
      "Episode 365/1000, Reward: -0.4306640624999998\n",
      "Episode 366/1000, Reward: 3.8391406249999984\n",
      "Episode 367/1000, Reward: 3.819609374999999\n",
      "Episode 368/1000, Reward: 8.8942578125\n",
      "Episode 369/1000, Reward: 3.8265234375000015\n",
      "Episode 370/1000, Reward: 1.3782031250000015\n",
      "Episode 371/1000, Reward: 4.3445898437500015\n",
      "Episode 372/1000, Reward: 3.8319140624999997\n",
      "Episode 373/1000, Reward: 4.72818359375\n",
      "Episode 374/1000, Reward: 3.176874999999999\n",
      "Episode 375/1000, Reward: 3.81326171875\n",
      "Episode 376/1000, Reward: 3.788535156250001\n",
      "Episode 377/1000, Reward: 3.800703124999999\n",
      "Episode 378/1000, Reward: 1.9732226562500006\n",
      "Episode 379/1000, Reward: 3.8485546875\n",
      "Episode 380/1000, Reward: 3.849453125000001\n",
      "Episode 381/1000, Reward: 3.8591210937500007\n",
      "Episode 382/1000, Reward: 1.9732226562500006\n",
      "Episode 383/1000, Reward: 2.5597460937500003\n",
      "Episode 384/1000, Reward: 3.8054296875\n",
      "Episode 385/1000, Reward: 3.8336914062499994\n",
      "Episode 386/1000, Reward: 3.8447265625\n",
      "Episode 387/1000, Reward: 3.844316406250001\n",
      "Episode 388/1000, Reward: 3.836308593750001\n",
      "Episode 389/1000, Reward: 3.830156250000001\n",
      "Episode 390/1000, Reward: 3.801953125\n",
      "Episode 391/1000, Reward: 4.192207031249998\n",
      "Episode 392/1000, Reward: 1.9732226562500006\n",
      "Episode 393/1000, Reward: 3.8455664062500015\n",
      "Episode 394/1000, Reward: 1.9732226562500006\n",
      "Episode 395/1000, Reward: 4.33052734375\n",
      "Episode 396/1000, Reward: 3.82095703125\n",
      "Episode 397/1000, Reward: 1.9732226562500006\n",
      "Episode 398/1000, Reward: 1.9732226562500006\n",
      "Episode 399/1000, Reward: 3.83625\n",
      "Episode 400/1000, Reward: 2.6573437500000003\n",
      "Episode 401/1000, Reward: 3.7837304687500026\n",
      "Episode 402/1000, Reward: 2.1693750000000005\n",
      "Episode 403/1000, Reward: 3.81572265625\n",
      "Episode 404/1000, Reward: 3.85982421875\n",
      "Episode 405/1000, Reward: 2.6577343750000004\n",
      "Episode 406/1000, Reward: 4.55673828125\n",
      "Episode 407/1000, Reward: 4.603125000000002\n",
      "Episode 408/1000, Reward: 3.8435546875\n",
      "Episode 409/1000, Reward: 3.8481640625000004\n",
      "Episode 410/1000, Reward: 6.544453125\n",
      "Episode 411/1000, Reward: 3.856093750000001\n",
      "Episode 412/1000, Reward: 3.8266406249999996\n",
      "Episode 413/1000, Reward: 3.818242187500001\n",
      "Episode 414/1000, Reward: 1.9732226562500006\n",
      "Episode 415/1000, Reward: 1.9732226562500006\n",
      "Episode 416/1000, Reward: 3.4226757812500015\n",
      "Episode 417/1000, Reward: 3.841113281250001\n",
      "Episode 418/1000, Reward: 3.846152343749999\n",
      "Episode 419/1000, Reward: 3.8163867187500005\n",
      "Episode 420/1000, Reward: 3.834453125000003\n",
      "Episode 421/1000, Reward: 3.8151171874999994\n",
      "Episode 422/1000, Reward: 3.7940429687500012\n",
      "Episode 423/1000, Reward: 3.8516015625000017\n",
      "Episode 424/1000, Reward: 3.8248437500000003\n",
      "Episode 425/1000, Reward: 3.835078125000001\n",
      "Episode 426/1000, Reward: 3.7943359374999996\n",
      "Episode 427/1000, Reward: 6.03580078125\n",
      "Episode 428/1000, Reward: 3.84521484375\n",
      "Episode 429/1000, Reward: 3.2398046875000004\n",
      "Episode 430/1000, Reward: 3.8283984374999998\n",
      "Episode 431/1000, Reward: 3.853671875\n",
      "Episode 432/1000, Reward: 1.9732226562500006\n",
      "Episode 433/1000, Reward: 4.829765625\n",
      "Episode 434/1000, Reward: 3.84462890625\n",
      "Episode 435/1000, Reward: 3.855546875\n",
      "Episode 436/1000, Reward: 3.854296875000001\n",
      "Episode 437/1000, Reward: 3.815234375\n",
      "Episode 438/1000, Reward: 3.857421875\n",
      "Episode 439/1000, Reward: 3.8255664062500028\n",
      "Episode 440/1000, Reward: 3.8501562500000004\n",
      "Episode 441/1000, Reward: 3.8261718750000027\n",
      "Episode 442/1000, Reward: 1.9732226562500006\n",
      "Episode 443/1000, Reward: 3.874648437500002\n",
      "Episode 444/1000, Reward: 4.026328125000001\n",
      "Episode 445/1000, Reward: 5.92611328125\n",
      "Episode 446/1000, Reward: 3.807089843750001\n",
      "Episode 447/1000, Reward: 3.8117382812500002\n",
      "Episode 448/1000, Reward: 3.8390820312500007\n",
      "Episode 449/1000, Reward: 1.9732226562500006\n",
      "Episode 450/1000, Reward: 3.8581250000000002\n",
      "Episode 451/1000, Reward: 4.34478515625\n",
      "Episode 452/1000, Reward: 4.138925781250001\n",
      "Episode 453/1000, Reward: 1.9732226562500006\n",
      "Episode 454/1000, Reward: 2.65546875\n",
      "Episode 455/1000, Reward: 2.8469531250000006\n",
      "Episode 456/1000, Reward: 4.932695312499998\n",
      "Episode 457/1000, Reward: 3.8322656249999993\n",
      "Episode 458/1000, Reward: 3.8096875000000017\n",
      "Episode 459/1000, Reward: 2.9503125\n",
      "Episode 460/1000, Reward: 3.8729101562500006\n",
      "Episode 461/1000, Reward: 3.854335937500001\n",
      "Episode 462/1000, Reward: 3.9200585937500003\n",
      "Episode 463/1000, Reward: 3.712812499999999\n",
      "Episode 464/1000, Reward: 3.9171484374999994\n",
      "Episode 465/1000, Reward: 2.365390625\n",
      "Episode 466/1000, Reward: 3.847246093750001\n",
      "Episode 467/1000, Reward: 3.7883007812500002\n",
      "Episode 468/1000, Reward: 3.2398046875000004\n",
      "Episode 469/1000, Reward: 3.8499804687500014\n",
      "Episode 470/1000, Reward: 3.8316015625000013\n",
      "Episode 471/1000, Reward: 1.9732226562500006\n",
      "Episode 472/1000, Reward: 3.71412109375\n",
      "Episode 473/1000, Reward: 3.809980468750001\n",
      "Episode 474/1000, Reward: 3.8174218750000004\n",
      "Episode 475/1000, Reward: 3.8354687500000004\n",
      "Episode 476/1000, Reward: 4.563769531249999\n",
      "Episode 477/1000, Reward: 3.836015624999999\n",
      "Episode 478/1000, Reward: 3.7234960937500015\n",
      "Episode 479/1000, Reward: 3.8348046875\n",
      "Episode 480/1000, Reward: 1.9732226562500006\n",
      "Episode 481/1000, Reward: 4.14404296875\n",
      "Episode 482/1000, Reward: 4.0463671875\n",
      "Episode 483/1000, Reward: 3.802617187499999\n",
      "Episode 484/1000, Reward: 3.7793945312500004\n",
      "Episode 485/1000, Reward: -1.6231445312500004\n",
      "Episode 486/1000, Reward: 3.7766015625000007\n",
      "Episode 487/1000, Reward: 3.840820312500001\n",
      "Episode 488/1000, Reward: 4.9567187499999985\n",
      "Episode 489/1000, Reward: 3.8478710937500002\n",
      "Episode 490/1000, Reward: 8.634609375000002\n",
      "Episode 491/1000, Reward: 4.427929687500001\n",
      "Episode 492/1000, Reward: 3.8470312499999997\n",
      "Episode 493/1000, Reward: 3.8609765624999994\n",
      "Episode 494/1000, Reward: 3.8305078125000014\n",
      "Episode 495/1000, Reward: 3.8402539062500014\n",
      "Episode 496/1000, Reward: 3.8378906250000013\n",
      "Episode 497/1000, Reward: 3.851171875000002\n",
      "Episode 498/1000, Reward: 3.8604687500000017\n",
      "Episode 499/1000, Reward: 4.346191406250001\n",
      "Episode 500/1000, Reward: 3.843964843750002\n",
      "Episode 501/1000, Reward: 3.8640820312499997\n",
      "Episode 502/1000, Reward: 3.80857421875\n",
      "Episode 503/1000, Reward: 3.8253125000000003\n",
      "Episode 504/1000, Reward: 4.953281250000001\n",
      "Episode 505/1000, Reward: 3.8082617187500007\n",
      "Episode 506/1000, Reward: 3.8030859375000006\n",
      "Episode 507/1000, Reward: 8.354531250000003\n",
      "Episode 508/1000, Reward: 4.315390625\n",
      "Episode 509/1000, Reward: 3.80806640625\n",
      "Episode 510/1000, Reward: 3.850957031250001\n",
      "Episode 511/1000, Reward: 3.8505078125000027\n",
      "Episode 512/1000, Reward: 3.8408398437500004\n",
      "Episode 513/1000, Reward: 4.627773437499999\n",
      "Episode 514/1000, Reward: 4.3382812500000005\n",
      "Episode 515/1000, Reward: 3.848515625\n",
      "Episode 516/1000, Reward: 3.816308593750002\n",
      "Episode 517/1000, Reward: 9.336289062499997\n",
      "Episode 518/1000, Reward: 1.9732226562500006\n",
      "Episode 519/1000, Reward: 3.8559765625000004\n",
      "Episode 520/1000, Reward: 4.449511718750001\n",
      "Episode 521/1000, Reward: 3.8278906250000007\n",
      "Episode 522/1000, Reward: 3.852265625000002\n",
      "Episode 523/1000, Reward: 6.759238281250004\n",
      "Episode 524/1000, Reward: 3.859140625000001\n",
      "Episode 525/1000, Reward: 6.219296875000001\n",
      "Episode 526/1000, Reward: 3.823808593750001\n",
      "Episode 527/1000, Reward: 4.924824218750001\n",
      "Episode 528/1000, Reward: 3.813593749999999\n",
      "Episode 529/1000, Reward: 3.8191210937500006\n",
      "Episode 530/1000, Reward: 4.920429687499999\n",
      "Episode 531/1000, Reward: 3.85517578125\n",
      "Episode 532/1000, Reward: 3.850703124999999\n",
      "Episode 533/1000, Reward: 6.295078125000001\n",
      "Episode 534/1000, Reward: 9.115058593750003\n",
      "Episode 535/1000, Reward: 3.8482421875000004\n",
      "Episode 536/1000, Reward: 3.8371875000000006\n",
      "Episode 537/1000, Reward: 3.8399414062500017\n",
      "Episode 538/1000, Reward: 4.230820312500001\n",
      "Episode 539/1000, Reward: 3.9622460937500015\n",
      "Episode 540/1000, Reward: 4.36732421875\n",
      "Episode 541/1000, Reward: 4.111171875\n",
      "Episode 542/1000, Reward: 3.8428124999999995\n",
      "Episode 543/1000, Reward: 3.8667578124999995\n",
      "Episode 544/1000, Reward: 4.208906249999998\n",
      "Episode 545/1000, Reward: 3.82279296875\n",
      "Episode 546/1000, Reward: 3.809042968750001\n",
      "Episode 547/1000, Reward: 3.7942382812499993\n",
      "Episode 548/1000, Reward: 4.94123046875\n",
      "Episode 549/1000, Reward: 4.554785156250001\n",
      "Episode 550/1000, Reward: 3.7855078125000006\n",
      "Episode 551/1000, Reward: 3.8262304687500013\n",
      "Episode 552/1000, Reward: 3.84763671875\n",
      "Episode 553/1000, Reward: 3.843886718750003\n",
      "Episode 554/1000, Reward: 3.8344726562500018\n",
      "Episode 555/1000, Reward: 3.7831640624999996\n",
      "Episode 556/1000, Reward: 3.803515625\n",
      "Episode 557/1000, Reward: 3.85990234375\n",
      "Episode 558/1000, Reward: 3.8312500000000016\n",
      "Episode 559/1000, Reward: 3.8633398437500004\n",
      "Episode 560/1000, Reward: 4.47228515625\n",
      "Episode 561/1000, Reward: 4.250703124999999\n",
      "Episode 562/1000, Reward: 3.792734375000001\n",
      "Episode 563/1000, Reward: 4.65607421875\n",
      "Episode 564/1000, Reward: 4.454843749999999\n",
      "Episode 565/1000, Reward: 4.836171875000002\n",
      "Episode 566/1000, Reward: 3.8173632812499996\n",
      "Episode 567/1000, Reward: 3.818828125000002\n",
      "Episode 568/1000, Reward: 3.18197265625\n",
      "Episode 569/1000, Reward: 3.8037499999999986\n",
      "Episode 570/1000, Reward: 1.9732226562500006\n",
      "Episode 571/1000, Reward: 8.33919921875\n",
      "Episode 572/1000, Reward: 3.8372265624999997\n",
      "Episode 573/1000, Reward: 3.827636718749999\n",
      "Episode 574/1000, Reward: 3.8007617187500005\n",
      "Episode 575/1000, Reward: 3.837480468750001\n",
      "Episode 576/1000, Reward: 4.448066406250001\n",
      "Episode 577/1000, Reward: 3.820488281250001\n",
      "Episode 578/1000, Reward: 3.8450585937499993\n",
      "Episode 579/1000, Reward: 3.8271093750000014\n",
      "Episode 580/1000, Reward: 3.8567382812500006\n",
      "Episode 581/1000, Reward: 3.85189453125\n",
      "Episode 582/1000, Reward: 3.86890625\n",
      "Episode 583/1000, Reward: 3.76595703125\n",
      "Episode 584/1000, Reward: 4.25009765625\n",
      "Episode 585/1000, Reward: 4.142031250000001\n",
      "Episode 586/1000, Reward: 3.8614453125000017\n",
      "Episode 587/1000, Reward: 3.8456054687500014\n",
      "Episode 588/1000, Reward: 3.854492187499999\n",
      "Episode 589/1000, Reward: 4.736152343750001\n",
      "Episode 590/1000, Reward: 3.8612890625\n",
      "Episode 591/1000, Reward: 3.8652148437500005\n",
      "Episode 592/1000, Reward: 3.8250195312500015\n",
      "Episode 593/1000, Reward: 4.759550781250001\n",
      "Episode 594/1000, Reward: 3.8595898437499994\n",
      "Episode 595/1000, Reward: 3.8487695312499994\n",
      "Episode 596/1000, Reward: 3.81400390625\n",
      "Episode 597/1000, Reward: 3.8148828125000005\n",
      "Episode 598/1000, Reward: 3.8114453125\n",
      "Episode 599/1000, Reward: 4.1539453125\n",
      "Episode 600/1000, Reward: 3.8043359375000008\n",
      "Episode 601/1000, Reward: 3.8548437500000015\n",
      "Episode 602/1000, Reward: 3.8259765624999997\n",
      "Episode 603/1000, Reward: 4.232265625000002\n",
      "Episode 604/1000, Reward: 3.8473046874999985\n",
      "Episode 605/1000, Reward: 3.8591601562499998\n",
      "Episode 606/1000, Reward: 3.8639843749999994\n",
      "Episode 607/1000, Reward: 3.8516406250000026\n",
      "Episode 608/1000, Reward: 4.8237890625\n",
      "Episode 609/1000, Reward: 3.867148437500001\n",
      "Episode 610/1000, Reward: 6.666152343750001\n",
      "Episode 611/1000, Reward: 3.8458007812500004\n",
      "Episode 612/1000, Reward: 6.5558593750000025\n",
      "Episode 613/1000, Reward: 3.8573437500000027\n",
      "Episode 614/1000, Reward: 5.92912109375\n",
      "Episode 615/1000, Reward: 6.730429687500001\n",
      "Episode 616/1000, Reward: 3.7908203125\n",
      "Episode 617/1000, Reward: 3.85005859375\n",
      "Episode 618/1000, Reward: 3.82509765625\n",
      "Episode 619/1000, Reward: 3.8300585937500005\n",
      "Episode 620/1000, Reward: 4.771777343750001\n",
      "Episode 621/1000, Reward: 3.8239257812500016\n",
      "Episode 622/1000, Reward: 6.2347656250000005\n",
      "Episode 623/1000, Reward: 3.825859375000002\n",
      "Episode 624/1000, Reward: 3.84216796875\n",
      "Episode 625/1000, Reward: 3.847539062500001\n",
      "Episode 626/1000, Reward: 6.3220703125\n",
      "Episode 627/1000, Reward: 3.87591796875\n",
      "Episode 628/1000, Reward: 3.866113281250002\n",
      "Episode 629/1000, Reward: 4.1613671875\n",
      "Episode 630/1000, Reward: 4.9448242187500036\n",
      "Episode 631/1000, Reward: 4.26095703125\n",
      "Episode 632/1000, Reward: 3.8456054687500023\n",
      "Episode 633/1000, Reward: 3.7984375000000004\n",
      "Episode 634/1000, Reward: 3.8650585937500006\n",
      "Episode 635/1000, Reward: 4.659101562500001\n",
      "Episode 636/1000, Reward: 5.95884765625\n",
      "Episode 637/1000, Reward: 6.436523437499999\n",
      "Episode 638/1000, Reward: 3.8625195312499994\n",
      "Episode 639/1000, Reward: 4.260175781250001\n",
      "Episode 640/1000, Reward: 3.8699414062500006\n",
      "Episode 641/1000, Reward: 3.8582421875\n",
      "Episode 642/1000, Reward: 3.8084570312500006\n",
      "Episode 643/1000, Reward: 3.8364062500000014\n",
      "Episode 644/1000, Reward: 8.334687500000005\n",
      "Episode 645/1000, Reward: 4.36986328125\n",
      "Episode 646/1000, Reward: 3.8461914062500004\n",
      "Episode 647/1000, Reward: 3.8654687499999993\n",
      "Episode 648/1000, Reward: 4.4633203125\n",
      "Episode 649/1000, Reward: 3.8522460937500016\n",
      "Episode 650/1000, Reward: 3.8614453125000003\n",
      "Episode 651/1000, Reward: 3.853066406250001\n",
      "Episode 652/1000, Reward: 3.857265625\n",
      "Episode 653/1000, Reward: 4.431699218750001\n",
      "Episode 654/1000, Reward: 5.9871484375\n",
      "Episode 655/1000, Reward: 4.063906250000002\n",
      "Episode 656/1000, Reward: 3.954472656250001\n",
      "Episode 657/1000, Reward: 4.869277343750001\n",
      "Episode 658/1000, Reward: 8.274218750000001\n",
      "Episode 659/1000, Reward: 4.858906249999999\n",
      "Episode 660/1000, Reward: 3.8513671875\n",
      "Episode 661/1000, Reward: 6.050605468750001\n",
      "Episode 662/1000, Reward: 3.812988281249999\n",
      "Episode 663/1000, Reward: 13.827382812500005\n",
      "Episode 664/1000, Reward: 5.959042968750003\n",
      "Episode 665/1000, Reward: 4.457519531250002\n",
      "Episode 666/1000, Reward: 3.8105859375000004\n",
      "Episode 667/1000, Reward: 6.055449218750001\n",
      "Episode 668/1000, Reward: 4.735019531250001\n",
      "Episode 669/1000, Reward: 12.815742187500007\n",
      "Episode 670/1000, Reward: 3.87146484375\n",
      "Episode 671/1000, Reward: 3.8131445312500007\n",
      "Episode 672/1000, Reward: 4.253828125000001\n",
      "Episode 673/1000, Reward: 6.23435546875\n",
      "Episode 674/1000, Reward: 3.7914648437499983\n",
      "Episode 675/1000, Reward: 3.85609375\n",
      "Episode 676/1000, Reward: 3.7991406249999997\n",
      "Episode 677/1000, Reward: 6.1468359375\n",
      "Episode 678/1000, Reward: 4.258535156249999\n",
      "Episode 679/1000, Reward: 3.8347656249999993\n",
      "Episode 680/1000, Reward: 6.365527343749999\n",
      "Episode 681/1000, Reward: 3.8528124999999998\n",
      "Episode 682/1000, Reward: 3.868125000000001\n",
      "Episode 683/1000, Reward: 4.655000000000002\n",
      "Episode 684/1000, Reward: 3.83955078125\n",
      "Episode 685/1000, Reward: 4.4667773437500005\n",
      "Episode 686/1000, Reward: 3.8363281250000023\n",
      "Episode 687/1000, Reward: 3.850273437500001\n",
      "Episode 688/1000, Reward: 3.8443359374999995\n",
      "Episode 689/1000, Reward: 4.66154296875\n",
      "Episode 690/1000, Reward: 3.863281250000002\n",
      "Episode 691/1000, Reward: 4.159570312500001\n",
      "Episode 692/1000, Reward: 4.149179687500001\n",
      "Episode 693/1000, Reward: 3.8184570312500012\n",
      "Episode 694/1000, Reward: 6.731933593750001\n",
      "Episode 695/1000, Reward: 3.7874218750000006\n",
      "Episode 696/1000, Reward: 3.8070507812500014\n",
      "Episode 697/1000, Reward: 6.725058593749999\n",
      "Episode 698/1000, Reward: 6.746152343749999\n",
      "Episode 699/1000, Reward: 3.8641015625\n",
      "Episode 700/1000, Reward: 4.2749609375\n",
      "Episode 701/1000, Reward: 3.8191796875\n",
      "Episode 702/1000, Reward: 8.282968749999998\n",
      "Episode 703/1000, Reward: 6.555722656250001\n",
      "Episode 704/1000, Reward: 3.8150390625000004\n",
      "Episode 705/1000, Reward: 3.9341992187500012\n",
      "Episode 706/1000, Reward: 3.86876953125\n",
      "Episode 707/1000, Reward: 3.838085937500001\n",
      "Episode 708/1000, Reward: 3.8454101562500016\n",
      "Episode 709/1000, Reward: 3.8613671875000013\n",
      "Episode 710/1000, Reward: 8.317031249999998\n",
      "Episode 711/1000, Reward: 3.835078125000001\n",
      "Episode 712/1000, Reward: 4.16806640625\n",
      "Episode 713/1000, Reward: 3.8663671875000007\n",
      "Episode 714/1000, Reward: 3.845312500000001\n",
      "Episode 715/1000, Reward: 3.8588671874999996\n",
      "Episode 716/1000, Reward: 3.8643359375\n",
      "Episode 717/1000, Reward: 13.093085937499996\n",
      "Episode 718/1000, Reward: 4.257597656250001\n",
      "Episode 719/1000, Reward: 8.284101562500005\n",
      "Episode 720/1000, Reward: 3.8299804687499996\n",
      "Episode 721/1000, Reward: 3.8526757812500003\n",
      "Episode 722/1000, Reward: 4.450097656250001\n",
      "Episode 723/1000, Reward: 3.8135351562500013\n",
      "Episode 724/1000, Reward: 4.277050781250001\n",
      "Episode 725/1000, Reward: 5.968535156250001\n",
      "Episode 726/1000, Reward: 3.8486914062500004\n",
      "Episode 727/1000, Reward: 8.717832031250003\n",
      "Episode 728/1000, Reward: 3.8377343749999997\n",
      "Episode 729/1000, Reward: 3.8551171875000003\n",
      "Episode 730/1000, Reward: 8.342382812500006\n",
      "Episode 731/1000, Reward: 3.8444140624999994\n",
      "Episode 732/1000, Reward: 3.861210937499999\n",
      "Episode 733/1000, Reward: 3.8570312500000004\n",
      "Episode 734/1000, Reward: 3.8282226562500004\n",
      "Episode 735/1000, Reward: 4.842480468750001\n",
      "Episode 736/1000, Reward: 8.330781250000003\n",
      "Episode 737/1000, Reward: 3.8301562500000017\n",
      "Episode 738/1000, Reward: 3.8561523437500007\n",
      "Episode 739/1000, Reward: 4.158203125000002\n",
      "Episode 740/1000, Reward: 3.961757812499999\n",
      "Episode 741/1000, Reward: 3.931445312500001\n",
      "Episode 742/1000, Reward: 6.4255078124999985\n",
      "Episode 743/1000, Reward: 4.0176367187499995\n",
      "Episode 744/1000, Reward: 3.85453125\n",
      "Episode 745/1000, Reward: 3.9679492187500007\n",
      "Episode 746/1000, Reward: 3.807734375\n",
      "Episode 747/1000, Reward: 3.8592382812499997\n",
      "Episode 748/1000, Reward: 3.863378906250002\n",
      "Episode 749/1000, Reward: 4.25072265625\n",
      "Episode 750/1000, Reward: 3.834980468749999\n",
      "Episode 751/1000, Reward: 6.124570312500001\n",
      "Episode 752/1000, Reward: 3.846152343750002\n",
      "Episode 753/1000, Reward: 4.0494140624999995\n",
      "Episode 754/1000, Reward: 3.969433593750002\n",
      "Episode 755/1000, Reward: 6.0556835937499995\n",
      "Episode 756/1000, Reward: 3.856484375000001\n",
      "Episode 757/1000, Reward: 3.8246875\n",
      "Episode 758/1000, Reward: 3.7894531249999988\n",
      "Episode 759/1000, Reward: 3.7961328125000007\n",
      "Episode 760/1000, Reward: 3.806191406249999\n",
      "Episode 761/1000, Reward: 3.8481640624999995\n",
      "Episode 762/1000, Reward: 6.356250000000002\n",
      "Episode 763/1000, Reward: 3.8221484374999983\n",
      "Episode 764/1000, Reward: 3.7974609375000004\n",
      "Episode 765/1000, Reward: 3.86796875\n",
      "Episode 766/1000, Reward: 3.85029296875\n",
      "Episode 767/1000, Reward: 6.160332031250001\n",
      "Episode 768/1000, Reward: 3.845664062500001\n",
      "Episode 769/1000, Reward: 3.8650585937500006\n",
      "Episode 770/1000, Reward: 3.8406640625\n",
      "Episode 771/1000, Reward: 3.8426171875000015\n",
      "Episode 772/1000, Reward: 6.35791015625\n",
      "Episode 773/1000, Reward: 4.341523437500001\n",
      "Episode 774/1000, Reward: 3.8254296875000025\n",
      "Episode 775/1000, Reward: 3.7922265625\n",
      "Episode 776/1000, Reward: 3.8123632812499992\n",
      "Episode 777/1000, Reward: 3.8365625000000003\n",
      "Episode 778/1000, Reward: 4.174062500000002\n",
      "Episode 779/1000, Reward: 3.8704296875000015\n",
      "Episode 780/1000, Reward: 3.853320312500002\n",
      "Episode 781/1000, Reward: 8.2861328125\n",
      "Episode 782/1000, Reward: 3.8625781250000015\n",
      "Episode 783/1000, Reward: 4.837851562499999\n",
      "Episode 784/1000, Reward: 3.85498046875\n",
      "Episode 785/1000, Reward: 3.8305273437499987\n",
      "Episode 786/1000, Reward: 3.8503125000000002\n",
      "Episode 787/1000, Reward: 3.796796875000001\n",
      "Episode 788/1000, Reward: 3.8575781250000007\n",
      "Episode 789/1000, Reward: 3.8011328124999997\n",
      "Episode 790/1000, Reward: 4.2421679687500005\n",
      "Episode 791/1000, Reward: 3.8496679687500004\n",
      "Episode 792/1000, Reward: 6.539726562499998\n",
      "Episode 793/1000, Reward: 3.8488671875000002\n",
      "Episode 794/1000, Reward: 4.148125\n",
      "Episode 795/1000, Reward: 8.326835937499997\n",
      "Episode 796/1000, Reward: 4.047128906249998\n",
      "Episode 797/1000, Reward: 6.465996093749999\n",
      "Episode 798/1000, Reward: 6.15712890625\n",
      "Episode 799/1000, Reward: 6.739785156249999\n",
      "Episode 800/1000, Reward: 3.8631054687500024\n",
      "Episode 801/1000, Reward: 3.96126953125\n",
      "Episode 802/1000, Reward: 3.8346093749999994\n",
      "Episode 803/1000, Reward: 3.9460156249999985\n",
      "Episode 804/1000, Reward: 6.6478515625000005\n",
      "Episode 805/1000, Reward: 3.8508593749999998\n",
      "Episode 806/1000, Reward: 3.81291015625\n",
      "Episode 807/1000, Reward: 3.869726562500001\n",
      "Episode 808/1000, Reward: 3.8545312500000017\n",
      "Episode 809/1000, Reward: 3.832968750000001\n",
      "Episode 810/1000, Reward: 3.831249999999999\n",
      "Episode 811/1000, Reward: 3.852578125\n",
      "Episode 812/1000, Reward: 3.825820312500001\n",
      "Episode 813/1000, Reward: 3.8548046875000006\n",
      "Episode 814/1000, Reward: 4.1758203125\n",
      "Episode 815/1000, Reward: 8.924765625\n",
      "Episode 816/1000, Reward: 8.282246093750002\n",
      "Episode 817/1000, Reward: 6.3455078125\n",
      "Episode 818/1000, Reward: 4.556132812500001\n",
      "Episode 819/1000, Reward: 6.761757812499999\n",
      "Episode 820/1000, Reward: 3.80171875\n",
      "Episode 821/1000, Reward: 3.861054687500001\n",
      "Episode 822/1000, Reward: 4.6574414062499985\n",
      "Episode 823/1000, Reward: 3.828203125000001\n",
      "Episode 824/1000, Reward: 4.759960937500002\n",
      "Episode 825/1000, Reward: 6.040488281249999\n",
      "Episode 826/1000, Reward: 3.7931054687499994\n",
      "Episode 827/1000, Reward: 3.8592773437500023\n",
      "Episode 828/1000, Reward: 8.337871093750001\n",
      "Episode 829/1000, Reward: 6.054375\n",
      "Episode 830/1000, Reward: 3.8405664062500025\n",
      "Episode 831/1000, Reward: 3.9315624999999996\n",
      "Episode 832/1000, Reward: 3.841601562500001\n",
      "Episode 833/1000, Reward: 3.8692578125000012\n",
      "Episode 834/1000, Reward: 8.296347656250001\n",
      "Episode 835/1000, Reward: 3.8546484375000007\n",
      "Episode 836/1000, Reward: 4.448886718750001\n",
      "Episode 837/1000, Reward: 3.8536914062500007\n",
      "Episode 838/1000, Reward: 4.557421874999999\n",
      "Episode 839/1000, Reward: 3.806699218750001\n",
      "Episode 840/1000, Reward: 3.9721484375000014\n",
      "Episode 841/1000, Reward: 3.8469921875000006\n",
      "Episode 842/1000, Reward: 3.8529882812499996\n",
      "Episode 843/1000, Reward: 5.969375\n",
      "Episode 844/1000, Reward: 3.83330078125\n",
      "Episode 845/1000, Reward: 3.8235742187499993\n",
      "Episode 846/1000, Reward: 3.820781249999999\n",
      "Episode 847/1000, Reward: 3.8476953125000004\n",
      "Episode 848/1000, Reward: 4.4306640625\n",
      "Episode 849/1000, Reward: 3.8092578125\n",
      "Episode 850/1000, Reward: 3.8407226562500005\n",
      "Episode 851/1000, Reward: 4.763320312499999\n",
      "Episode 852/1000, Reward: 3.8631835937500005\n",
      "Episode 853/1000, Reward: 3.951015625\n",
      "Episode 854/1000, Reward: 3.8200000000000003\n",
      "Episode 855/1000, Reward: 6.061972656250002\n",
      "Episode 856/1000, Reward: 4.1758203125\n",
      "Episode 857/1000, Reward: 3.8286328125\n",
      "Episode 858/1000, Reward: 6.049531250000001\n",
      "Episode 859/1000, Reward: 3.853320312500002\n",
      "Episode 860/1000, Reward: 4.112617187500001\n",
      "Episode 861/1000, Reward: 4.754882812499999\n",
      "Episode 862/1000, Reward: 3.8470703124999996\n",
      "Episode 863/1000, Reward: 3.8307226562500007\n",
      "Episode 864/1000, Reward: -2.84248046875\n",
      "Episode 865/1000, Reward: 3.8166796875000015\n",
      "Episode 866/1000, Reward: 3.8539062499999988\n",
      "Episode 867/1000, Reward: 3.7933203125000023\n",
      "Episode 868/1000, Reward: 4.947558593749998\n",
      "Episode 869/1000, Reward: 4.562265625\n",
      "Episode 870/1000, Reward: 3.8438085937499995\n",
      "Episode 871/1000, Reward: 3.830000000000001\n",
      "Episode 872/1000, Reward: 8.347558593750001\n",
      "Episode 873/1000, Reward: 3.865605468750001\n",
      "Episode 874/1000, Reward: 3.8341992187500002\n",
      "Episode 875/1000, Reward: 6.3514453125\n",
      "Episode 876/1000, Reward: 3.853398437500001\n",
      "Episode 877/1000, Reward: 6.23068359375\n",
      "Episode 878/1000, Reward: 6.029433593750001\n",
      "Episode 879/1000, Reward: 3.8108984374999992\n",
      "Episode 880/1000, Reward: 3.798125000000001\n",
      "Episode 881/1000, Reward: 4.749492187500001\n",
      "Episode 882/1000, Reward: 3.9232031250000006\n",
      "Episode 883/1000, Reward: 3.8647265625000014\n",
      "Episode 884/1000, Reward: 6.264101562500001\n",
      "Episode 885/1000, Reward: 3.8621484375000015\n",
      "Episode 886/1000, Reward: 3.8624414062500003\n",
      "Episode 887/1000, Reward: 3.867675781250001\n",
      "Episode 888/1000, Reward: 3.8466992187499995\n",
      "Episode 889/1000, Reward: 3.84716796875\n",
      "Episode 890/1000, Reward: 4.341347656249998\n",
      "Episode 891/1000, Reward: 3.815917968750001\n",
      "Episode 892/1000, Reward: 3.859257812499999\n",
      "Episode 893/1000, Reward: 3.8562109374999998\n",
      "Episode 894/1000, Reward: 4.662656250000001\n",
      "Episode 895/1000, Reward: 3.8414843749999994\n",
      "Episode 896/1000, Reward: 4.406875000000001\n",
      "Episode 897/1000, Reward: 4.8386328125\n",
      "Episode 898/1000, Reward: 3.8570312499999986\n",
      "Episode 899/1000, Reward: 3.83228515625\n",
      "Episode 900/1000, Reward: 6.142558593750001\n",
      "Episode 901/1000, Reward: 3.8400000000000003\n",
      "Episode 902/1000, Reward: 9.423437499999997\n",
      "Episode 903/1000, Reward: 4.87013671875\n",
      "Episode 904/1000, Reward: 4.47296875\n",
      "Episode 905/1000, Reward: 8.307695312500003\n",
      "Episode 906/1000, Reward: 3.7865234375000005\n",
      "Episode 907/1000, Reward: 3.8628906250000012\n",
      "Episode 908/1000, Reward: 3.855507812499999\n",
      "Episode 909/1000, Reward: 3.8488867187499993\n",
      "Episode 910/1000, Reward: 3.801718750000002\n",
      "Episode 911/1000, Reward: 3.7990625000000002\n",
      "Episode 912/1000, Reward: 3.8167187500000006\n",
      "Episode 913/1000, Reward: 3.8588281249999987\n",
      "Episode 914/1000, Reward: 3.8206445312499993\n",
      "Episode 915/1000, Reward: 6.356738281250001\n",
      "Episode 916/1000, Reward: 3.8298632812499998\n",
      "Episode 917/1000, Reward: 3.8429296875\n",
      "Episode 918/1000, Reward: 4.239843749999998\n",
      "Episode 919/1000, Reward: 3.851054687500002\n",
      "Episode 920/1000, Reward: 4.6393359375\n",
      "Episode 921/1000, Reward: 3.7976953125\n",
      "Episode 922/1000, Reward: 6.515761718749999\n",
      "Episode 923/1000, Reward: 3.810195312500001\n",
      "Episode 924/1000, Reward: 3.8667578125\n",
      "Episode 925/1000, Reward: 4.069902343750001\n",
      "Episode 926/1000, Reward: 3.865800781249999\n",
      "Episode 927/1000, Reward: 3.8495898437500022\n",
      "Episode 928/1000, Reward: 3.800058593750001\n",
      "Episode 929/1000, Reward: 3.8166015625000007\n",
      "Episode 930/1000, Reward: 8.29734375\n",
      "Episode 931/1000, Reward: 3.8546875000000003\n",
      "Episode 932/1000, Reward: 3.8438476562500012\n",
      "Episode 933/1000, Reward: 3.8566796875000016\n",
      "Episode 934/1000, Reward: 3.8748242187500024\n",
      "Episode 935/1000, Reward: 3.7744335937500004\n",
      "Episode 936/1000, Reward: 4.640703125\n",
      "Episode 937/1000, Reward: 8.323671874999999\n",
      "Episode 938/1000, Reward: 3.838417968750001\n",
      "Episode 939/1000, Reward: 3.8324609375000014\n",
      "Episode 940/1000, Reward: 3.822656250000002\n",
      "Episode 941/1000, Reward: 3.8762109375000007\n",
      "Episode 942/1000, Reward: 13.211249999999996\n",
      "Episode 943/1000, Reward: 3.8757421875\n",
      "Episode 944/1000, Reward: 4.8328320312500015\n",
      "Episode 945/1000, Reward: 3.8700585937500014\n",
      "Episode 946/1000, Reward: 8.33484375\n",
      "Episode 947/1000, Reward: 3.858320312499999\n",
      "Episode 948/1000, Reward: 3.854375000000002\n",
      "Episode 949/1000, Reward: 3.8390234375000003\n",
      "Episode 950/1000, Reward: 3.866425781250001\n",
      "Episode 951/1000, Reward: 3.8334374999999996\n",
      "Episode 952/1000, Reward: 3.862578125\n",
      "Episode 953/1000, Reward: 4.167890624999997\n",
      "Episode 954/1000, Reward: 4.04892578125\n",
      "Episode 955/1000, Reward: 6.344628906250002\n",
      "Episode 956/1000, Reward: 3.8523046875\n",
      "Episode 957/1000, Reward: 6.50447265625\n",
      "Episode 958/1000, Reward: 4.2755078125\n",
      "Episode 959/1000, Reward: 3.8542578125\n",
      "Episode 960/1000, Reward: 4.1674218750000005\n",
      "Episode 961/1000, Reward: 4.1613671875000025\n",
      "Episode 962/1000, Reward: 3.8328124999999993\n",
      "Episode 963/1000, Reward: 4.0362890625\n",
      "Episode 964/1000, Reward: 8.299042968750006\n",
      "Episode 965/1000, Reward: 3.8681640624999987\n",
      "Episode 966/1000, Reward: 3.8570117187500026\n",
      "Episode 967/1000, Reward: 4.62923828125\n",
      "Episode 968/1000, Reward: 3.85115234375\n",
      "Episode 969/1000, Reward: 3.803867187499999\n",
      "Episode 970/1000, Reward: 4.9599414062500005\n",
      "Episode 971/1000, Reward: 8.311210937500006\n",
      "Episode 972/1000, Reward: 8.330527343750003\n",
      "Episode 973/1000, Reward: 3.8587890624999996\n",
      "Episode 974/1000, Reward: 4.239726562499999\n",
      "Episode 975/1000, Reward: 3.80908203125\n",
      "Episode 976/1000, Reward: 8.323593750000004\n",
      "Episode 977/1000, Reward: 3.854257812500001\n",
      "Episode 978/1000, Reward: 3.8391601562499993\n",
      "Episode 979/1000, Reward: 3.8136328125000007\n",
      "Episode 980/1000, Reward: 3.8675390624999997\n",
      "Episode 981/1000, Reward: 3.840371093750002\n",
      "Episode 982/1000, Reward: 3.828945312500002\n",
      "Episode 983/1000, Reward: 4.551621093750002\n",
      "Episode 984/1000, Reward: 3.8565234375\n",
      "Episode 985/1000, Reward: 3.8558203124999997\n",
      "Episode 986/1000, Reward: 4.448300781250002\n",
      "Episode 987/1000, Reward: 8.328652343749999\n",
      "Episode 988/1000, Reward: 3.839824218749999\n",
      "Episode 989/1000, Reward: 4.1510156249999985\n",
      "Episode 990/1000, Reward: 3.8332617187499993\n",
      "Episode 991/1000, Reward: 3.858359375000002\n",
      "Episode 992/1000, Reward: 3.8076562500000013\n",
      "Episode 993/1000, Reward: 3.791328125000001\n",
      "Episode 994/1000, Reward: 4.069707031250002\n",
      "Episode 995/1000, Reward: 6.036582031250002\n",
      "Episode 996/1000, Reward: 3.8497656250000025\n",
      "Episode 997/1000, Reward: 4.552207031250001\n",
      "Episode 998/1000, Reward: 3.8675390624999997\n",
      "Episode 999/1000, Reward: 3.8729882812500005\n",
      "Episode 1000/1000, Reward: 3.878378906250001\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelul actor-critic cu CNN\n",
   "id": "dd0fa42d27d95940"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.514867600Z",
     "start_time": "2024-12-31T08:12:10.961848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inițializăm Actorul și Criticul\n",
    "actor2 = Actor(state_dim, action_dim)\n",
    "critic2 = Critic(state_dim)\n",
    "\n",
    "# Optimizatori\n",
    "actor_optimizer2 = optim.Adam(actor2.parameters(), lr=1e-04)\n",
    "critic_optimizer2 = optim.Adam(critic2.parameters(), lr=1e-03)\n",
    "\n",
    "# Funcția de pierdere pentru Critic\n",
    "critic_loss_fn2 = nn.MSELoss()"
   ],
   "id": "39dbcd3c5b9ef652",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.514867600Z",
     "start_time": "2025-01-02T22:08:22.010140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actor3 = Actor(state_dim, action_dim)\n",
    "critic3 = Critic(state_dim)\n",
    "actor_optimizer3 = optim.Adam(actor3.parameters(), lr=1e-04)\n",
    "critic_optimizer3 = optim.Adam(critic3.parameters(), lr=1e-03)"
   ],
   "id": "854e7c714680b3d",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.514867600Z",
     "start_time": "2025-01-03T09:08:39.067897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def train_actor_critic(env, actor, critic, actor_optimizer, critic_optimizer,critic_loss_fn):\n",
    "    \"\"\"Antrenează actorul și criticul.\"\"\"\n",
    "    # Hyperparametrii\n",
    "    \n",
    "    gamma = 0.99  # Discount factor\n",
    "    epsilon = 1.0  # Valoarea inițială pentru explorare\n",
    "    epsilon_min = 0.1  # Valoarea minimă pentru explorare\n",
    "    epsilon_decay = 0.995  # Rata de scădere a epsilonului\n",
    "    num_episodes = 10000\n",
    "    \n",
    "    model_actiune_curenta.eval()\n",
    "    actor.train()\n",
    "    critic.train()\n",
    "    \n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]  # Resetăm mediul\n",
    "        state_pixels = env.render()\n",
    "        # print(\"state_pixels\",state_pixels.shape)\n",
    "        processed_frame = preprocess(state_pixels).unsqueeze(0)\n",
    "        # print(\"processed_frame\",processed_frame.shape)\n",
    "        state = model_actiune_curenta(processed_frame)[0]\n",
    "        state = state.detach().numpy()\n",
    "        # print(\"state\",state.shape)\n",
    "        # print(\"state\",state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "    \n",
    "        while not done:\n",
    "            # Explorare epsilon-greedy\n",
    "            if np.random.rand() < epsilon:\n",
    "                # Alegem o acțiune aleatorie pentru explorare\n",
    "                action = env.action_space.sample()\n",
    "                log_prob = None  # Nu avem log_prob pentru acțiuni exploratorii\n",
    "            else:\n",
    "                # Alegem acțiunea pe baza actorului\n",
    "                action, action_probs = select_action(state, actor)\n",
    "                # print(\"action_probs\",action_probs)\n",
    "                log_prob = torch.log(action_probs[0, action])\n",
    "    \n",
    "            # Executăm acțiunea și obținem recompensele și starea următoare\n",
    "            _ , reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            # Compute next state with our model CNN\n",
    "            new_state_pixels = env.render()\n",
    "            procesed_frame = preprocess(new_state_pixels).unsqueeze(0)\n",
    "            next_state = model_actiune_curenta(procesed_frame)[0]\n",
    "            next_state = next_state.detach().numpy()\n",
    "            \n",
    "            # print(\"next_state\",next_state)\n",
    "            # print(\"next_state_target\",next_state_target)\n",
    "            # Calculăm centrul pipe-ului și penalizăm pentru deviere\n",
    "            next_pipe_center = (state[4] + state[5]) / 2\n",
    "            distance_from_center = abs(state[9] - next_pipe_center)\n",
    "            reward -= distance_from_center * 0.01\n",
    "            if reward > 0.5:\n",
    "                reward += 1 # Bonus pentru trecerea printr-un pipe\n",
    "            \n",
    "            # Calculăm TD Target și Advantage\n",
    "            value = critic(torch.FloatTensor(state).unsqueeze(0)).item()\n",
    "            next_value = critic(torch.FloatTensor(next_state).unsqueeze(0)).item() if not done else 0\n",
    "            td_target = reward + gamma * next_value\n",
    "            advantage = td_target - value\n",
    "    \n",
    "            # Actualizăm actorul și criticul\n",
    "            if log_prob is not None:\n",
    "                update_actor(actor, actor_optimizer, log_prob, advantage)\n",
    "            update_critic(critic, critic_optimizer, state, td_target,critic_loss_fn)\n",
    "    \n",
    "            # Trecem la următoarea stare\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "    \n",
    "    \n",
    "        # Reducem epsilon (explorare mai mică pe măsură ce învățăm)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Feedback despre performanță\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "        \n",
    "    env.close()\n"
   ],
   "id": "27c22baf9a938840",
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.539713800Z",
     "start_time": "2025-01-03T09:08:42.419719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "train_actor_critic(env, actor3, critic3, actor_optimizer3, critic_optimizer3,critic_loss_fn2)"
   ],
   "id": "95ac0916ab54769d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10000, Reward: -8.213829040527344\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pas 5: Testare",
   "id": "1aa1187cf759aba1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.540712900Z",
     "start_time": "2025-01-02T22:14:17.379229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\" , use_lidar=False)\n",
    "# print(\"Modelul Actor-Critic cu CNN\",actor2)\n",
    "# print(\"Modelul Actor-Critic fara CNN\",actor)\n",
    "actor2.eval()\n",
    "# critic2.eval()\n",
    "for i in range(10):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, action_probs = select_action_without_explore(state, actor3)\n",
    "        # print(\"action_probs\",action_probs , \"action\",action)\n",
    "        state, reward, done, _,_= env.step(action)\n",
    "        env.render()  # Vizualizăm episodul\n",
    "env.close()\n"
   ],
   "id": "1bc16d7064331276",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[231], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;66;03m# print(\"action_probs\",action_probs , \"action\",action)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m         state, reward, done, _,_\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m---> 13\u001B[0m         \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Vizualizăm episodul\u001B[39;00m\n\u001B[0;32m     14\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m     )\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:332\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RenderFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[RenderFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:303\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_render_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv)\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flappy_bird_gymnasium\\envs\\flappy_bird_env.py:407\u001B[0m, in \u001B[0;36mFlappyBirdEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_display()\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_display()\n\u001B[1;32m--> 407\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fps_clock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# evaluare",
   "id": "b0d8e1a219ce1a2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.540712900Z",
     "start_time": "2025-01-03T09:07:56.451335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(env, actor, num_eval_episodes=100):\n",
    "    scores = []\n",
    "    survival_times = []\n",
    "    pipes_passed = []\n",
    "\n",
    "    for _ in range(num_eval_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        frames = 0\n",
    "        pipes = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = select_action_without_explore(state, actor)  # Politica deterministă (fără explorare)\n",
    "            next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "            # Adunăm scorurile și statistici\n",
    "            episode_reward += reward\n",
    "            frames += 1\n",
    "            if reward > 0.5:  # Ex: Recompensă mare pentru trecerea printr-un pipe\n",
    "                pipes += 1\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "        survival_times.append(frames)\n",
    "        pipes_passed.append(pipes)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_survival = np.mean(survival_times)\n",
    "    avg_pipes = np.mean(pipes_passed)\n",
    "\n",
    "    print(f\"Evaluare pe {num_eval_episodes} episoade:\")\n",
    "    print(f\"Scor mediu: {avg_score}\")\n",
    "    print(f\"Timp mediu de supraviețuire: {avg_survival} cadre\")\n",
    "    print(f\"Pipe-uri trecute în medie: {avg_pipes}\")\n",
    "\n",
    "    return scores, survival_times, pipes_passed\n",
    "\n",
    "\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\" , use_lidar=False)\n",
    "# După antrenare:\n",
    "scores, survival_times, pipes_passed = evaluate_model(env, actor3)\n",
    "\n",
    "# Vizualizare\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Recompensa medie\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(scores)\n",
    "plt.title(\"Scor pe Episod\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Scor\")\n",
    "\n",
    "# Supraviețuirea medie\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(survival_times)\n",
    "plt.title(\"Timp de Supraviețuire pe Episod\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Cadre\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "6e2876e9748c3cee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluare pe 100 episoade:\n",
      "Scor mediu: 3.2999999999999985\n",
      "Timp mediu de supraviețuire: 50.0 cadre\n",
      "Pipe-uri trecute în medie: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.541713100Z",
     "start_time": "2025-01-02T22:01:50.301746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def save_training_progress(actor, critic, actor_optimizer, critic_optimizer, replay_buffer,model_actiune_curenta, filepath):\n",
    "    \"\"\"\n",
    "    Save the training progress including models, optimizers, and replay buffer.\n",
    "\n",
    "    Parameters:\n",
    "        actor (nn.Module): The actor model.\n",
    "        critic (nn.Module): The critic model.\n",
    "        model_actiune_curenta (nn.Module): The current action model.\n",
    "        replay_buffer (deque): The replay buffer.\n",
    "        actor_optimizer (torch.optim.Optimizer): The optimizer for the actor model.\n",
    "        critic_optimizer (torch.optim.Optimizer): The optimizer for the critic model.\n",
    "        filepath (str): The file path to save the progress.\n",
    "    \"\"\"\"\"\n",
    "    # Create a dictionary to hold all the state dictionaries\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': actor.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "        'model_actiune_curenta_state_dict': model_actiune_curenta.state_dict(),\n",
    "        'replay_buffer': list(replay_buffer),  # Convert deque to list for saving\n",
    "        'critic_optimizer_state_dict': critic_optimizer.state_dict()\n",
    "    }\n",
    "    # Compress and save the checkpoint\n",
    "    with gzip.open(filepath, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    \n",
    "    "
   ],
   "id": "b7ef173c51f73a1c",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.541713100Z",
     "start_time": "2025-01-02T22:03:13.870057Z"
    }
   },
   "cell_type": "code",
   "source": "save_training_progress(actor3, critic3, actor_optimizer3, critic_optimizer3,replay_buffer,model_actiune_curenta, 'training_progress_compress.pth')",
   "id": "1467313edd2b51c0",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.541713100Z",
     "start_time": "2025-01-02T22:05:17.757961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def load_training_progress(filepath, actor, critic, actor_optimizer,critic_optimizer,replay_buffer,model_actiune_curenta):\n",
    "    \"\"\"\n",
    "    Load the training progress including models, optimizers, and replay buffer.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path to load the progress from.\n",
    "        actor (nn.Module): The actor model.\n",
    "        critic (nn.Module): The critic model.\n",
    "        actor_optimizer (torch.optim.Optimizer): The optimizer for the actor model.\n",
    "        critic_optimizer (torch.optim.Optimizer): The optimizer for the critic model.\n",
    "        replay_buffer (deque): The replay buffer.\n",
    "        model_actiune_curenta (nn.Module): The current CNN model.\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    # Load and decompress the checkpoint\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "\n",
    "    # Load the state dictionaries\n",
    "    actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "    replay_buffer.extend(checkpoint['replay_buffer'])\n",
    "    model_actiune_curenta.load_state_dict(checkpoint['model_actiune_curenta_state_dict'])\n",
    "    actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "    critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "\n",
    "    "
   ],
   "id": "934fae4e5978434d",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.541713100Z",
     "start_time": "2025-01-02T22:09:16.945573Z"
    }
   },
   "cell_type": "code",
   "source": "load_training_progress('training_progress_compress.pth', actor3, critic3, actor_optimizer3, critic_optimizer3,replay_buffer, model_actiune_curenta)",
   "id": "248593556ffcef81",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.542862900Z",
     "start_time": "2025-01-02T22:06:08.385597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "def get_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return size\n",
    "\n",
    "file1 = './training_progress_compress.pth'\n",
    "\n",
    "\n",
    "size1 = get_file_size(file1)\n",
    "\n",
    "print(f\"Size of {file1}: {size1 / (1024 * 1024)} MB\")\n"
   ],
   "id": "7f6113ab51638515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of ./training_progress_compress.pth: 38.83856773376465 MB\n"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.542862900Z",
     "start_time": "2025-01-02T22:07:24.224862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds the size of objects.\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Mark as seen\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "size_in_bytes = get_size(replay_buffer)\n",
    "size_2 = get_size(model_actiune_curenta)\n",
    "size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "size_in_mb2 = size_2 / (1024 * 1024)\n",
    "print(f\"Size of replay_buffer: {size_in_mb} MB\")\n",
    "print(f\"Size of model_actiune_curenta: {size_in_mb2} MB\")"
   ],
   "id": "b32fdcb06ea5e99a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of replay_buffer: 27.773784637451172 MB\n",
      "Size of model_actiune_curenta: 0.019158363342285156 MB\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:53:52.542862900Z",
     "start_time": "2025-01-02T22:07:19.823560Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(replay_buffer))",
   "id": "cb5bdfedbb783a71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "execution_count": 219
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
