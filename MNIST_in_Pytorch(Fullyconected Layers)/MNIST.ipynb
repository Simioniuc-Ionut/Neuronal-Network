{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:38:54.346731Z",
     "start_time": "2024-12-01T19:38:54.341144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modulefinder import Module\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn # To inherit our neural network\n",
    "from torch.utils.data import Dataset,DataLoader # For management of the dataset (batches)\n",
    "from tqdm import tqdm # For nice progress bar!\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self,root,train,download):\n",
    "        if(train):\n",
    "            dt_transforms = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,)),\n",
    "            # transforms.Lambda(lambda x : x.view(-1,784))\n",
    "            ])\n",
    "            self.dataset = MNIST(root=root, train=train, download=download, transform=dt_transforms)\n",
    "        else:\n",
    "            dt_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,)),\n",
    "            # transforms.Lambda(lambda x : x.view(-1,784))\n",
    "            ])\n",
    "            self.dataset = MNIST(root=root, train=train, download=download, transform=dt_transforms)\n",
    "    def loadeData(self,batch_size=64):\n",
    "        return DataLoader(self.dataset,batch_size=batch_size,shuffle=True)\n",
    "    def loadTestData(self,batch_size=64):\n",
    "        return DataLoader(self.dataset,batch_size=batch_size,shuffle=False)\n",
    "        # data_lines = int(input_data_size / batch_size)\n",
    "        # \n",
    "        # self.proc_data = torch.empty((data_lines,batch_size,784),dtype=torch.float32 )\n",
    "        # self.proc_label = torch.empty((data_lines, batch_size), dtype=torch.int8)\n",
    "        # \n",
    "        # batch_idx = 0\n",
    "        # \n",
    "        # for x, y in self.processed_dataset:\n",
    "        #     \n",
    "        #     self.proc_data[batch_idx] = x.squeeze()\n",
    "        #     self.proc_label[batch_idx] = y\n",
    "        #     batch_idx += 1\n",
    "        # return self.proc_data,self.proc_label\n"
   ],
   "id": "3210a240db1e1db3",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:38:55.882009Z",
     "start_time": "2024-12-01T19:38:55.777334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load dataset\n",
    "train_dataset = MNISTDataset('data/train',True,False)\n",
    "test_dataset = MNISTDataset('data/test',False,False)\n",
    "# proc_train_data,proc_train_label = train_dataset.loadeData(60000)\n",
    "# proc_test_data,proc_test_label = test_dataset.loadeData(10000)"
   ],
   "id": "57ed005371ae2b21",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Inspect a sample from the dataset\n"
   ],
   "id": "e0be6ad3f7436a8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Inspect a sample from the dataset\n",
    "# sample_image = proc_train_data[0][0]\n",
    "# sample_label = proc_train_label[0][0]\n",
    "# print(\"Sample image shape:\", sample_image.shape)\n",
    "# print(\"Sample label:\", sample_label)\n",
    "# sample_image\n",
    "\n",
    "# # Crearea unui tensor 2D de exemplu\n",
    "# tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6],[4, 5, 6]])\n",
    "\n",
    "# # Aplatizarea tensorului\n",
    "# flattened_tensor = torch.flatten(tensor_2d)\n",
    "# \n",
    "# print(\"Tensor original:\\n\", tensor_2d)\n",
    "# print(\"Tensor aplatizat:\\n\", flattened_tensor)"
   ],
   "id": "e9d29b3be5cedb4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Model\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size,hidden1,hidden2,num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1,hidden2)\n",
    "        self.out = nn.Linear(hidden2,num_classes)\n",
    "        self.activation_fct = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(p=0.05)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def forward_validation(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def evalTest(self):\n",
    "        self.loader=test_dataset.loadeData()\n",
    "    def evalTraining(self): \n",
    "        self.loader=train_dataset.loadeData()\n",
    "        \n",
    "    def train(self,criterion,optimizer,num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx,(data,targets), in enumerate(tqdm(self.loader)):\n",
    "                optimizer.zero_grad()\n",
    "                # print(\"Data shape:\",data.shape)\n",
    "                # Get to correct shape\n",
    "                data = data.reshape(data.shape[0], -1)\n",
    "                # print(data.shape,\"\\n\")\n",
    "                \n",
    "                # Forward\n",
    "                # scores = self.forward_train(data)\n",
    "                scores = self(data) # same as self.forward(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                \n",
    "                #adam step\n",
    "                optimizer.step()\n",
    "            \n",
    "            # print(\"loss: \",loss)    \n",
    "    \n",
    "\n",
    "      "
   ],
   "id": "151ba8ee481d9393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:40:51.440035Z",
     "start_time": "2024-12-01T19:40:51.434194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model\n",
    "\n",
    "class NN_2(nn.Module):\n",
    "    def __init__(self,input_size,hidden1,hidden2,num_classes):\n",
    "        super(NN_2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1,hidden2)\n",
    "        self.out = nn.Linear(hidden2,num_classes)\n",
    "        self.activation_fct = nn.ReLU()\n",
    "        # self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        # self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def forward_validation(self,x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def evalTest(self):\n",
    "        self.loader=test_dataset.loadTestData()\n",
    "    def evalTraining(self): \n",
    "        self.loader=train_dataset.loadeData()\n",
    "        \n",
    "    def train(self,criterion,optimizer,num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx,(data,targets), in enumerate(tqdm(self.loader)):\n",
    "                optimizer.zero_grad()\n",
    "                # print(\"Data shape:\",data.shape)\n",
    "                # Get to correct shape\n",
    "                data = data.reshape(data.shape[0], -1)\n",
    "                # print(data.shape,\"\\n\")\n",
    "                \n",
    "                # Forward\n",
    "                # scores = self.forward_train(data)\n",
    "                scores = self(data) # same as self.forward(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                \n",
    "                #adam step\n",
    "                optimizer.step()\n",
    "       \n",
    "    \n",
    "\n",
    "      "
   ],
   "id": "921150e071b9f0a3",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:39:38.166665Z",
     "start_time": "2024-12-01T19:39:38.162058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_accuracy(model):\n",
    " \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in model.loader:\n",
    "            # Forward pass\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            scores = model.forward_validation(x)\n",
    "            \n",
    "            _, predictions = scores.max(dim=1)\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "\n"
   ],
   "id": "2e4847ce8ee5ce36",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:39:42.493651Z",
     "start_time": "2024-12-01T19:39:42.489566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def write_predictions_to_file(model, filename='predictions.csv'):\n",
    "    model.evalTest()\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"ID,Target\\n\")\n",
    "        with torch.no_grad():\n",
    "            for idx, (data, _) in enumerate(model.loader):\n",
    "                data = data.view(data.size(0), -1) # transform (64,28,28) to (64,784)\n",
    "                outputs = model.forward_validation(data)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                for i, prediction in enumerate(predictions):\n",
    "                    f.write(f\"{idx * model.loader.batch_size + i},{prediction.item()}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `test_loader` is your DataLoader for the test dataset\n",
    "# write_predictions_to_file(model, test_loader)"
   ],
   "id": "825620364600c61d",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "1138fb9ca86e4f4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    " \n",
    "# Initialize Module\n",
    "model = NN(input_size = input_size ,hidden1=128,hidden2=64, num_classes= num_classes,)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# SGD with momentum\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n"
   ],
   "id": "3ef9e1e0d08918e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# view what view is doing\n",
    "for x,y in model.loader:\n",
    "    x = x.squeeze()\n",
    "    x = x.view(x.size(0),-1)\n",
    "    print(x.size() , y.size())\n",
    "    print(x.shape, y.shape)\n",
    "    break\n"
   ],
   "id": "c3649e1c5c6b23ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.evalTraining()\n",
    "print(\"Accuracy on training\")\n",
    "model.train(criterion,optimizer,num_epochs)\n",
    "check_accuracy(model)"
   ],
   "id": "f8744067a168c9f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Accuracy on validation\")\n",
    "model.evalTest()\n",
    "check_accuracy(model)"
   ],
   "id": "ef80c496f71870dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model2 = NN_2(input_size = input_size ,hidden1=128,hidden2=64, num_classes= num_classes,)\n",
    "model2.fc1.load_state_dict(model.fc1.state_dict())\n",
    "model2.fc2.load_state_dict(model.fc2.state_dict())\n",
    "model2.out.load_state_dict(model.out.state_dict())\n",
    "\n"
   ],
   "id": "9a76ff10af941a24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:44:46.877338Z",
     "start_time": "2024-12-01T19:44:46.871175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with batch normalization\n",
    "model3 = NN_2(input_size = input_size ,hidden1=128,hidden2=64, num_classes= num_classes,)\n",
    "model3.fc1.load_state_dict(model2.fc1.state_dict())\n",
    "model3.fc2.load_state_dict(model2.fc2.state_dict())\n",
    "model3.out.load_state_dict(model2.out.state_dict())"
   ],
   "id": "39a62991dd19c891",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:43:38.616923Z",
     "start_time": "2024-12-01T22:43:38.613939Z"
    }
   },
   "cell_type": "code",
   "source": "model3.dropout = nn.Dropout(p=0.05)",
   "id": "3d56e65c7aa74abd",
   "outputs": [],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:44:40.462529Z",
     "start_time": "2024-12-01T19:44:40.431173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the augmented dataset\n",
    "augmented_train_dataset = MNISTDataset('data/train', True, False)\n",
    "model3.loader = augmented_train_dataset.loadeData()"
   ],
   "id": "9f24375e202f5cb6",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:43:40.436083Z",
     "start_time": "2024-12-01T22:43:40.432246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reinitialize the optimizer for mode\n",
    "# l2\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n"
   ],
   "id": "41c895853ce8697f",
   "outputs": [],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:52:48.949386Z",
     "start_time": "2024-12-01T22:49:20.303896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model3.evalTraining()\n",
    "print(\"Accuracy on training\")\n",
    "model3.train(criterion,optimizer3,10)\n",
    "# check_accuracy(model3)\n"
   ],
   "id": "6556389c73e762c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 49.18it/s]\n",
      "100%|██████████| 938/938 [00:20<00:00, 45.56it/s]\n",
      "100%|██████████| 938/938 [00:19<00:00, 47.22it/s]\n",
      "100%|██████████| 938/938 [00:20<00:00, 46.02it/s]\n",
      "100%|██████████| 938/938 [00:20<00:00, 45.46it/s]\n",
      "100%|██████████| 938/938 [00:20<00:00, 46.47it/s]\n",
      "100%|██████████| 938/938 [00:20<00:00, 46.19it/s]\n",
      "100%|██████████| 938/938 [00:21<00:00, 43.29it/s]\n",
      "100%|██████████| 938/938 [00:23<00:00, 40.32it/s]\n",
      "100%|██████████| 938/938 [00:22<00:00, 41.41it/s]\n"
     ]
    }
   ],
   "execution_count": 335
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:54:06.069307Z",
     "start_time": "2024-12-01T22:54:03.948999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Accuracy on validation\")\n",
    "model3.evalTest()\n",
    "(check_accuracy(model3))"
   ],
   "id": "7803a84f69955768",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation\n",
      "Got 9852 / 10000 with accuracy 98.52\n"
     ]
    }
   ],
   "execution_count": 336
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reinitialize the optimizer for mode\n",
    "# l2\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n"
   ],
   "id": "e649e0a4bde5d9cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model2.evalTraining()\n",
    "# print(\"Accuracy on training\")\n",
    "# model2.train(criterion,optimizer2,num_epochs)\n",
    "# check_accuracy(model2)\n"
   ],
   "id": "4e8f3f34a1c6e4c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T19:40:18.900453Z",
     "start_time": "2024-12-01T19:40:17.384267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Accuracy on validation\")\n",
    "model2.evalTest()\n",
    "check_accuracy(model2)"
   ],
   "id": "387007514498b2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation\n",
      "Got 9803 / 10000 with accuracy 98.03\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:25:35.494939Z",
     "start_time": "2024-12-01T22:25:33.686969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make predictions\n",
    "write_predictions_to_file(model3, 'predictions9873.csv')\n"
   ],
   "id": "99d26916f0645e60",
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:29:44.259338Z",
     "start_time": "2024-12-01T22:29:44.240935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Salvare ponderi\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "print(\"Model weights saved!\")"
   ],
   "id": "1e51bbc4f49a9a61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved!\n"
     ]
    }
   ],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Weights\n",
    "model = NN(input_size=input_size, hidden1=128, hidden2=64, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))"
   ],
   "id": "c6a64496113844ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the entire model\n",
    "torch.save(model, 'complete_model.pth')\n",
    "print(\"Complete model saved!\")"
   ],
   "id": "cf6ff28380dd4fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:47:10.135007Z",
     "start_time": "2024-12-01T22:47:10.119096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the entire model\n",
    "model = torch.load('model_weights.pth')"
   ],
   "id": "b3a19158586d8446",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1768\\4252404703.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('model_weights.pth')\n"
     ]
    }
   ],
   "execution_count": 333
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Save the model, optimizer, and training state\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename='data/save/checkpoint.pth.tar'):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")"
   ],
   "id": "3f683dc0657f6a54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(model2.fc1, 'data/save/fc1.pth')\n",
    "torch.save(model2.fc2, 'data/save/fc2.pth')\n",
    "torch.save(model2.out, 'data/save/out.pth')\n",
    "torch.save(model2, 'data/save/model.pth')\n"
   ],
   "id": "67d2e8c8627ee96d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.load('data/save/fc1.pth',model.fc1)\n",
    "torch.load('data/save/fc2.pth',model.fc2)\n",
    "torch.load('data/save/out.pth',model.out)"
   ],
   "id": "507182eac199b8a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(model.fc1.weight.shape)\n",
    "print(model.fc2.weight.shape)\n",
    "print(model.out.weight.shape)\n",
    "print(model.out.bias)"
   ],
   "id": "5ae6caf0d5212c2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
