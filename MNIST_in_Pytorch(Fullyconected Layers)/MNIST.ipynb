{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modulefinder import Module\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn # To inherit our neural network\n",
    "from torch.utils.data import Dataset,DataLoader # For management of the dataset (batches)\n",
    "from tqdm import tqdm # For nice progress bar!\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self,root,train,download):\n",
    "        if(train):\n",
    "            dt_transforms = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,)),\n",
    "            # transforms.Lambda(lambda x : x.view(-1,784))\n",
    "            ])\n",
    "            self.dataset = MNIST(root=root, train=train, download=download, transform=dt_transforms)\n",
    "        else:\n",
    "            dt_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,)),\n",
    "            # transforms.Lambda(lambda x : x.view(-1,784))\n",
    "            ])\n",
    "            self.dataset = MNIST(root=root, train=train, download=download, transform=dt_transforms)\n",
    "    def loadeData(self,batch_size=64):\n",
    "        return DataLoader(self.dataset,batch_size=batch_size,shuffle=True)\n",
    "    def loadTestData(self,batch_size=64):\n",
    "        return DataLoader(self.dataset,batch_size=batch_size,shuffle=False)\n",
    "        # data_lines = int(input_data_size / batch_size)\n",
    "        # \n",
    "        # self.proc_data = torch.empty((data_lines,batch_size,784),dtype=torch.float32 )\n",
    "        # self.proc_label = torch.empty((data_lines, batch_size), dtype=torch.int8)\n",
    "        # \n",
    "        # batch_idx = 0\n",
    "        # \n",
    "        # for x, y in self.processed_dataset:\n",
    "        #     \n",
    "        #     self.proc_data[batch_idx] = x.squeeze()\n",
    "        #     self.proc_label[batch_idx] = y\n",
    "        #     batch_idx += 1\n",
    "        # return self.proc_data,self.proc_label\n"
   ],
   "id": "3210a240db1e1db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load dataset\n",
    "train_dataset = MNISTDataset('data/train',True,False)\n",
    "test_dataset = MNISTDataset('data/test',False,False)\n",
    "# proc_train_data,proc_train_label = train_dataset.loadeData(60000)\n",
    "# proc_test_data,proc_test_label = test_dataset.loadeData(10000)"
   ],
   "id": "57ed005371ae2b21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Inspect a sample from the dataset\n"
   ],
   "id": "e0be6ad3f7436a8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Inspect a sample from the dataset\n",
    "# sample_image = proc_train_data[0][0]\n",
    "# sample_label = proc_train_label[0][0]\n",
    "# print(\"Sample image shape:\", sample_image.shape)\n",
    "# print(\"Sample label:\", sample_label)\n",
    "# sample_image\n",
    "\n",
    "# # Crearea unui tensor 2D de exemplu\n",
    "# tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6],[4, 5, 6]])\n",
    "\n",
    "# # Aplatizarea tensorului\n",
    "# flattened_tensor = torch.flatten(tensor_2d)\n",
    "# \n",
    "# print(\"Tensor original:\\n\", tensor_2d)\n",
    "# print(\"Tensor aplatizat:\\n\", flattened_tensor)"
   ],
   "id": "e9d29b3be5cedb4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Model\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size,hidden1,hidden2,num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1,hidden2)\n",
    "        self.out = nn.Linear(hidden2,num_classes)\n",
    "        self.activation_fct = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(p=0.05)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def forward_validation(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def evalTest(self):\n",
    "        self.loader=test_dataset.loadeData()\n",
    "    def evalTraining(self): \n",
    "        self.loader=train_dataset.loadeData()\n",
    "        \n",
    "    def train(self,criterion,optimizer,num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx,(data,targets), in enumerate(tqdm(self.loader)):\n",
    "                optimizer.zero_grad()\n",
    "                # print(\"Data shape:\",data.shape)\n",
    "                # Get to correct shape\n",
    "                data = data.reshape(data.shape[0], -1)\n",
    "                # print(data.shape,\"\\n\")\n",
    "                \n",
    "                # Forward\n",
    "                # scores = self.forward_train(data)\n",
    "                scores = self(data) # same as self.forward(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                \n",
    "                #adam step\n",
    "                optimizer.step()\n",
    "            \n",
    "            # print(\"loss: \",loss)    \n",
    "    \n",
    "\n",
    "      "
   ],
   "id": "151ba8ee481d9393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Model\n",
    "\n",
    "class NN_2(nn.Module):\n",
    "    def __init__(self,input_size,hidden1,hidden2,num_classes):\n",
    "        super(NN_2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1,hidden2)\n",
    "        self.out = nn.Linear(hidden2,num_classes)\n",
    "        self.activation_fct = nn.ReLU()\n",
    "        # self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        # self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def forward_validation(self,x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.activation_fct(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def evalTest(self):\n",
    "        self.loader=test_dataset.loadTestData()\n",
    "    def evalTraining(self): \n",
    "        self.loader=train_dataset.loadeData()\n",
    "        \n",
    "    def train(self,criterion,optimizer,num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx,(data,targets), in enumerate(tqdm(self.loader)):\n",
    "                optimizer.zero_grad()\n",
    "                # print(\"Data shape:\",data.shape)\n",
    "                # Get to correct shape\n",
    "                data = data.reshape(data.shape[0], -1)\n",
    "                # print(data.shape,\"\\n\")\n",
    "                \n",
    "                # Forward\n",
    "                # scores = self.forward_train(data)\n",
    "                scores = self(data) # same as self.forward(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                \n",
    "                #adam step\n",
    "                optimizer.step()\n",
    "       \n",
    "    \n",
    "\n",
    "      "
   ],
   "id": "921150e071b9f0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_accuracy(model):\n",
    " \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in model.loader:\n",
    "            # Forward pass\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            scores = model.forward_validation(x)\n",
    "            \n",
    "            _, predictions = scores.max(dim=1)\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "\n"
   ],
   "id": "2e4847ce8ee5ce36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def write_predictions_to_file(model, filename='predictions.csv'):\n",
    "    model.evalTest()\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"ID,Target\\n\")\n",
    "        with torch.no_grad():\n",
    "            for idx, (data, _) in enumerate(model.loader):\n",
    "                data = data.view(data.size(0), -1) # transform (64,28,28) to (64,784)\n",
    "                outputs = model.forward_validation(data)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                for i, prediction in enumerate(predictions):\n",
    "                    f.write(f\"{idx * model.loader.batch_size + i},{prediction.item()}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `test_loader` is your DataLoader for the test dataset\n",
    "# write_predictions_to_file(model, test_loader)"
   ],
   "id": "825620364600c61d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "1138fb9ca86e4f4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    " \n",
    "# Initialize Module\n",
    "model = NN(input_size = input_size ,hidden1=128,hidden2=64, num_classes= num_classes,)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# SGD with momentum\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n"
   ],
   "id": "3ef9e1e0d08918e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# view what view is doing\n",
    "for x,y in model.loader:\n",
    "    x = x.squeeze()\n",
    "    x = x.view(x.size(0),-1)\n",
    "    print(x.size() , y.size())\n",
    "    print(x.shape, y.shape)\n",
    "    break\n"
   ],
   "id": "c3649e1c5c6b23ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.evalTraining()\n",
    "print(\"Accuracy on training\")\n",
    "model.train(criterion,optimizer,num_epochs)\n",
    "check_accuracy(model)"
   ],
   "id": "f8744067a168c9f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Accuracy on validation\")\n",
    "model.evalTest()\n",
    "check_accuracy(model)"
   ],
   "id": "ef80c496f71870dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model2 = NN_2(input_size = input_size ,hidden1=128,hidden2=64, num_classes= num_classes,)\n",
    "model2.fc1.load_state_dict(model.fc1.state_dict())\n",
    "model2.fc2.load_state_dict(model.fc2.state_dict())\n",
    "model2.out.load_state_dict(model.out.state_dict())\n",
    "\n"
   ],
   "id": "9a76ff10af941a24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with batch normalization\n",
    "model3 = NN_2(input_size = input_size ,hidden1=128,hidden2=64, num_classes= num_classes,)\n",
    "model3.fc1.load_state_dict(model2.fc1.state_dict())\n",
    "model3.fc2.load_state_dict(model2.fc2.state_dict())\n",
    "model3.out.load_state_dict(model2.out.state_dict())"
   ],
   "id": "39a62991dd19c891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model3.dropout = nn.Dropout(p=0.1)",
   "id": "3d56e65c7aa74abd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reload the augmented dataset\n",
    "augmented_train_dataset = MNISTDataset('data/train', True, False)\n",
    "model3.loader = augmented_train_dataset.loadeData()"
   ],
   "id": "9f24375e202f5cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reinitialize the optimizer for mode\n",
    "# l2\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n"
   ],
   "id": "41c895853ce8697f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model3.evalTraining()\n",
    "print(\"Accuracy on training\")\n",
    "model3.train(criterion,optimizer3,10)\n",
    "# check_accuracy(model3)\n"
   ],
   "id": "6556389c73e762c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Accuracy on validation\")\n",
    "model3.evalTest()\n",
    "(check_accuracy(model3))"
   ],
   "id": "7803a84f69955768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reinitialize the optimizer for mode\n",
    "# l2\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n"
   ],
   "id": "e649e0a4bde5d9cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model2.evalTraining()\n",
    "# print(\"Accuracy on training\")\n",
    "# model2.train(criterion,optimizer2,num_epochs)\n",
    "# check_accuracy(model2)\n"
   ],
   "id": "4e8f3f34a1c6e4c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Accuracy on validation\")\n",
    "model2.evalTest()\n",
    "check_accuracy(model2)"
   ],
   "id": "387007514498b2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make predictions\n",
    "write_predictions_to_file(model3, 'predictions9873.csv')\n"
   ],
   "id": "99d26916f0645e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Salvare ponderi\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "print(\"Model weights saved!\")"
   ],
   "id": "1e51bbc4f49a9a61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Weights\n",
    "model = NN(input_size=input_size, hidden1=128, hidden2=64, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))"
   ],
   "id": "c6a64496113844ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the entire model\n",
    "torch.save(model, 'complete_model.pth')\n",
    "print(\"Complete model saved!\")"
   ],
   "id": "cf6ff28380dd4fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the entire model\n",
    "model = torch.load('model_weights.pth')"
   ],
   "id": "b3a19158586d8446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Save the model, optimizer, and training state\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename='data/save/checkpoint.pth.tar'):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")"
   ],
   "id": "3f683dc0657f6a54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(model2.fc1, 'data/save/fc1.pth')\n",
    "torch.save(model2.fc2, 'data/save/fc2.pth')\n",
    "torch.save(model2.out, 'data/save/out.pth')\n",
    "torch.save(model2, 'data/save/model.pth')\n"
   ],
   "id": "67d2e8c8627ee96d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.load('data/save/fc1.pth',model.fc1)\n",
    "torch.load('data/save/fc2.pth',model.fc2)\n",
    "torch.load('data/save/out.pth',model.out)"
   ],
   "id": "507182eac199b8a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(model.fc1.weight.shape)\n",
    "print(model.fc2.weight.shape)\n",
    "print(model.out.weight.shape)\n",
    "print(model.out.bias)"
   ],
   "id": "5ae6caf0d5212c2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
