{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform= lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    mnist_data= []\n",
    "    mnist_labels= []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "def process_data(mnist_data, mnist_labels):\n",
    "    mnist_data = np.array(mnist_data).reshape(-1, 784)\n",
    "    mnist_labels = np.array(mnist_labels).reshape(-1, 1)\n",
    "\n",
    "    processed_mnist_labels = np.zeros((len(mnist_labels), 10))\n",
    "    for idx,mnist_label in enumerate(mnist_labels):\n",
    "        # One hot encoding\n",
    "        value = mnist_label[0]\n",
    "        processed_mnist_labels[idx, value] = 1\n",
    "    \n",
    "    # normalization\n",
    "    normalize_mnist_data = mnist_data / 255\n",
    "    return normalize_mnist_data, processed_mnist_labels"
   ],
   "id": "9ba842d0e98c8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "u = 0 , sigma = radical(2/fain_in + fan_out)",
   "id": "112a0d6d12f4eb27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:50:25.697845Z",
     "start_time": "2024-11-09T17:50:25.685741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split in batches\n",
    "# 2. Split data into batches\n",
    "def split_in_batches(data, labels, batch_size=100):\n",
    "    # Shuffle the data and labels in unison\n",
    "    permutation = np.random.permutation(len(data))\n",
    "    shuffled_data = data[permutation]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    \n",
    "    # Split into batches\n",
    "    batched_data = []\n",
    "    batched_labels = []\n",
    "    for i in range(0, len(shuffled_data), batch_size):\n",
    "        batched_data.append(shuffled_data[i:i+batch_size])\n",
    "        batched_labels.append(shuffled_labels[i:i+batch_size])\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    batched_data = np.array(batched_data)\n",
    "    batched_labels = np.array(batched_labels)\n",
    "    \n",
    "    return batched_data, batched_labels"
   ],
   "id": "a08501f1d6c79e89",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# weight and biass initialization with Xavier\n",
    "\n",
    "\n",
    "def xavier_init():\n",
    "    fan_out = 10\n",
    "    fan_in = 784\n",
    "    fan_hidden = 100\n",
    "    # we use normal distribution with function np.random.randn\n",
    "    w1 = np.random.randn(fan_in, fan_hidden) * np.sqrt(2 / (fan_in + fan_hidden))\n",
    "    b1 = np.random.randn(1, fan_hidden) * np.sqrt(2 / (1 + fan_hidden))\n",
    "    w2 = np.random.randn(fan_hidden, fan_out) * np.sqrt(2 / (fan_hidden + fan_out))\n",
    "    b2 = np.random.randn(1, fan_out) * np.sqrt(2 / (1 + fan_out))\n",
    "    return w1, b1, w2, b2\n"
   ],
   "id": "8a16fbe93e2204b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:07:09.170558Z",
     "start_time": "2024-11-09T18:07:09.165018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dropout_function(layer_output, dropout_rate):\n",
    "    # Generate a mask that will zero out neurons with probability dropout_rate\n",
    "    mask = (np.random.rand(*layer_output.shape) < (1- dropout_rate)).astype(float)\n",
    "    # Apply the mask to the layer output and scale neurons that remain active \n",
    "    return layer_output * mask / (1 - dropout_rate)"
   ],
   "id": "a4d0916da6bbdb30",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T20:58:50.655474Z",
     "start_time": "2024-11-09T20:58:50.647260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2 activation function \n",
    "#  tanh for hidden layer\n",
    "def activation_tanh(z):\n",
    "    return  np.tanh(z) #(np.exp(z) - np.exp(z))/(np.exp(z) + np.exp(-z)) # or : np.tanh(z)\n",
    "def activation_softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True) # normalization \n",
    "\n",
    "# derivative of activation function\n",
    "def derivative_tanh(z):\n",
    "    return 1 - np.power(activation_tanh(z), 2) # derivata lui tanh este (1 - a1^2) unde a1 este rezultatul functiei de activare\n",
    "\n",
    "# z is the output of the activation function\n",
    "def derivative_softmax(a2_softmax, y_target):\n",
    "    \"\"\"Derivative of the softmax function with respect to the cross-entropy loss\"\"\"\n",
    "    return a2_softmax - y_target\n",
    "\n",
    "def forward_propagation(x_img,w1,b1,w2,b2):\n",
    "    z1 = x_img @ w1 + b1\n",
    "    a1 = activation_tanh(z1)\n",
    "    z2 = a1 @ w2 +  b2\n",
    "    a2 = activation_softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# optimization technique\n",
    "def forward_propagation_with_dropout(x_img,w1,b1,w2,b2):\n",
    "    z1 = x_img @ w1 + b1\n",
    "    a1 = activation_tanh(z1)\n",
    "    # here we apply dropout\n",
    "    a1_dropped = dropout_function(a1, 0.10)\n",
    "    z2 = a1_dropped @ w2 + b2\n",
    "    a2 = activation_softmax(z2)\n",
    "    return z1, a1_dropped, z2, a2\n",
    "\n",
    "# def regulariation_L1_L2(w1,w2,b1,b2):\n",
    "#     # L1 regularization\n",
    "#     w1 = w1 - 0.01*()\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6]]) \n",
    "print(np.exp(A)) # compute e^A"
   ],
   "id": "7b2d4f148f43ed93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.71828183   7.3890561   20.08553692]\n",
      " [ 54.59815003 148.4131591  403.42879349]]\n"
     ]
    }
   ],
   "execution_count": 517
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:49:17.155748Z",
     "start_time": "2024-11-09T17:49:17.148986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loss function\n",
    "def cross_entropy_loss(y_target,y_prediction_after_softmax):\n",
    "    length = y_target.shape[0]\n",
    "    return -np.sum(y_target * np.log(y_prediction_after_softmax)) / length\n",
    "\n",
    "\n"
   ],
   "id": "198c7f747ec0c6f7",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:46:33.329392Z",
     "start_time": "2024-11-11T11:46:33.323585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(process_train_X, process_train_Y, w1,b1,w2,b2):\n",
    "    batched_train_data, batched_train_labels = split_in_batches(process_train_X, process_train_Y)\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    lambda_L1 = 0.0001  # Coeficient de regularizare pentru L1\n",
    "    lambda_L2 = 0.0001  # Coeficient de regularizare pentru L2\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "\n",
    "    for batch_data, batch_labels in zip(batched_train_data, batched_train_labels):\n",
    "        size_batch = batch_data.shape[1] # Dimension of the batch is [100,784]\n",
    "        z1, a1_tanh, z2, a2_softmax = forward_propagation_with_dropout(batch_data,w1,b1,w2,b2)\n",
    "\n",
    "        # We apply chain rule\n",
    "        \n",
    "        # Derivates for Loss function and softmax\n",
    "        dC_dz2 = derivative_softmax(a2_softmax,batch_labels) # a2 - y , y -> target ,a2 -> prediction\n",
    "        \n",
    "        # dC_dz2 = a2 - y\n",
    "        # dz2_da1 =  ...\n",
    "        # da1_dw2 = ....\n",
    "        # dC_dw2 = dC_dz2 * dz2_da1*da1_w2\n",
    "        \n",
    "        \n",
    "        # Derivates for w2,b2\n",
    "        dC_dw2 = (a1_tanh.T @ dC_dz2) / size_batch\n",
    "        dC_db2 = np.sum(dC_dz2,axis=0,keepdims=True) / size_batch\n",
    "        \n",
    "        # Derivates for activation function tanh and z1\n",
    "        dz2_da1 = dC_dz2 @ w2.T\n",
    "        da1_dz1 = dz2_da1 * derivative_tanh(a1_tanh)\n",
    "        \n",
    "        # Derivates for w1,b1\n",
    "        dC_dw1 = (batch_data.T @ da1_dz1) / size_batch \n",
    "        dC_db1 = np.sum(da1_dz1,axis=0,keepdims=True)/size_batch\n",
    "        \n",
    "        # Apply regularization L1, L2\n",
    "\n",
    "# Update weight-urile cu Elastic Net (L1 + L2)\n",
    "    ##L1 regularization penalize weight that approach to 0 and L2 regularization penalize weight that are too large\n",
    "        \n",
    "    # lambda_L1 * np.sign(w1) -> L1 regularization\n",
    "        # np.sign(w1) -> return 1 if w1 > 0, -1 if w1 < 0, 0 if w1 = 0  --> actually its show the direction of the weight(derivative) \n",
    "    # lambda_L2 * w1 -> L2 regularization ,is the sum of the square of the weights but in gradient descent we use the derivative of the square of the weights(2*w1) but 2 is absorbed in the learning rate, so we use only w1\n",
    "        # L2 regularization is the sum of the square of the weights \n",
    "        \n",
    "        # Gradients calculus\n",
    "          # Weight and biass actualization\n",
    "            \n",
    "        # w1 -= learning_rate * (dC_dw1 + lambda_L1 * np.sign(w1) + 2 * lambda_L2 * w1)  \n",
    "        w1 -= learning_rate * dC_dw1\n",
    "        b1 -= learning_rate * dC_db1\n",
    "        w2 -= learning_rate * dC_dw2\n",
    "        # w2 -= learning_rate * (dC_dw2 + lambda_L2 * np.sign(w2) + 2 * lambda_L2 * w2)\n",
    "        b2 -= learning_rate * dC_db2\n",
    "\n",
    "        # Compute the loss\n",
    "        C_loss = cross_entropy_loss(batch_labels, a2_softmax)\n",
    "        epoch_loss += C_loss\n",
    "\n",
    "    # regularization_loss = (lambda_L1 * np.sum(np.abs(w1)) \n",
    "    #                   + lambda_L2 * np.sum(w1**2) \n",
    "    #                   + lambda_L1 * np.sum(np.abs(w2)) \n",
    "    #                   + lambda_L2 * np.sum(w2**2))\n",
    "    # print(f\"Regularization loss: {regularization_loss}\")\n",
    "    # epoch_loss /= len(batched_train_data) + regularization_loss\n",
    "    epoch_loss /= len(batched_train_data)\n",
    "    print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "    return epoch_loss"
   ],
   "id": "df34b509a5ba6a31",
   "outputs": [],
   "execution_count": 643
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:46:50.297040Z",
     "start_time": "2024-11-11T11:46:47.500359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# main\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_x, test_y = download_mnist(False)\n",
    "\n",
    "process_train_X, process_train_Y = process_data(train_X, train_Y)\n",
    "process_test_x, process_test_y = process_data(test_x, test_y)\n",
    "\n",
    "# initialization\n",
    "w1,b1,w2,b2 = xavier_init() # weight and biass initialization with Xavier on each layer one time\n"
   ],
   "id": "fb7f519e2ba90fa4",
   "outputs": [],
   "execution_count": 645
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:50:23.954688Z",
     "start_time": "2024-11-11T11:50:15.206151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# train\n",
    "import time\n",
    "print(\"Training\")\n",
    "# print(\"--------w1:\",w1 ,\"---b1---\",b1)\n",
    "# print(\"--------w2:\",w2 ,\"---b2---\",b2)\n",
    "start_time = time.time()\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch :{epoch}\")\n",
    "    train_epoch(process_train_X, process_train_Y, w1,b1,w2,b2)\n",
    "stop_time= time.time() - start_time\n",
    "print(f\"Training time: {stop_time} seconds\")\n"
   ],
   "id": "9c697df4ac460767",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch :0\n",
      "Epoch loss: 0.06478582451128467\n",
      "Epoch :1\n",
      "Epoch loss: 0.06670326958056723\n",
      "Epoch :2\n",
      "Epoch loss: 0.06545342345150809\n",
      "Epoch :3\n",
      "Epoch loss: 0.06813692919223849\n",
      "Epoch :4\n",
      "Epoch loss: 0.06879220753368648\n",
      "Epoch :5\n",
      "Epoch loss: 0.06684614234773649\n",
      "Epoch :6\n",
      "Epoch loss: 0.06826184795121515\n",
      "Epoch :7\n",
      "Epoch loss: 0.0666084236859378\n",
      "Epoch :8\n",
      "Epoch loss: 0.06951017501782274\n",
      "Epoch :9\n",
      "Epoch loss: 0.06875523274507941\n",
      "Training time: 8.742879390716553 seconds\n"
     ]
    }
   ],
   "execution_count": 652
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:02:43.440024Z",
     "start_time": "2024-11-09T18:02:43.433046Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 212,
   "source": [
    "def accuracy(data,labels,w1,b1,w2,b2):\n",
    "    w1_copy= w1.copy()\n",
    "    b1_copy= b1.copy()\n",
    "    w2_copy= w2.copy()\n",
    "    b2_copy= b2.copy()\n",
    "    _,_,_,a2 = forward_propagation(data,w1_copy,b1_copy,w2_copy,b2_copy)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return f\"{accuracy * 100}%\""
   ],
   "id": "beabbcd0f2a9d714"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:50:27.632713Z",
     "start_time": "2024-11-11T11:50:27.583192Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Accuracy : \",accuracy(process_test_x,process_test_y,w1,b1,w2,b2))",
   "id": "805bb32cf95909e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  97.63%\n"
     ]
    }
   ],
   "execution_count": 653
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:26:05.417434Z",
     "start_time": "2024-11-10T16:26:05.412502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "# Save the model parameters to a file\n",
    "\n",
    "# Create a dictionary with the model parameters\n",
    "model_parameters = {\n",
    "    \"w1\": w1,\n",
    "    \"b1\": b1,\n",
    "    \"w2\": w2,\n",
    "    \"b2\": b2\n",
    "}\n",
    "\n",
    "# Save the model parameters to a file\n",
    "with open(\"model_parameters_after_dropout.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_parameters, file) \n"
   ],
   "id": "24e5f0ae02ad84f6",
   "outputs": [],
   "execution_count": 579
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:49:54.609310Z",
     "start_time": "2024-11-11T11:49:54.600546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# Load the model parameters from a file\n",
    "\n",
    "\n",
    "# with open(\"model_parameters_after_dropout.pkl\", \"rb\") as file:\n",
    "#     model_parameters = pickle.load(file)\n",
    "\n",
    "with open(\"model_parameters_before_dropout.pkl\", \"rb\") as file:\n",
    "    model_parameters = pickle.load(file)\n",
    "\n",
    "# Extract the model parametersZ\n",
    "w1 = model_parameters[\"w1\"]\n",
    "b1 = model_parameters[\"b1\"]\n",
    "w2 = model_parameters[\"w2\"]\n",
    "b2 = model_parameters[\"b2\"]\n"
   ],
   "id": "c595c38f164ccc76",
   "outputs": [],
   "execution_count": 650
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:52:31.853177Z",
     "start_time": "2024-11-09T17:52:31.845480Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(60000, 784)\n",
      "A\n"
     ]
    }
   ],
   "execution_count": 191,
   "source": [
    "print(process_train_Y.shape)\n",
    "print(process_train_X.shape)\n",
    "print(\"A\")"
   ],
   "id": "e7afd020f9961d71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7ae0f2c37db90409"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./img/tanh.png\" alt=\"Alt text\" width=\"500\"/>",
   "id": "a722bbbf6220e2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8a0b32ef98d5740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
