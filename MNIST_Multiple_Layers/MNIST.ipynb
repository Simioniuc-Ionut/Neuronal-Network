{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform= lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    mnist_data= []\n",
    "    mnist_labels= []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "def process_data(mnist_data, mnist_labels):\n",
    "    mnist_data = np.array(mnist_data).reshape(-1, 784)\n",
    "    mnist_labels = np.array(mnist_labels).reshape(-1, 1)\n",
    "\n",
    "    processed_mnist_labels = np.zeros((len(mnist_labels), 10))\n",
    "    for idx,mnist_label in enumerate(mnist_labels):\n",
    "        # One hot encoding\n",
    "        value = mnist_label[0]\n",
    "        processed_mnist_labels[idx, value] = 1\n",
    "    \n",
    "    # normalization\n",
    "    normalize_mnist_data = mnist_data / 255\n",
    "    return normalize_mnist_data, processed_mnist_labels"
   ],
   "id": "9ba842d0e98c8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "u = 0 , sigma = radical(2/fain_in + fan_out)",
   "id": "112a0d6d12f4eb27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:50:25.697845Z",
     "start_time": "2024-11-09T17:50:25.685741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split in batches\n",
    "# 2. Split data into batches\n",
    "def split_in_batches(data, labels, batch_size=100):\n",
    "    # Shuffle the data and labels in unison\n",
    "    permutation = np.random.permutation(len(data))\n",
    "    shuffled_data = data[permutation]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    \n",
    "    # Split into batches\n",
    "    batched_data = []\n",
    "    batched_labels = []\n",
    "    for i in range(0, len(shuffled_data), batch_size):\n",
    "        batched_data.append(shuffled_data[i:i+batch_size])\n",
    "        batched_labels.append(shuffled_labels[i:i+batch_size])\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    batched_data = np.array(batched_data)\n",
    "    batched_labels = np.array(batched_labels)\n",
    "    \n",
    "    return batched_data, batched_labels "
   ],
   "id": "a08501f1d6c79e89",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# weight and biass initialization with Xavier\n",
    "\n",
    "\n",
    "def xavier_init():\n",
    "    fan_out = 10\n",
    "    fan_in = 784\n",
    "    fan_hidden = 100\n",
    "    # we use normal distribution with function np.random.randn\n",
    "    w1 = np.random.randn(fan_in, fan_hidden) * np.sqrt(2 / (fan_in + fan_hidden))\n",
    "    b1 = np.random.randn(1, fan_hidden) * np.sqrt(2 / (1 + fan_hidden))\n",
    "    w2 = np.random.randn(fan_hidden, fan_out) * np.sqrt(2 / (fan_hidden + fan_out))\n",
    "    b2 = np.random.randn(1, fan_out) * np.sqrt(2 / (1 + fan_out))\n",
    "    return w1, b1, w2, b2 \n"
   ],
   "id": "8a16fbe93e2204b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:07:09.170558Z",
     "start_time": "2024-11-09T18:07:09.165018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dropout_function(layer_output, dropout_rate):\n",
    "    # Generate a mask that will zero out neurons with probability dropout_rate\n",
    "    mask = (np.random.rand(*layer_output.shape) < (1- dropout_rate)).astype(float)\n",
    "    # Apply the mask to the layer output and scale neurons that remain active \n",
    "    return layer_output * mask / (1 - dropout_rate)"
   ],
   "id": "a4d0916da6bbdb30",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T20:58:50.655474Z",
     "start_time": "2024-11-09T20:58:50.647260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2 activation function \n",
    "#  tanh for hidden layer\n",
    "def activation_tanh(z):\n",
    "    return  np.tanh(z) #(np.exp(z) - np.exp(z))/(np.exp(z) + np.exp(-z)) # or : np.tanh(z)\n",
    "def activation_softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True) # normalization \n",
    "\n",
    "# derivative of activation function\n",
    "def derivative_tanh(z):\n",
    "    return 1 - np.power(activation_tanh(z), 2) # derivata lui tanh este (1 - a1^2) unde a1 este rezultatul functiei de activare\n",
    "\n",
    "# z is the output of the activation function\n",
    "def derivative_softmax(a2_softmax, y_target):\n",
    "    \"\"\"Derivative of the softmax function with respect to the cross-entropy loss\"\"\"\n",
    "    return a2_softmax - y_target\n",
    "\n",
    "def forward_propagation(x_img,w1,b1,w2,b2):\n",
    "    z1 = x_img @ w1 + b1\n",
    "    a1 = activation_tanh(z1)\n",
    "    z2 = a1 @ w2 +  b2\n",
    "    a2 = activation_softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# optimization technique\n",
    "def forward_propagation_with_dropout(x_img,w1,b1,w2,b2):\n",
    "    z1 = x_img @ w1 + b1\n",
    "    a1 = activation_tanh(z1) \n",
    "    # here we apply dropout\n",
    "    a1_dropped = dropout_function(a1, 0.10) \n",
    "    z2 = a1_dropped @ w2 + b2 \n",
    "    a2 = activation_softmax(z2) \n",
    "    return z1, a1_dropped, z2, a2\n",
    "\n",
    "# def regulariation_L1_L2(w1,w2,b1,b2):\n",
    "#     # L1 regularization\n",
    "#     w1 = w1 - 0.01*()\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6]]) \n",
    "print(np.exp(A)) # compute e^A"
   ],
   "id": "7b2d4f148f43ed93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.71828183   7.3890561   20.08553692]\n",
      " [ 54.59815003 148.4131591  403.42879349]]\n"
     ]
    }
   ],
   "execution_count": 517
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:49:17.155748Z",
     "start_time": "2024-11-09T17:49:17.148986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loss function\n",
    "def cross_entropy_loss(y_target,y_prediction_after_softmax):\n",
    "    length = y_target.shape[0] \n",
    "    return -np.sum(y_target * np.log(y_prediction_after_softmax)) / length\n",
    "\n",
    "\n"
   ],
   "id": "198c7f747ec0c6f7",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:36:13.957791Z",
     "start_time": "2024-11-11T20:36:13.929855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def train_epoch(process_train_X, process_train_Y, w1,b1,w2,b2):\n",
    "    batched_train_data, batched_train_labels = split_in_batches(process_train_X, process_train_Y)\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    lambda_L1 = 0.0001  \n",
    "    lambda_L2 = 0.0001  # Coeficient de regularizare pentru L2\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "\n",
    "    for batch_data, batch_labels in zip(batched_train_data, batched_train_labels):\n",
    "        size_batch = batch_data.shape[1] # Dimension of the batch_data is [600,100,784]\n",
    "        z1, a1_tanh, z2, a2_softmax = forward_propagation_with_dropout(batch_data,w1,b1,w2,b2)\n",
    "\n",
    "        # We apply chain rule\n",
    "        \n",
    "        # Derivates for Loss function and softmax\n",
    "        dC_dz2 = derivative_softmax(a2_softmax,batch_labels) # a2 - y , y -> target ,a2 -> prediction \n",
    "        \n",
    "        # dC_dz2 = a2 - y\n",
    "        # dz2_da1 =  ...\n",
    "        # da1_dw2 = ....\n",
    "        # dC_dw2 = dC_dz2 * dz2_da1*da1_w2\n",
    "        \n",
    "        \n",
    "        # Derivates for w2,b2\n",
    "        dC_dw2 = (a1_tanh.T @ dC_dz2) / size_batch \n",
    "        dC_db2 = np.sum(dC_dz2,axis=0,keepdims=True) / size_batch \n",
    "        \n",
    "        # Derivates for activation function tanh and z1\n",
    "        dC_da1 = dC_dz2 @ w2.T \n",
    "        dC_dz1 = dC_da1 * derivative_tanh(a1_tanh) \n",
    "      \n",
    "        # Derivates for w1,b1 ( batch_data = x ,inputul de pe primul layer\n",
    "        dC_dw1 = (batch_data.T @ dC_dz1) / size_batch  \n",
    "        dC_db1 = np.sum(dC_dz1,axis=0,keepdims=True)/size_batch \n",
    "        \n",
    "        # Apply regularization L1, L2\n",
    "\n",
    "# Update weight-urile cu Elastic Net (L1 + L2)\n",
    "    ##L1 regularization penalize weight that approach to 0 and L2 regularization penalize weight that are too large\n",
    "        \n",
    "    # lambda_L1 * np.sign(w1) -> L1 regularization\n",
    "        # np.sign(w1) -> return 1 if w1 > 0, -1 if w1 < 0, 0 if w1 = 0  --> actually its show the direction of the weight(derivative) \n",
    "    # lambda_L2 * w1 -> L2 regularization ,is the sum of the square of the weights but in gradient descent we use the derivative of the square of the weights(2*w1) but 2 is absorbed in the learning rate, so we use only w1\n",
    "        # L2 regularization is the sum of the square of the weights \n",
    "        \n",
    "        # Gradients calculus\n",
    "          # Weight and biass actualization\n",
    "            \n",
    "        w1 -= learning_rate * (dC_dw1 + lambda_L1 * np.sign(w1) + 2 * lambda_L2 * w1)  \n",
    "        # w1 -= learning_rate * dC_dw1 \n",
    "        b1 -= learning_rate * dC_db1 \n",
    "        # w2 -= learning_rate * dC_dw2 \n",
    "        w2 -= learning_rate * (dC_dw2 + lambda_L2 * np.sign(w2) + 2 * lambda_L2 * w2)\n",
    "        b2 -= learning_rate * dC_db2\n",
    "        \n",
    "        # Compute the loss\n",
    "        C_loss = cross_entropy_loss(batch_labels, a2_softmax)\n",
    "        epoch_loss += C_loss \n",
    "        \n",
    "    regularization_loss = (lambda_L1 * np.sum(np.abs(w1)) \n",
    "                      + lambda_L2 * np.sum(w1**2) \n",
    "                      + lambda_L1 * np.sum(np.abs(w2)) \n",
    "                      + lambda_L2 * np.sum(w2**2))\n",
    "    \n",
    "    print(f\"Regularization loss: {regularization_loss} , {regularization_loss.shape}\")\n",
    "    epoch_loss /= len(batched_train_data) + regularization_loss\n",
    "    # epoch_loss /= len(batched_train_data)\n",
    "    print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "    return epoch_loss"
   ],
   "id": "a09f6f6653eaad5f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:46:50.297040Z",
     "start_time": "2024-11-11T11:46:47.500359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# main\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_x, test_y = download_mnist(False)\n",
    "\n",
    "process_train_X, process_train_Y = process_data(train_X, train_Y)\n",
    "process_test_x, process_test_y = process_data(test_x, test_y)\n",
    "\n",
    "# initialization\n",
    "w1,b1,w2,b2 = xavier_init() # weight and biass initialization with Xavier on each layer one time\n"
   ],
   "id": "fb7f519e2ba90fa4",
   "outputs": [],
   "execution_count": 645
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:27:28.199798Z",
     "start_time": "2024-11-11T13:26:11.497795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# train\n",
    "import time\n",
    "print(\"Training\")\n",
    "# print(\"--------w1:\",w1 ,\"---b1---\",b1)\n",
    "# print(\"--------w2:\",w2 ,\"---b2---\",b2)\n",
    "start_time = time.time()\n",
    "for epoch in range(50):\n",
    "    print(f\"Epoch :{epoch}\")\n",
    "    train_epoch(process_train_X, process_train_Y, w1,b1,w2,b2)\n",
    "stop_time= time.time() - start_time\n",
    "print(f\"Training time: {stop_time} seconds\")\n"
   ],
   "id": "9c697df4ac460767",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch :0\n",
      "Regularization loss: 0.2952854013853178 , ()\n",
      "Epoch loss: 0.1124608941857557\n",
      "Epoch :1\n",
      "Regularization loss: 0.2920543443864804 , ()\n",
      "Epoch loss: 0.11613324196783607\n",
      "Epoch :2\n",
      "Regularization loss: 0.28886418841883027 , ()\n",
      "Epoch loss: 0.1177100528388101\n",
      "Epoch :3\n",
      "Regularization loss: 0.2857165543043999 , ()\n",
      "Epoch loss: 0.11912745021712076\n",
      "Epoch :4\n",
      "Regularization loss: 0.2826123806839247 , ()\n",
      "Epoch loss: 0.11944634226790196\n",
      "Epoch :5\n",
      "Regularization loss: 0.27954765409494403 , ()\n",
      "Epoch loss: 0.12085512770218906\n",
      "Epoch :6\n",
      "Regularization loss: 0.27650987855755926 , ()\n",
      "Epoch loss: 0.12088858648350775\n",
      "Epoch :7\n",
      "Regularization loss: 0.2735235770935996 , ()\n",
      "Epoch loss: 0.12052700606633086\n",
      "Epoch :8\n",
      "Regularization loss: 0.27057880970914994 , ()\n",
      "Epoch loss: 0.12561384522515356\n",
      "Epoch :9\n",
      "Regularization loss: 0.26767004884852863 , ()\n",
      "Epoch loss: 0.12422413787747127\n",
      "Epoch :10\n",
      "Regularization loss: 0.26479249992410403 , ()\n",
      "Epoch loss: 0.1258532097501346\n",
      "Epoch :11\n",
      "Regularization loss: 0.261972064707888 , ()\n",
      "Epoch loss: 0.12846900118005677\n",
      "Epoch :12\n",
      "Regularization loss: 0.2591652096620116 , ()\n",
      "Epoch loss: 0.12920691638956408\n",
      "Epoch :13\n",
      "Regularization loss: 0.25640031465284674 , ()\n",
      "Epoch loss: 0.12869728725824242\n",
      "Epoch :14\n",
      "Regularization loss: 0.25369648249865 , ()\n",
      "Epoch loss: 0.1331320885837044\n",
      "Epoch :15\n",
      "Regularization loss: 0.25101451142275716 , ()\n",
      "Epoch loss: 0.13199896492776525\n",
      "Epoch :16\n",
      "Regularization loss: 0.24839020774552953 , ()\n",
      "Epoch loss: 0.13762659315337158\n",
      "Epoch :17\n",
      "Regularization loss: 0.24578923448998774 , ()\n",
      "Epoch loss: 0.13483263889305655\n",
      "Epoch :18\n",
      "Regularization loss: 0.24322297910728916 , ()\n",
      "Epoch loss: 0.13692891455786052\n",
      "Epoch :19\n",
      "Regularization loss: 0.2407036371801136 , ()\n",
      "Epoch loss: 0.13790406498990995\n",
      "Epoch :20\n",
      "Regularization loss: 0.238205603766318 , ()\n",
      "Epoch loss: 0.1405143713526233\n",
      "Epoch :21\n",
      "Regularization loss: 0.23574919273509037 , ()\n",
      "Epoch loss: 0.13929219119320574\n",
      "Epoch :22\n",
      "Regularization loss: 0.2333288738288809 , ()\n",
      "Epoch loss: 0.14213295243647744\n",
      "Epoch :23\n",
      "Regularization loss: 0.23094084113295246 , ()\n",
      "Epoch loss: 0.14388003480256495\n",
      "Epoch :24\n",
      "Regularization loss: 0.22858533262179956 , ()\n",
      "Epoch loss: 0.14487350023701875\n",
      "Epoch :25\n",
      "Regularization loss: 0.22627304889900965 , ()\n",
      "Epoch loss: 0.14693976608694612\n",
      "Epoch :26\n",
      "Regularization loss: 0.22399163117163712 , ()\n",
      "Epoch loss: 0.14792910194469866\n",
      "Epoch :27\n",
      "Regularization loss: 0.22173734825632205 , ()\n",
      "Epoch loss: 0.14752569158660306\n",
      "Epoch :28\n",
      "Regularization loss: 0.21952221944571165 , ()\n",
      "Epoch loss: 0.14915744608099757\n",
      "Epoch :29\n",
      "Regularization loss: 0.2173384193368818 , ()\n",
      "Epoch loss: 0.15215523570522757\n",
      "Epoch :30\n",
      "Regularization loss: 0.2151814005175105 , ()\n",
      "Epoch loss: 0.1507658664798895\n",
      "Epoch :31\n",
      "Regularization loss: 0.21306622560531804 , ()\n",
      "Epoch loss: 0.15345741910586913\n",
      "Epoch :32\n",
      "Regularization loss: 0.21098335648474237 , ()\n",
      "Epoch loss: 0.15471604694358948\n",
      "Epoch :33\n",
      "Regularization loss: 0.20894187624115512 , ()\n",
      "Epoch loss: 0.15751987976537252\n",
      "Epoch :34\n",
      "Regularization loss: 0.20690548895343042 , ()\n",
      "Epoch loss: 0.15589164230603023\n",
      "Epoch :35\n",
      "Regularization loss: 0.20492434061427256 , ()\n",
      "Epoch loss: 0.16146872861004655\n",
      "Epoch :36\n",
      "Regularization loss: 0.20296826628587317 , ()\n",
      "Epoch loss: 0.16078844107934054\n",
      "Epoch :37\n",
      "Regularization loss: 0.2010316573118903 , ()\n",
      "Epoch loss: 0.16160221263584715\n",
      "Epoch :38\n",
      "Regularization loss: 0.19913255971271088 , ()\n",
      "Epoch loss: 0.16234242806976137\n",
      "Epoch :39\n",
      "Regularization loss: 0.1972516765046453 , ()\n",
      "Epoch loss: 0.16212908367962495\n",
      "Epoch :40\n",
      "Regularization loss: 0.19541273325294772 , ()\n",
      "Epoch loss: 0.16354633694146176\n",
      "Epoch :41\n",
      "Regularization loss: 0.193595275442275 , ()\n",
      "Epoch loss: 0.16464851123908356\n",
      "Epoch :42\n",
      "Regularization loss: 0.19182040849577128 , ()\n",
      "Epoch loss: 0.16605382631411553\n",
      "Epoch :43\n",
      "Regularization loss: 0.19005593735432808 , ()\n",
      "Epoch loss: 0.1696546787501745\n",
      "Epoch :44\n",
      "Regularization loss: 0.18831625095546956 , ()\n",
      "Epoch loss: 0.16753927661931264\n",
      "Epoch :45\n",
      "Regularization loss: 0.18661285502225405 , ()\n",
      "Epoch loss: 0.1709098382996815\n",
      "Epoch :46\n",
      "Regularization loss: 0.18492981466531683 , ()\n",
      "Epoch loss: 0.17076245574483262\n",
      "Epoch :47\n",
      "Regularization loss: 0.1832775848276816 , ()\n",
      "Epoch loss: 0.16998034070883905\n",
      "Epoch :48\n",
      "Regularization loss: 0.18166164903535917 , ()\n",
      "Epoch loss: 0.17502188063000615\n",
      "Epoch :49\n",
      "Regularization loss: 0.18005501539353427 , ()\n",
      "Epoch loss: 0.17391766789820953\n",
      "Training time: 76.69588541984558 seconds\n"
     ]
    }
   ],
   "execution_count": 681
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:02:43.440024Z",
     "start_time": "2024-11-09T18:02:43.433046Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 212,
   "source": [
    "def accuracy(data,labels,w1,b1,w2,b2):\n",
    "    w1_copy= w1.copy()\n",
    "    b1_copy= b1.copy()\n",
    "    w2_copy= w2.copy()\n",
    "    b2_copy= b2.copy()\n",
    "    _,_,_,a2 = forward_propagation(data,w1_copy,b1_copy,w2_copy,b2_copy)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return f\"{accuracy * 100}%\""
   ],
   "id": "beabbcd0f2a9d714"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:24:10.548784Z",
     "start_time": "2024-11-11T13:24:10.495467Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Accuracy : \",accuracy(process_test_x,process_test_y,w1,b1,w2,b2))",
   "id": "805bb32cf95909e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  97.28999999999999%\n"
     ]
    }
   ],
   "execution_count": 680
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:26:05.417434Z",
     "start_time": "2024-11-10T16:26:05.412502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "# Save the model parameters to a file\n",
    "\n",
    "# Create a dictionary with the model parameters\n",
    "model_parameters = {\n",
    "    \"w1\": w1,\n",
    "    \"b1\": b1,\n",
    "    \"w2\": w2,\n",
    "    \"b2\": b2\n",
    "}\n",
    "\n",
    "# Save the model parameters to a file\n",
    "with open(\"model_parameters_after_dropout.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_parameters, file) \n"
   ],
   "id": "24e5f0ae02ad84f6",
   "outputs": [],
   "execution_count": 579
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:28:57.167564Z",
     "start_time": "2024-11-11T12:28:57.163020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# Load the model parameters from a file\n",
    "\n",
    "\n",
    "# with open(\"model_parameters_after_dropout.pkl\", \"rb\") as file:\n",
    "#     model_parameters = pickle.load(file)\n",
    "\n",
    "with open(\"model_parameters_before_dropout.pkl\", \"rb\") as file:\n",
    "    model_parameters = pickle.load(file)\n",
    "\n",
    "# Extract the model parametersZ\n",
    "w1 = model_parameters[\"w1\"]\n",
    "b1 = model_parameters[\"b1\"]\n",
    "w2 = model_parameters[\"w2\"]\n",
    "b2 = model_parameters[\"b2\"]\n"
   ],
   "id": "c595c38f164ccc76",
   "outputs": [],
   "execution_count": 669
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:52:31.853177Z",
     "start_time": "2024-11-09T17:52:31.845480Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(60000, 784)\n",
      "A\n"
     ]
    }
   ],
   "execution_count": 191,
   "source": [
    "print(process_train_Y.shape)\n",
    "print(process_train_X.shape)\n",
    "print(\"A\")"
   ],
   "id": "e7afd020f9961d71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7ae0f2c37db90409"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"./img/tanh.png\" alt=\"Alt text\" width=\"500\"/>",
   "id": "a722bbbf6220e2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8a0b32ef98d5740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
